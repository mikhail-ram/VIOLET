{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mikhail-ram/VIOLET/blob/main/VIOLET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YOPkAhcNpgy"
      },
      "source": [
        "# Vectorized Invariance Optimization for Language Embeddings using Twins\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHS4tdu1qrMN"
      },
      "source": [
        "## Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aIp_k903exP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets nlpaug sacremoses optuna kaleido\n",
        "!python -m nltk.downloader averaged_perceptron_tagger_eng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIVgUzYUqvy5"
      },
      "source": [
        "## Implementing Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qq6bUXBM1lIz"
      },
      "outputs": [],
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "class BackTranslationAug:\n",
        "    def __init__(self, model_name_en_to_fr, model_name_fr_to_en, device='cuda'):\n",
        "        # Load models and tokenizers for English-French and French-English translation\n",
        "        self.device = device\n",
        "        self.name = \"BackTranslation_Aug\"\n",
        "        self.action = \"insert\"\n",
        "        self.aug_p = 0.3\n",
        "\n",
        "        # Load English to French model and tokenizer\n",
        "        self.tokenizer_en_to_fr = MarianTokenizer.from_pretrained(model_name_en_to_fr)\n",
        "        self.model_en_to_fr = MarianMTModel.from_pretrained(model_name_en_to_fr).to(self.device)\n",
        "\n",
        "        # Load French to English model and tokenizer\n",
        "        self.tokenizer_fr_to_en = MarianTokenizer.from_pretrained(model_name_fr_to_en)\n",
        "        self.model_fr_to_en = MarianMTModel.from_pretrained(model_name_fr_to_en).to(self.device)\n",
        "\n",
        "    def augment(self, texts, num_beams=1):\n",
        "        with torch.no_grad():\n",
        "            # Translate English to French\n",
        "            inputs_en_to_fr = self.tokenizer_en_to_fr(texts, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
        "            with torch.amp.autocast(self.device):  # Enable mixed precision\n",
        "                translated_to_fr = self.model_en_to_fr.generate(**inputs_en_to_fr, num_beams=num_beams)\n",
        "            translated_to_fr = self.tokenizer_en_to_fr.batch_decode(translated_to_fr, skip_special_tokens=True)\n",
        "\n",
        "            # Translate French back to English\n",
        "            inputs_fr_to_en = self.tokenizer_fr_to_en(translated_to_fr, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
        "            with torch.amp.autocast(self.device):  # Enable mixed precision\n",
        "                translated_back_to_en = self.model_fr_to_en.generate(**inputs_fr_to_en, num_beams=num_beams)\n",
        "\n",
        "        return self.tokenizer_fr_to_en.batch_decode(translated_back_to_en, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nmpCA0FBawv"
      },
      "source": [
        "## Training Loop (w/ shuffling for dev sentences + mixup regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "EaG-_6UtyyNy",
        "outputId": "1ce16dfa-7534-4bda-c82f-a4b40d5c9e48"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"f238a3ba-b8bf-47f8-87f2-5c06438e2f2f\" class=\"plotly-graph-div\" style=\"height:800px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f238a3ba-b8bf-47f8-87f2-5c06438e2f2f\")) {                    Plotly.newPlot(                        \"f238a3ba-b8bf-47f8-87f2-5c06438e2f2f\",                        [{\"mode\":\"lines+markers\",\"name\":\"Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232],\"y\":[8041.51025390625,7232.7744140625,8159.3759765625,8302.6630859375,9230.3212890625,7842.63134765625,8334.623046875,8301.26171875,8591.384765625,7858.5068359375,8573.404296875,7841.314453125,8254.32421875,7856.70458984375,7835.4033203125,7540.98046875,8121.830078125,7864.88671875,7362.29541015625,7080.77880859375,7785.5380859375,8540.955078125,7735.212890625,8338.8740234375,7763.087890625,8035.83740234375,7479.54931640625,7552.02001953125,8185.86767578125,7943.72607421875,7168.22998046875,8236.6416015625,7580.77587890625,7295.30126953125,7136.51708984375,6896.0390625,7784.55517578125,7958.98828125,8125.58740234375,8189.11328125,8516.1787109375,7311.1044921875,7523.1611328125,7419.93603515625,7888.81494140625,7893.03076171875,7705.0126953125,7452.40185546875,8379.5869140625,7806.25390625,7750.64453125,7367.26708984375,7594.306640625,7284.154296875,7216.73583984375,7877.640625,7343.05517578125,7087.359375,7153.2939453125,7028.72265625,7188.78076171875,7720.955078125,7671.61181640625,7145.4814453125,7095.0732421875,7473.431640625,7734.8603515625,8358.9580078125,7277.822265625,7659.3369140625,7504.27099609375,7766.291015625,7392.69140625,7320.74169921875,6938.69091796875,7847.880859375,7426.03125,7324.19775390625,7064.5263671875,7372.26806640625,7628.69677734375,7516.0126953125,7837.29833984375,7352.587890625,7987.72509765625,7299.5244140625,8147.96142578125,7612.39697265625,7844.89990234375,8005.078125,8432.515625,8397.080078125,7170.68310546875,8199.7939453125,7710.6123046875,8418.2666015625,8037.251953125,8071.8642578125,7435.23291015625,7769.59765625,7983.66259765625,7936.02392578125,8337.6318359375,7724.19873046875,7713.52880859375,7141.79931640625,6954.50439453125,7328.6669921875,7684.6748046875,7551.693359375,7978.4111328125,7088.9873046875,8280.96875,8002.91552734375,7415.181640625,7825.41650390625,6993.84423828125,8118.841796875,8279.9716796875,7771.658203125,7994.26806640625,7644.376953125,7319.90771484375,7875.4169921875,8036.18994140625,8132.13134765625,7659.8408203125,7521.00244140625,7681.61083984375,7586.24609375,7269.6318359375,7785.7841796875,8413.6728515625,7029.58642578125,7192.43310546875,8104.556640625,7346.9306640625,8106.853515625,7178.27783203125,7259.9677734375,7121.6767578125,8249.1962890625,8098.912109375,7596.30419921875,7538.02099609375,7255.5625,7358.751953125,7453.603515625,8178.06103515625,8001.86376953125,8046.86767578125,6923.5595703125,7881.37939453125,7780.04345703125,7139.4150390625,7358.287109375,7878.02685546875,7412.16357421875,7407.244140625,7968.283203125,7619.24072265625,7093.59326171875,7773.22021484375,7990.03173828125,7738.845703125,7156.060546875,7358.951171875,7365.25927734375,7655.24609375,8153.81201171875,8080.83056640625,7839.74462890625,7268.09912109375,7779.31103515625,7988.9619140625,7944.951171875,7946.64892578125,7608.01513671875,7223.6181640625,7583.81982421875,7182.15380859375,7713.00341796875,7771.21044921875,7391.15478515625,7345.90185546875,7387.91015625,6954.9970703125,7493.3271484375,7831.82763671875,8027.3447265625,6923.3837890625,7979.755859375,8067.91357421875,7933.49560546875,8156.24755859375,7546.0322265625,7536.55712890625,7354.74609375,7783.3427734375,7727.70751953125,8025.7646484375,7221.03125,7228.1884765625,7465.3837890625,8017.01318359375,7223.13720703125,7423.77392578125,8167.27978515625,8004.578125,7856.630859375,7792.24072265625,7386.88916015625,7220.396484375,7204.595703125,8283.9609375,8042.740234375,7473.65576171875,8699.3642578125,7849.39501953125,7308.4404296875,7449.19482421875,7930.50146484375,7872.96142578125,7518.55419921875,7588.71435546875,8174.8994140625,7800.99365234375,8077.33984375,7946.359375,8542.552734375,7456.751953125,7768.87353515625,7699.765625,7965.6689453125,7380.85205078125,7991.5625,7743.90185546875,7864.2919921875,7953.63720703125,7976.7763671875,8016.77001953125,7546.19189453125,7972.6806640625,7897.95166015625,8323.0693359375,7642.31298828125,8267.625,7063.33251953125,7913.77978515625,7934.05517578125,7274.53271484375,7441.52880859375,7081.56689453125,7359.521484375,7029.44921875,7126.609375,7857.59619140625,8034.4033203125,7043.01806640625,7202.546875,7162.755859375,7822.9345703125,7726.96533203125,7859.919921875,7690.97216796875,7255.94873046875,7749.6796875,7832.0,6831.0712890625,7751.95068359375,7372.73486328125,7778.92138671875,7864.759765625,8107.47265625,7271.853515625,7543.63916015625,8262.4736328125,7794.2041015625,7423.1103515625,7906.89208984375,7878.4189453125,7207.5849609375,6977.5048828125,7165.0810546875,7481.53759765625,6981.3095703125,7968.4111328125,6754.4462890625,8057.3623046875,7984.1708984375,7051.49169921875,6815.095703125,6955.69384765625,7671.5712890625,7723.53076171875,7474.22705078125,7044.72607421875,7179.654296875,7618.4091796875,7097.83544921875,7961.36865234375,7867.9951171875,7585.5302734375,7934.92919921875,7634.8671875,6872.6611328125,6867.1259765625,7176.40576171875,7295.28271484375,6926.02685546875,8167.42822265625,7813.06689453125,7732.2998046875,7694.6318359375,7076.3291015625,7968.88916015625,7111.21044921875,7141.46240234375,7883.11865234375,7494.4990234375,6938.6123046875,6974.29296875,7904.6123046875,7267.47900390625,6989.03662109375,8081.818359375,6600.9228515625,7792.943359375,7489.3466796875,7338.36279296875,6827.84228515625,7630.66943359375,7838.47607421875,7392.82470703125,7176.5830078125,6837.67626953125,7161.27294921875,7360.5244140625,6814.97412109375,7784.21435546875,6960.1455078125,7100.0,7684.82275390625,7821.05029296875,7351.3759765625,7033.38671875,7687.80224609375,7827.3359375,7683.4912109375,7070.7451171875,7551.00390625,7179.65966796875,6750.03076171875,7409.15478515625,7960.6689453125,8202.3603515625,7701.39111328125,7585.4404296875,7171.05615234375,6987.92138671875,7184.60693359375,7867.46240234375,6890.31787109375,8063.947265625,7697.0390625,7413.09033203125,7454.9072265625,7112.26416015625,7115.2724609375,7475.16015625,7479.24658203125,7745.08837890625,7191.3212890625,7637.5458984375,6854.80078125,7260.58740234375,6730.65966796875,7113.5361328125,6763.32275390625,6895.658203125,7212.0390625,7236.939453125,7455.638671875,7098.86083984375,7240.876953125,6789.55810546875,7518.1611328125,7708.8583984375,6785.03369140625,7199.66162109375,7786.802734375,6981.23583984375,6797.662109375,7643.30126953125,6905.33984375,7513.1943359375,6993.12109375,6839.85498046875,7158.36328125,6687.375,6939.677734375,7740.07568359375,7980.26611328125,6699.33154296875,6741.6591796875,7116.490234375,7393.0439453125,7588.8857421875,7229.67822265625,7696.517578125,7696.046875,7457.67431640625,7006.9521484375,7023.83447265625,6850.34033203125,7419.607421875,6871.24853515625,7743.14892578125,6954.62158203125,7764.34326171875,7337.61474609375,7535.9189453125,6718.2841796875,7021.36962890625,7653.4580078125,7460.076171875,7797.21337890625,6930.537109375,7396.06103515625,7441.65625,7162.43798828125,7091.8583984375,6833.32080078125,7287.013671875,7211.9228515625,7554.904296875,7268.46630859375,6979.30224609375,6831.65576171875,7630.93017578125,7390.27392578125,6868.564453125,7021.09423828125,7247.6669921875,6796.8056640625,7958.1220703125,7424.0361328125,6712.6474609375,6805.38134765625,6477.15234375,7110.46923828125,7894.4462890625,7044.0986328125,6850.88720703125,7694.9189453125,7189.75390625,7762.82666015625,6866.8486328125,7689.4208984375,7603.7919921875,7089.9052734375,6871.2177734375,7478.5380859375,6632.25048828125,6687.6728515625,6889.9462890625,7589.3828125,7289.78466796875,7593.201171875,6894.439453125,6955.9609375,6571.7548828125,6910.7744140625,7500.4638671875,7226.193359375,6999.02001953125,7283.8798828125,6663.599609375,6989.2138671875,7208.18017578125,6602.06640625,6940.494140625,7736.93212890625,6703.29345703125,6921.04296875,6950.8486328125,7443.9658203125,7896.08349609375,7965.73291015625,7123.9619140625,6803.6494140625,6661.64404296875,7994.1083984375,7637.80615234375,6876.36328125,7482.37060546875,7511.42431640625,7416.72607421875,7564.4091796875,7555.01904296875,7494.3740234375,7548.8388671875,6775.591796875,7065.4755859375,6996.181640625,6512.28759765625,6640.36181640625,7757.47216796875,7079.79638671875,7051.990234375,7118.576171875,7699.66064453125,7018.7734375,6677.88037109375,7697.5263671875,6632.83544921875,6975.32470703125,7020.39892578125,6867.4052734375,6995.70166015625,7796.8896484375,6745.427734375,7671.81494140625,7263.81982421875,6917.81787109375,7501.42822265625,7270.08935546875,7614.2177734375,6988.7314453125,7361.32861328125,7071.87353515625,6824.98974609375,6882.30322265625,7496.96875,7168.0390625,7517.154296875,6678.20166015625,7337.65087890625,7056.142578125,7412.7724609375,7915.939453125,6888.78759765625,7110.52001953125,7492.033203125,7109.70166015625,6837.79345703125,6855.91162109375,7418.208984375,7247.169921875,7488.759765625,7070.30517578125,7505.32861328125,7173.78857421875,7201.23388671875,7216.3505859375,7580.04248046875,7037.99755859375,7228.98095703125,7199.1650390625,7200.9306640625,6994.33935546875,7607.4716796875,7478.013671875,7506.25537109375,7389.791015625,7384.5419921875,6786.162109375,7448.9423828125,7644.09765625,7123.2275390625,7294.2666015625,7316.79541015625,7690.63818359375,7102.0302734375,6939.18994140625,7630.9248046875,6851.17529296875,7241.92578125,7821.00244140625,7430.17138671875,7194.3720703125,6976.0888671875,7015.6630859375,7548.251953125,6817.71435546875,7521.931640625,7401.83837890625,7097.93408203125,7258.94091796875,7216.99462890625,7213.2607421875,7450.55712890625,6653.119140625,7334.1630859375,7199.79345703125,7720.4326171875,7020.33349609375,6700.57861328125,6801.4482421875,6841.52783203125,7382.658203125,7091.4990234375,7481.84765625,6809.5693359375,6956.5439453125,6704.57470703125,6844.38720703125,6934.29345703125,7669.51611328125,7717.94580078125,6931.16845703125,7584.98291015625,7303.92578125,7181.28271484375,7268.1201171875,6818.41796875,7538.16552734375,7344.96142578125,7112.1865234375,7059.185546875,7349.72900390625,6814.99365234375,6500.6748046875,6683.88037109375,7866.07958984375,7696.97998046875,6783.56201171875,7640.9384765625,6972.07373046875,7566.59521484375,7190.70947265625,7037.1181640625,6693.0185546875,7362.68359375,7026.4375,7442.86474609375,6671.6123046875,7105.7412109375,6854.06787109375,6833.87451171875,7057.82763671875,7413.2294921875,7191.77783203125,7127.14013671875,6868.46142578125,7148.583984375,7327.41796875,7148.88134765625,6926.01171875,6607.10546875,7824.60009765625,7869.5078125,7486.56787109375,6881.7080078125,6616.82763671875,6869.119140625,7387.85302734375,7288.27392578125,7336.00537109375,7651.77392578125,7542.0439453125,6853.201171875,6750.07958984375,7662.5185546875,7275.5693359375,7317.76611328125,7421.943359375,7409.275390625,6599.48388671875,7186.2001953125,6743.03369140625,7445.0703125,6811.70361328125,6464.4521484375,7206.87451171875,7562.8125,6683.11376953125,7100.33203125,7112.5068359375,6549.60546875,6965.95654296875,6543.240234375,6686.17236328125,6609.73046875,7100.162109375,6808.1640625,7037.6826171875,7576.21875,6452.572265625,6549.67041015625,7664.71875,7609.607421875,7209.759765625,7423.15234375,6725.43359375,6746.5166015625,6788.1318359375,7235.416015625,7638.69482421875,6849.8935546875,6868.30126953125,6615.2978515625,7450.341796875,7036.6708984375,7095.12744140625,7539.267578125,7378.04345703125,6955.33544921875,6569.95166015625,7805.55859375,7615.23486328125,7350.4287109375,7909.9072265625,6791.9228515625,7226.853515625,7591.50634765625,6745.47021484375,7347.5595703125,7481.1875,7376.04345703125,6641.1875,7437.7021484375,7089.22900390625,6438.3720703125,7228.99609375,6512.294921875,6732.20751953125,6984.302734375,7425.72802734375,7027.396484375,6763.591796875,7201.4970703125,6566.61083984375,6846.107421875,7483.16357421875,6833.7900390625,7225.05615234375,6673.7099609375,7344.4326171875,7232.111328125,7427.3759765625,7249.2783203125,6975.29345703125,6645.1005859375,6946.96826171875,7395.04931640625,7382.67626953125,7133.2578125,7173.42578125,7122.408203125,7528.86328125,6720.0634765625,7102.42236328125,6854.6943359375,7650.5634765625,7306.15673828125,6769.83642578125,7377.13232421875,7320.15625,7231.4228515625,6787.078125,7227.57470703125,7007.3203125,7700.140625,7051.3037109375,7322.767578125,7649.60595703125,6926.6416015625,6977.421875,7046.369140625,7226.7265625,6970.9580078125,7111.486328125,6712.1591796875,7202.89111328125,6741.18798828125,6540.4912109375,6914.9560546875,6746.5283203125,6871.22900390625,7588.44873046875,7253.14794921875,6917.21044921875,7272.41748046875,6885.123046875,7219.71484375,7159.49853515625,6993.345703125,6685.54248046875,6659.83935546875,6794.40087890625,6526.01953125,6451.861328125,7307.70751953125,6838.62451171875,6968.16796875,6945.7841796875,7121.3193359375,6657.298828125,7165.15185546875,7172.45703125,7011.93603515625,7208.09814453125,7129.5400390625,6710.880859375,7326.28662109375,7582.1171875,7398.2021484375,6679.69287109375,7243.3798828125,7448.00732421875,6789.77734375,7071.7783203125,6542.5703125,6662.48583984375,7096.2109375,6751.251953125,7197.6689453125,7588.62060546875,6705.31640625,7905.57958984375,7679.3759765625,7352.40625,7495.70654296875,6779.1640625,6527.4423828125,7361.4677734375,7097.9599609375,6833.48193359375,6518.568359375,7146.48291015625,7290.9384765625,6932.18310546875,6868.27099609375,6651.2490234375,7077.51025390625,7312.88330078125,6729.3388671875,6760.45556640625,6564.43603515625,7179.24462890625,6737.78076171875,6918.7333984375,6986.32861328125,6996.119140625,6833.27685546875,7006.8974609375,6650.63037109375,6742.599609375,7094.61669921875,7342.82861328125,7301.236328125,7141.0634765625,6781.9755859375,7682.6904296875,6712.416015625,7069.7392578125,6664.21484375,7386.84326171875,7410.146484375,6876.4248046875,7328.248046875,6921.5673828125,6946.185546875,7323.142578125,7026.0224609375,7355.5166015625,6853.1806640625,7419.59912109375,7079.91650390625,6457.42919921875,7506.29296875,7117.49169921875,6726.4609375,7266.88232421875,7240.21337890625,6384.21728515625,6692.73779296875,6952.73779296875,6920.8095703125,6871.5771484375,6626.48583984375,7585.90185546875,6863.27685546875,7046.91943359375,6875.42236328125,7286.068359375,6555.47705078125,7314.39453125,6522.15625,6930.39013671875,6904.3232421875,7043.71923828125,6591.83203125,7096.9697265625,7348.9111328125,6550.9267578125,7376.13134765625,7552.51318359375,7750.27001953125,6542.2138671875,7880.16455078125,7448.52490234375,7224.24951171875,7009.00830078125,6869.1318359375,7136.2734375,6852.75634765625,6881.20263671875,7652.990234375,6638.03515625,7761.744140625,6953.3095703125,6595.27197265625,6941.6123046875,7053.30322265625,7489.77978515625,7071.6953125,7241.2412109375,7427.37548828125,7389.8857421875,7661.14599609375,7026.29443359375,7362.876953125,7168.03759765625,7417.0966796875,6554.54443359375,7254.919921875,7573.1240234375,6738.44775390625,7080.82275390625,7230.76171875,7088.10693359375,7483.169921875,7335.79248046875,6990.84423828125,6621.8740234375,6673.25830078125,7085.7861328125,7126.42822265625,7675.4091796875,7046.72119140625,6609.6083984375,7651.255859375,6672.46337890625,6709.07861328125,7569.9814453125,7257.96435546875,7140.5986328125,7412.736328125,6725.43603515625,7334.57763671875,6249.080078125,6889.06689453125,6484.82373046875,7373.6015625,6345.4921875,6942.76416015625,7436.1826171875,7632.3623046875,7083.34814453125,7468.8642578125,7241.9462890625,7528.5615234375,6931.5107421875,7006.1611328125,6760.91162109375,6615.9892578125,7288.8330078125,7466.61328125,6359.18115234375,7040.02587890625,6717.25390625,7287.6904296875,7042.9189453125,6546.19189453125,7552.26123046875,6667.26708984375,6776.69091796875,6523.68408203125,7492.1513671875,7069.1630859375,6871.97314453125,7219.7685546875,6634.6845703125,6709.7275390625,6944.24365234375,6641.59765625,6854.8828125,6929.7734375,7078.923828125,6660.3076171875,7527.9599609375,7772.55810546875,6494.8203125,7432.87646484375,7199.59765625,6755.33935546875,7434.111328125,6388.67919921875,7547.13134765625,6842.5908203125,7481.3955078125,6555.736328125,7185.3408203125,7231.912109375,6548.96484375,6411.8955078125,7229.09326171875,6598.564453125,6810.71484375,7014.89990234375,6920.24609375,6383.5859375,7372.5703125,6810.64892578125,7539.58544921875,7325.8212890625,7247.03369140625,7550.6591796875,7235.74462890625,6973.4521484375,7020.1552734375,6681.3623046875,6764.6318359375,7380.0634765625,6899.76953125,7355.72119140625,7567.33642578125,7282.841796875,7683.6572265625,7203.66552734375,6750.9736328125,7206.505859375,7368.8349609375,7609.4384765625,6711.59228515625,7682.57470703125,7044.892578125,6489.8642578125,7269.015625,6641.25244140625,6775.8134765625,7584.2265625,6575.7373046875,6806.83349609375,7342.48388671875,6342.0458984375,6711.02734375,6825.779296875,6954.3193359375,7626.6806640625,7100.59521484375,6917.1875,7155.9599609375,6576.21484375,7124.759765625,6807.65380859375,7390.74365234375,6430.58935546875,6479.71923828125,6363.09716796875,7061.32666015625,7348.3828125,6795.10302734375,7170.84912109375,6991.6904296875,7635.904296875,6468.7890625,6654.76708984375,7333.009765625,7352.59521484375,6641.61474609375,6795.9716796875,6725.19921875,6838.7119140625,6871.51953125,6780.7607421875,7074.6240234375,6601.57958984375,7811.89208984375,7335.56298828125,6647.86767578125,6429.26611328125,7364.015625,6895.4326171875,7279.29296875,6839.248046875,6495.05810546875,6872.787109375,7284.046875,6513.35009765625,6371.47412109375,7289.13232421875,6845.23681640625,6441.17041015625,7306.8994140625,7416.0634765625,6563.42724609375,7080.5126953125,6764.1796875,7177.72705078125,6572.46875,7146.47607421875,6884.94384765625,7237.7431640625,6659.384765625,6623.3740234375,6709.38232421875,7022.22607421875,7219.3505859375,7562.14111328125,7155.90185546875,7193.46240234375,6849.5751953125,6937.93212890625,6542.6552734375,7525.1708984375,7444.23046875,6523.78515625,7123.4189453125,6729.876953125,7407.4501953125,6425.388671875,6727.21435546875,7710.90625,7554.966796875,7039.7158203125,6836.8955078125,7028.49267578125,7234.76806640625,7409.822265625,7339.533203125,7430.60546875,6573.4189453125,6750.91455078125,7007.9912109375,7225.34033203125,6853.16357421875,7384.99609375,6583.212890625,7484.0830078125,6953.3818359375,7134.6513671875,7480.4560546875,7087.12255859375,6817.9716796875,7415.4931640625,7257.568359375,6685.40576171875,6547.64013671875,7161.73779296875,6940.4873046875,7442.48583984375,7442.208984375,7087.06787109375,6680.41796875,7197.37548828125,6453.91015625,6836.75048828125,7162.2314453125,6772.2294921875,7540.5517578125,6705.0771484375,7077.1875,6358.54638671875,6556.79345703125,7443.89892578125,6519.8486328125,7100.7158203125,6745.59814453125,7171.3642578125,7244.27294921875,6387.62744140625,7679.4912109375,7030.20947265625,6809.818359375,7318.044921875,7433.71044921875,7133.18017578125,6662.37841796875,6960.7880859375,6819.13671875,6776.35986328125,7306.0947265625,6684.82080078125,6833.875,6722.6474609375,7269.529296875,6889.4462890625,7419.46875,6793.05078125,6970.60009765625,6825.6962890625,6669.81201171875,6681.771484375,7384.197265625,7276.9794921875,7287.3369140625,7281.318359375,6982.75537109375,6514.51953125,7447.7021484375,7144.1611328125,6572.7939453125,7484.181640625,7214.9169921875,7496.72900390625,7340.3447265625,6472.70068359375,6757.02880859375,7134.607421875,7243.73291015625,6636.884765625,7021.2939453125,7346.12060546875,6640.6396484375,7031.255859375,6505.5830078125,6575.0302734375,6878.75830078125,7302.72802734375,7120.01708984375,6586.1962890625,6589.197265625,7520.205078125,7123.75341796875,7323.427734375,7460.41162109375,7020.9208984375,6326.5771484375,7309.95947265625,7053.03271484375],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"lines+markers\",\"name\":\"Mean Gradient Norm\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232],\"y\":[null,null,null,null,null,null,null,null,null,null,null,197.54425786062623,129.41263670429993,141.7085563880476,125.65508861915376,121.15158832452201,122.20539299655574,100.42889378488883,106.9160232833021,122.41863551852519,101.01496341535142,127.13051880958753,111.66552287593926,116.79781048562279,102.4973804673166,113.62455478877314,114.80584568972898,102.41257342417381,117.398248287318,94.03648578706351,102.30462077159501,106.60636597900671,111.21455390737916,96.61730099492411,94.2369996158678,91.60054504054419,101.54879484983117,108.96306788411235,105.8164235651127,130.67988559665693,119.83942354438392,97.07524980577115,94.61603017933602,96.23570891382701,94.75929652115438,103.98737836516169,103.60489851564064,88.78090080452716,110.5250017084137,95.48269362560382,109.5508634134454,92.92337369033781,89.0356815271032,96.10377810035828,92.23843348729235,108.43044338758587,92.68578265198495,87.3678827546611,92.30642259373838,85.23900468570223,102.46583483765916,99.92135260582987,103.20473631713674,84.07088451973895,80.3093597611536,88.3326628419081,101.34935203050645,114.7578676636863,77.07965680793497,86.53027413736423,85.56692312640116,103.76987954762431,100.52897029541643,109.13067858956487,91.27860046388123,115.13526562072971,90.68127699884698,85.52632291112305,80.88847407244519,91.1491521413287,102.13052822947049,100.18775633971872,105.02632827262555,92.66077079863095,90.32810379911047,91.97070180170272,93.98073267512717,78.33003047836898,97.33200950394148,98.45345902878789,113.28528144172266,null,89.27385586855658,94.31470533352336,99.80311087474534,111.4618937322062,85.70314466574236,100.96978587744121,91.10673566666306,84.58774163473382,93.83639115274273,103.21028009606978,98.62039986316697,102.42103459127114,87.2108911702896,91.08693554897658,74.7842803116598,93.62317516969858,100.75541467927516,79.27161974264223,108.36968255705129,80.1520237446556,103.50031477869108,87.55664574607749,80.95169864128724,91.8296187431305,79.97533108816693,102.02585138047,109.74067979766438,101.00282215403699,86.43259629593472,93.40739353923786,97.62777446662469,89.71140158490137,86.64874487433552,89.55303499689705,92.72104700584478,82.11557094083817,92.97616129655096,89.00303055496606,96.85101210296081,80.55206617786557,127.60853208360895,81.55185876347566,78.90288124596607,96.37060122442485,93.43299730707263,97.35965067651901,86.91171542527167,91.74945765239927,84.61652442074892,97.84885114062837,96.7583584778362,90.24243408199675,94.70224505906309,77.86131659747821,88.86253468965529,90.0830925410766,108.37159721964059,89.04596867471204,99.04849692996248,71.72645292536566,96.57365920823734,104.13611596188616,81.17461311887575,92.78759116739448,104.14865497309034,88.22898028719918,83.85286490237323,85.05272586994846,98.28944459514928,89.65399521585476,92.48214869122793,91.35137066028945,105.02395509227217,85.00431407455243,81.04833077509578,93.28815903809803,92.19049965318823,93.24452828108289,92.57670902283098,95.52051558895477,92.20938343396479,93.55286847728281,96.72931802571354,108.35554434976957,93.78328507253781,108.94198815413155,87.81486769657633,86.66701095987692,84.6424237314651,87.23628128215138,91.71363907461456,83.13700723688794,84.85581289862762,102.21262114452523,77.96005261423582,79.05278601238216,88.28842178430891,93.6313910108326,73.40507671850744,87.59935757584593,106.23452768446303,78.22755703869736,98.97527925518277,87.58797637034625,77.93045971010675,85.56889215607585,87.81097682910712,88.63893592013082,93.1495272088647,80.85321600007319,83.26740720094635,82.54574300391857,91.63860539835247,92.33466045107218,104.34372149299895,89.50675197547504,85.90835093625326,94.2645671692232,80.10739419164386,80.86014029577846,87.1954726209493,87.80617484575707,97.12012086228385,87.84075924690423,84.10552921829803,126.39021918995269,95.91392330673035,81.63981195472576,83.2336327981905,96.81775899928235,85.17790044895243,87.84052342707577,94.66204586881015,93.94806969163879,86.30094916346455,99.415534889296,91.73393279574867,122.22360811639356,94.11822187627912,73.83878769722143,97.55759317308294,91.7546908895352,100.78945298263793,99.85222343439693,83.85432388070959,82.1059539275705,90.50571516477206,92.38463447516017,169.7382324077525,87.4129404777795,93.94869967926437,105.37663379912983,91.82389018447927,84.61803643092533,98.04924466048573,72.50184964887431,83.20243871302135,97.72601132631769,94.99441202477175,82.13012498442214,72.6827778205374,74.22446693088247,80.14864401080925,84.6955908455297,99.28132757810093,105.18263997668653,78.89773566840955,76.6400865461101,70.62278207202415,79.6496368796046,94.05805544968673,102.6730816819934,79.69960674686702,71.32213287150998,77.66580600368118,90.57307651101904,68.0858455505442,81.22424104056397,74.73145092729216,77.71337555621916,79.33129279107315,108.88095357536768,88.0443756533039,80.24900807675486,102.36873855021261,83.03353888363955,71.19512141680626,88.49444863528778,79.29058997198007,79.14701441388439,63.38171768036433,71.4406058670487,73.87982843214503,68.08119607995151,76.75858564275012,59.896052003906114,87.6334647068634,83.07920452391603,69.30269890739642,61.68003836139718,63.12048218122756,74.99088316438973,65.21948022156654,76.09935373455377,68.63398974857456,72.93050120981056,71.78742402770551,62.20580299232456,83.72393048012232,73.18103047283863,92.46236843898878,72.98192124305584,79.63818038318222,72.66431948577853,64.76455266553174,85.29421239306456,74.50622521380808,69.96521281526073,93.06059040622243,71.66843992149268,69.89170493594315,73.17076460403956,65.43792744867619,86.96509896379297,80.11198019793825,67.8612560638919,79.58600214522319,71.58247003318382,63.88190396564713,64.61785501374901,93.11679090032555,65.06713534329282,64.50791222630195,92.51733216537866,52.500669106416574,79.23842432288382,83.90401004605977,74.82436852484989,66.8725833164119,69.46618282311174,77.09001086360252,68.61938566536091,65.09464346102716,63.38412174767835,76.35852914380214,64.43852588970077,55.93908604817491,79.60437623936802,64.31387499740985,76.52390945563846,84.11196523100375,71.5778673533837,89.71593319962044,66.69179748539112,66.21739002356036,79.37138608117561,72.06724573273175,89.77267143488531,76.61233670318468,70.41519037587638,60.61755249378125,74.5220081509107,90.58957994549192,82.55898844402505,68.13837895608918,81.71817244336799,79.39157059939745,66.62102680469754,78.36305651703952,77.53534911471759,63.99505817421921,88.11999895881841,66.84507923146735,68.46845953690081,65.81079379945676,72.5312657616497,70.27146702056315,67.97784076716631,62.66709846497593,67.37051234582309,69.4168869594587,60.06749092183357,68.52830673233738,65.22775054767182,55.15757638463205,74.80374139470568,58.797501885832474,65.55392533814586,62.497986344295285,62.63195106058652,80.57586346600085,64.40641935048127,68.77939615822739,61.828120974054634,69.88857690230769,63.79056717801263,60.955529789521,69.48078446513082,79.87461282849694,59.69067510302402,61.31234725368292,72.45540921561097,69.90194959459167,60.98252443897629,72.47194653232067,55.81726878038608,64.20366192095673,58.65077750346563,63.7511215046046,72.19381947281659,78.88862168889113,58.06303704921246,57.02551826047525,54.16121529790508,74.36322301893073,62.04496168819202,79.44107396644324,78.5110097139579,87.2468308516488,69.1466388697713,67.46876173279132,62.75541316130942,62.97480700555819,67.734348996304,61.69680108618996,63.19886854269106,58.246015674725555,70.35699527517866,73.8501365994605,63.80524299406159,58.108220111449384,61.41838083613583,67.88773435370292,69.72317793230947,74.07484134758809,61.14417558017145,64.32836163059892,59.41720272067383,70.6942024946042,57.99483389837777,60.01526816314151,68.12383289046055,56.23371752598007,78.40209484868318,60.05603097312388,62.64191722263729,60.473938365075156,75.67687293661403,64.68733417530086,59.77037155425834,71.90102583223411,63.32932086321007,61.05710188076782,99.67223467046169,71.80784392950156,58.85023762683684,65.98379688342008,48.03682827553883,72.12068975118153,77.99142259639649,59.119925243235755,63.941371731580716,68.18188896091407,60.517444184150456,77.88429817875965,57.78082225128999,64.76255994100592,73.42907162176978,50.983025998345504,60.71946574153864,59.318666715848174,56.22107855774562,56.77478350797716,76.99900418253169,72.81282575810069,64.02238135552965,67.23075672309557,62.308743731716,60.26603768420578,53.70237691073893,60.49987616301232,61.83671367753726,72.51590651309112,68.10856461848816,62.22799150749191,55.04280215397377,68.49320168754889,67.81215202345068,57.01355429953799,59.55960912612803,71.65166205807242,60.84633438246621,66.16639133822646,65.46347371018024,69.37037222462068,72.47348554440829,74.28505212730099,55.25522123340574,47.54491451390887,51.27175219066093,78.45960110387914,71.56539733981215,67.00593190099443,66.14541198284147,68.36065791048522,77.05846818807531,66.21312563114891,58.47296048088457,67.50202960688945,68.82674253665556,51.16345612063975,72.44396062819526,74.87527916272374,53.91202640419434,51.104277356385836,74.49984534714535,57.35157899147242,57.68896779391541,50.159491831293224,61.85542042378761,75.43845051700136,57.592510542289475,61.94610206644401,58.36289105866962,53.23216767408854,74.54081740610079,57.67422731474414,62.28884829799302,93.42276064852697,54.7360791665641,73.70156961279552,58.808561309627564,60.75165870700176,83.18584519881472,67.21032804647118,59.58873646691069,59.92599520029237,77.53902771378439,58.87434270639824,60.417013468474124,61.732570594818114,68.0343695778107,62.140562087086515,56.882543577424165,52.47644888203962,79.8313595699357,69.48578783841634,72.93800707574567,125.50231909196123,58.23881786133463,59.28178306699057,60.9166904216502,60.55111859141692,58.46525366210909,60.76470612448037,60.711165598092975,57.791100274387716,66.27566175031009,63.76543148712955,69.05334686528246,56.55525877837586,69.84350292356197,79.38407295150242,71.51021392432538,60.19572972223537,65.51198028953291,67.33896309248311,59.690128579520724,62.72958057058461,75.16735984039293,67.95473863182922,72.24408357646733,56.330041821883654,65.37016781303171,55.20595313742925,68.8517784865953,60.4501188382584,80.96013973951098,72.79168381029035,52.36948834675838,64.23506644721469,77.11491242093078,68.09113913468327,68.36540482692591,57.128564763892214,55.7676363655702,67.63812724727283,63.3183612202211,63.25616331741041,57.76488489739558,51.10462227690721,67.3039505001834,61.010361148375935,56.95257733132841,54.45108866290647,65.7679851295114,57.75045410064337,75.44554780355388,53.53751930917055,68.65083396424008,53.53815339604521,54.798622956188716,67.43678777496397,74.03835517716566,69.28839326861134,64.48956038991453,57.363066130932694,57.384477677285616,58.991185802324246,49.29135247593285,68.9674186653906,61.01401968252217,62.43144457020957,59.954733714409564,56.17771863572616,59.1786487991184,85.36427221495505,67.38277981444213,61.300783881213434,77.42359963638388,65.67429766795436,76.62732480289682,66.63308202002102,51.37135179786061,56.93948719981998,56.54589234430499,62.38366966455478,50.736283401462906,68.87392037600478,58.07258081427405,45.665244908469184,53.14899874023553,72.95435366471334,70.8921635432765,48.96927874935614,66.23176707594158,61.60763776072254,67.37347418474438,54.80043464848089,61.36750241436054,55.40185406095894,59.99087156909008,71.06082685183618,56.7557561946718,59.67538000175608,80.46778990343626,56.05523169575734,58.50615869295373,62.66005890944735,58.79951613540424,60.95128184163308,59.94523972753729,60.73309644234544,52.23381027355799,59.033216224673865,54.605181561548775,54.24376905058866,54.216059553045596,79.86294358488045,85.78573262798314,74.71849810285758,61.58192104394816,52.41007041231684,52.42637409085674,58.42224832183939,58.223798267475,76.39089411247333,63.43588450457237,57.60525728764626,52.13268840076715,54.67323185812613,65.98378250591239,66.15851342333258,50.064385144406245,56.70440025661256,64.59490303397307,58.29031001875804,50.87322759748379,49.603728323179986,54.50864679402072,57.28968735023566,46.4563065200676,52.139604257793714,62.7395766307995,58.32242137212872,64.02760051832226,62.79404574437933,52.1779769366346,66.12994491871683,50.96942148967603,51.328437175016376,58.1291388217244,72.02424502507432,55.876289295674965,66.37144220812391,59.97791606665358,43.840528410666764,44.409541359622956,62.339807873032306,79.64690483093698,63.08740160517761,66.5865519124786,53.16552057766711,49.170446005736856,51.666755638015154,60.815162093982664,69.11428842404165,60.62887643505946,55.623783191950245,52.64942227796046,54.64510674431191,51.20947933179418,59.73133981080411,62.516649507334165,59.11994031624921,60.65856288950363,63.13472251893887,77.09776457250375,72.38658992937718,60.15495402182718,73.3471261495819,53.68729060137338,58.948911843094784,61.777441604431374,62.06908079285216,61.98106803491151,66.62642911346566,54.46738087829709,46.20778964854973,67.87947786097764,58.8018387726676,45.98868554238016,54.222258869206684,48.91622061636016,55.46498484359987,60.971900534011525,58.94477087231736,57.86456501325789,67.56333812509024,52.23507530677822,48.15090534689532,57.84638481576707,73.87496953843596,57.80893497435167,56.78879741583265,43.7345992357093,63.45991859307121,56.725081552101265,59.54480459659,64.52670958059275,63.51410402655856,58.98671918197263,61.77837769252702,66.29248496290288,48.349186335312254,58.07218424581871,51.40402563512556,58.22795393349242,61.17900107114342,58.0354112705007,67.21729536852284,62.661048077470575,67.8114190998161,53.749493687116235,48.67675605318795,76.72811430252898,54.41804787839384,53.54029655270641,47.33903380887241,52.823726046399905,47.1659346290673,68.51564966651603,53.94547509130301,56.75203352392585,65.45458936211837,56.0746556008877,61.7132068638344,49.187776422288785,82.29197879307245,62.262713257042805,50.63618565031349,50.573820983442445,52.37356092696226,52.518395280888534,45.89274402599935,49.651939497110284,47.65034378507315,59.37533108818046,71.52823791047472,62.716931389215794,63.79137957861325,53.50525316012037,59.01941105267069,53.12062132244008,45.45347842068411,46.91027144753232,41.811703126369316,54.690266867531925,66.27205626717219,46.322696077154,45.58285185725738,49.255644052910085,39.249748736517894,64.47417762769916,60.74751176234393,55.73230335000164,52.25051939997673,61.03864725177585,46.79710882710668,51.53525812535809,48.44311424462329,74.099702246872,59.52538310428157,60.310567394357925,65.46668650905077,63.27429726547562,55.70778373783855,50.524858762830455,54.308098334183,63.511462366648416,53.419588352300266,50.978383266626395,49.254465555043375,55.62853176176079,58.87478041705358,41.37620341516225,62.43486909893436,49.695689817829866,82.74141614434606,72.45358148102422,51.13457449286878,60.35867818198074,54.6999111356781,48.869798265635794,57.33033714783803,53.92768063232397,53.05059264334095,47.105441819151714,69.36529641418969,52.25559930423383,50.32390331967565,43.527616484964064,50.86279731927442,57.78337688590506,53.58934948413658,60.93195957098854,45.30734558187716,55.688778342052444,56.144743013553914,58.628890800178,45.55417044427868,53.039214262932816,58.69850552871053,61.362458778628564,44.737112725368874,56.13697094076711,53.15857116896794,53.47844520353723,58.11325279535485,54.78007803093552,54.664374619305725,51.04751356165201,64.8435608745719,39.43278355189462,44.98082942608055,49.199710306078316,61.96059484391338,63.253374588311765,63.22414385832532,49.88104585914905,65.4638532671304,62.65712470228664,55.40372750574431,56.554886399074725,55.51291567478866,55.97615735564284,54.390049463491174,49.10180711916559,56.942975143782,56.544912907557,63.97241662225634,61.6297344195953,53.18845881024354,54.379667325508194,49.672706143600465,50.32258103342927,59.69834532817739,56.634998233747254,57.23710189629255,49.74989188140387,61.57076806694868,53.47421740755502,53.92679570199959,42.32029136828292,54.969307408026026,48.69252743199131,62.59806012383528,49.62134840244037,55.51308682288896,65.87992927297314,50.380239086411635,47.9244009142804,47.71147040209768,53.713870500303294,50.552485067853944,63.27432546945699,66.68793819074637,69.05117551260038,49.462219314342406,71.93246194516446,60.295798908882375,50.7778909568875,44.52182761273625,60.80960742574336,88.47575364107983,51.77722085921076,50.373394898999216,68.87367129558804,52.39381201778827,82.76405430658568,46.62902329994058,50.83658430504225,50.40086530459465,65.17516993376559,72.7998760018095,58.61891884659461,49.19290602583538,62.532318027599125,53.84996712844345,65.09280541401884,50.287420689250446,63.089733839767725,49.962491920809015,52.1987541641041,38.693582217493436,46.40662657713035,61.57815794605358,57.83145494152696,61.67289319030449,59.03791459333477,59.17934562288992,66.42648282912606,59.450369817420714,47.75020831103764,55.04795074264528,44.025294810257826,51.86585274220107,61.67003530990145,73.40468268810322,47.48292277977819,47.85352955092064,70.92692643380707,52.526016823111405,45.43121872776486,68.78435877352301,50.165711210142376,48.66092540534773,65.12607557532792,46.58041880690204,61.826024509793335,34.346496368017775,63.372290554420026,51.19108306874685,47.833454487084566,46.65557096982245,59.66817337780781,55.138600431676316,71.16533870241804,50.206479570392105,58.92039162231998,56.13718618320957,68.34864666174661,58.868510650123355,45.85435172041811,50.07435601406813,53.41989273049385,57.0348124067069,65.48165614152663,46.3628610067785,41.535636183714296,55.81028630372438,55.96132990257646,52.6317770320837,46.53607208216291,58.28474281277635,55.14806502563733,62.78233713907684,50.34123671205965,64.06841368326606,53.33863028870042,63.81803588489864,62.89133436588245,54.90814918808958,61.128274354633824,59.040826192658216,46.364095415198015,55.585433722623854,60.7120554531017,51.72905982665491,56.81875476707168,67.61345001199982,65.03828912198009,52.45966528774677,62.4045786910435,65.84704319079269,51.43544045105877,60.646258571941104,45.70387020319368,78.21215467089192,47.98941826069808,59.035290622187794,47.29179321338889,61.445456546400564,50.14323347300652,49.92290675279991,43.297825276474065,46.85546997022054,46.03733856759183,50.43866876075909,47.397439200955304,47.24712373272686,46.1171432069782,69.19343525798996,45.37859676525589,54.23324216263129,63.22962076680546,51.29096596741021,54.959080797256206,61.51299076451292,56.74564245553554,62.306504874076204,50.195297661646684,54.24708692112095,61.491149240095154,57.712756774429074,55.59989629121224,68.56689739286236,58.745745799535946,61.48476036737154,89.60830321168893,61.19729191630153,62.83453289955066,58.997483707752934,70.93172591223772,55.28782530489431,72.13615477744077,51.301243393909246,47.12561889107727,53.467490207339516,54.49459064488008,45.322867324902674,76.42642115843785,48.66775648198222,60.79022644692237,51.09841538010611,45.65432603044665,52.457790528152884,56.01231675647977,49.326400341824666,61.672855299708125,48.26188302200406,72.49632306157332,54.066365547944514,47.38510570405743,48.10912608028124,45.87868077692408,61.720848457594236,44.077477847520406,49.107702200212664,41.872782406120066,46.51316629238216,53.51004767071263,61.21079542493837,68.10275787010409,49.478095017690904,62.08428269476561,39.58130435718979,62.11213251345752,65.50233196532474,53.5974789256437,53.98243351322408,40.212326781509866,57.513488704877915,50.992485869059486,53.096321889615616,64.23972189344168,51.08003270097247,56.45146257763425,71.45018418402769,56.172067201489405,46.99011245598849,47.514121432056086,57.9907397831025,57.95250032557185,57.677140448769755,48.295571995464,46.798888421001735,53.82699921901174,62.82614964809777,48.2625729138076,42.340470138292545,50.19366431570604,44.187067452883056,41.9021241061093,46.23524710594969,60.2002522485451,47.43973578913216,54.947049846291584,63.84806993600266,50.415067835117874,47.17147681013487,76.20699080496472,42.45129737173864,42.29962534838547,51.305602498671334,57.08088351872554,46.10238797977813,56.02312961376447,48.67252928016776,68.75227849184122,56.08542199415383,58.35010140689414,44.130988678301335,44.09418435893636,53.043444715865405,58.04733672951603,57.887454161481394,52.87499568326798,52.71885253692893,55.754460086799554,77.38819411379683,44.40067149828201,41.868134477192235,60.45116221564465,63.7757481623782,58.27270207797584,60.992303031236915,59.05233661830268,43.79794937263268,70.56310915705615,72.59806086466342,57.349758926087404,56.99217135879335,44.867873961128254,64.97438842446918,55.90232452491329,52.482745752884775,62.716195592890145,53.493898820642826,55.45425379415956,41.99711639617865,53.551330887300786,55.997650788690926,43.521224408211765,56.77514812629626,65.9297084554783,45.96215566245749,49.02760755625352,53.35149938760826,50.9665273156695,46.59456563711551,58.23050436036724,56.40201795616759,51.09002045960827,54.45428183339645,46.81566712920027,43.9999817778016,53.473844539186885,62.087094228166976,48.633139744311045,62.05121071747154,44.16357251916018,68.54054760264685,40.27238623985476,49.6170567100173,71.26264641518861,50.3710441249435,56.22055646726713,45.4329404875777,57.42446421741917,51.39221589632727,44.464222108438165,68.81023306938603,57.14204068818976,45.49478454157438,62.331879710866794,58.596319958121484,53.08982240346269,38.657750784565344,47.62277776061633,49.17268911750469,56.51162287203905,68.39305832213509,64.16765015964864,34.150304615440916,49.413607573822006,50.978247508469266,47.03433344876044,55.618773359354314,50.18795745083572,53.86489982883949,47.736756818356895,49.06375665239787,56.34248213689872,49.045112535569956,48.804367786094566,61.968151244551386,61.302755945830945,49.44428060195067,47.635970834124166,57.061309822618,44.67503044912149,49.842576281754795,59.943100599969036,44.4773169885268,56.51154911857505,57.53444212673711,44.9067678258435,53.342813302933095,72.31198146790206,63.87131935480875,39.202860294548174,45.30176292596078,50.221045068123985,52.38394929802755,54.98722083561695,40.90600655698265,51.41740650192286,49.3507511244127,47.604088527724585,59.70336988325289,45.19475066540726,45.18846980853156,62.10070076364215,55.65053016334941,58.00886312839358,61.08893489511226,55.91837869498991,39.51574843929011,50.48070593000241,60.50824772252758],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"mode\":\"lines+markers\",\"name\":\"Variance\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232],\"y\":[0.1173095703125,0.1175537109375,0.11761474609375,0.1170654296875,0.11810302734375,0.11724853515625,0.11700439453125,0.116943359375,0.1170654296875,0.1177978515625,0.1168212890625,0.11712646484375,0.11834716796875,0.1220703125,0.12408447265625,0.1275634765625,0.13232421875,0.13330078125,0.1396484375,0.13916015625,0.1348876953125,0.1409912109375,0.1416015625,0.14111328125,0.1417236328125,0.1446533203125,0.144775390625,0.147216796875,0.145263671875,0.14794921875,0.1484375,0.1485595703125,0.14892578125,0.150146484375,0.153076171875,0.1556396484375,0.1553955078125,0.1544189453125,0.1600341796875,0.1611328125,0.156982421875,0.160888671875,0.1595458984375,0.16064453125,0.16064453125,0.1580810546875,0.1646728515625,0.1646728515625,0.161865234375,0.164306640625,0.170166015625,0.164306640625,0.167236328125,0.1676025390625,0.172119140625,0.1676025390625,0.1751708984375,0.1705322265625,0.17529296875,0.169677734375,0.169921875,0.1734619140625,0.1737060546875,0.1744384765625,0.1795654296875,0.176513671875,0.1761474609375,0.17626953125,0.1754150390625,0.1810302734375,0.1763916015625,0.175048828125,0.180419921875,0.17919921875,0.1785888671875,0.1767578125,0.178466796875,0.17822265625,0.177001953125,0.1785888671875,0.17431640625,0.178466796875,0.18359375,0.1815185546875,0.182373046875,0.1805419921875,0.1812744140625,0.181396484375,0.181884765625,0.18310546875,0.1832275390625,0.1798095703125,0.187744140625,0.1795654296875,0.184814453125,0.1893310546875,0.1934814453125,0.1942138671875,0.189697265625,0.187744140625,0.1898193359375,0.1875,0.189697265625,0.2000732421875,0.1956787109375,0.1915283203125,0.1944580078125,0.19970703125,0.192626953125,0.1947021484375,0.193359375,0.1988525390625,0.1925048828125,0.192138671875,0.1968994140625,0.2017822265625,0.201171875,0.195556640625,0.2000732421875,0.200927734375,0.2042236328125,0.2005615234375,0.1990966796875,0.203857421875,0.19873046875,0.203125,0.206298828125,0.2039794921875,0.205810546875,0.201416015625,0.19921875,0.20556640625,0.1998291015625,0.20849609375,0.2078857421875,0.208984375,0.2095947265625,0.202880859375,0.212158203125,0.208740234375,0.201904296875,0.213134765625,0.21044921875,0.2060546875,0.2115478515625,0.213623046875,0.2125244140625,0.205322265625,0.2115478515625,0.2093505859375,0.212646484375,0.213134765625,0.21484375,0.2164306640625,0.2158203125,0.220458984375,0.210693359375,0.2093505859375,0.2144775390625,0.2139892578125,0.2120361328125,0.2169189453125,0.2216796875,0.2132568359375,0.2176513671875,0.2149658203125,0.218017578125,0.2117919921875,0.2159423828125,0.213134765625,0.216796875,0.2213134765625,0.2183837890625,0.2176513671875,0.2200927734375,0.2237548828125,0.2191162109375,0.22509765625,0.219970703125,0.2254638671875,0.2276611328125,0.2176513671875,0.2225341796875,0.2261962890625,0.215087890625,0.2337646484375,0.22412109375,0.21630859375,0.2242431640625,0.2254638671875,0.221435546875,0.223388671875,0.2164306640625,0.2275390625,0.2213134765625,0.2255859375,0.222412109375,0.2220458984375,0.2235107421875,0.218017578125,0.2181396484375,0.220947265625,0.219482421875,0.2230224609375,0.2303466796875,0.2230224609375,0.2261962890625,0.219970703125,0.223876953125,0.2259521484375,0.2315673828125,0.227294921875,0.2332763671875,0.2249755859375,0.23046875,0.22314453125,0.230224609375,0.2335205078125,0.2279052734375,0.2333984375,0.2288818359375,0.2330322265625,0.2335205078125,0.2261962890625,0.2218017578125,0.229248046875,0.24267578125,0.23046875,0.2342529296875,0.226318359375,0.240966796875,0.2332763671875,0.2291259765625,0.235595703125,0.23974609375,0.230224609375,0.2451171875,0.2366943359375,0.232421875,0.2449951171875,0.2347412109375,0.2391357421875,0.2327880859375,0.23291015625,0.2322998046875,0.2425537109375,0.2337646484375,0.2388916015625,0.233154296875,0.2330322265625,0.237060546875,0.238037109375,0.238037109375,0.24462890625,0.2320556640625,0.2347412109375,0.2366943359375,0.2261962890625,0.2352294921875,0.24169921875,0.2388916015625,0.2366943359375,0.2349853515625,0.2230224609375,0.2359619140625,0.226318359375,0.2276611328125,0.2291259765625,0.2274169921875,0.2352294921875,0.224365234375,0.2200927734375,0.2264404296875,0.2293701171875,0.2197265625,0.2230224609375,0.2208251953125,0.2264404296875,0.220947265625,0.2174072265625,0.223876953125,0.2293701171875,0.2286376953125,0.229248046875,0.220947265625,0.226806640625,0.2274169921875,0.232421875,0.224853515625,0.23095703125,0.22607421875,0.2181396484375,0.2239990234375,0.2236328125,0.22802734375,0.2139892578125,0.218505859375,0.2275390625,0.220458984375,0.2196044921875,0.220703125,0.2247314453125,0.217529296875,0.223388671875,0.2244873046875,0.2164306640625,0.223876953125,0.209716796875,0.2156982421875,0.2230224609375,0.212646484375,0.2147216796875,0.22021484375,0.221923828125,0.2196044921875,0.2200927734375,0.212158203125,0.2213134765625,0.2135009765625,0.220703125,0.218505859375,0.2208251953125,0.225830078125,0.209716796875,0.2108154296875,0.2135009765625,0.2130126953125,0.213623046875,0.222412109375,0.2139892578125,0.2110595703125,0.2130126953125,0.2093505859375,0.2138671875,0.2110595703125,0.2137451171875,0.210693359375,0.2149658203125,0.202880859375,0.2086181640625,0.210693359375,0.2098388671875,0.2161865234375,0.207275390625,0.2025146484375,0.2000732421875,0.212646484375,0.2091064453125,0.2139892578125,0.2247314453125,0.21484375,0.2039794921875,0.2059326171875,0.206298828125,0.20654296875,0.202880859375,0.2095947265625,0.2054443359375,0.200439453125,0.2105712890625,0.2115478515625,0.1951904296875,0.2030029296875,0.1982421875,0.210205078125,0.202392578125,0.2098388671875,0.204345703125,0.2099609375,0.2059326171875,0.2030029296875,0.2037353515625,0.210205078125,0.211181640625,0.215576171875,0.20361328125,0.20751953125,0.21044921875,0.20849609375,0.1978759765625,0.202880859375,0.2059326171875,0.206787109375,0.2030029296875,0.202392578125,0.206787109375,0.19921875,0.2012939453125,0.2064208984375,0.203369140625,0.2000732421875,0.1981201171875,0.1937255859375,0.2025146484375,0.202880859375,0.2034912109375,0.207275390625,0.19677734375,0.2076416015625,0.2015380859375,0.1962890625,0.19970703125,0.1883544921875,0.1978759765625,0.20166015625,0.2039794921875,0.19287109375,0.1910400390625,0.197998046875,0.18603515625,0.205078125,0.2066650390625,0.197021484375,0.1971435546875,0.1932373046875,0.1988525390625,0.19970703125,0.1943359375,0.2054443359375,0.194091796875,0.1964111328125,0.2010498046875,0.20458984375,0.194091796875,0.195068359375,0.2030029296875,0.1953125,0.2005615234375,0.20166015625,0.2021484375,0.19482421875,0.1968994140625,0.1939697265625,0.192138671875,0.2025146484375,0.1956787109375,0.196533203125,0.199951171875,0.194580078125,0.189208984375,0.1944580078125,0.197265625,0.194091796875,0.1917724609375,0.2015380859375,0.187744140625,0.1866455078125,0.1883544921875,0.1962890625,0.1956787109375,0.189208984375,0.1884765625,0.19189453125,0.18701171875,0.1920166015625,0.1920166015625,0.1944580078125,0.193603515625,0.19580078125,0.19189453125,0.193359375,0.1859130859375,0.1885986328125,0.18310546875,0.1903076171875,0.19287109375,0.184326171875,0.1978759765625,0.1846923828125,0.1947021484375,0.192626953125,0.18896484375,0.1944580078125,0.1978759765625,0.197021484375,0.1864013671875,0.1932373046875,0.197509765625,0.1888427734375,0.1866455078125,0.190185546875,0.1895751953125,0.198974609375,0.193603515625,0.186279296875,0.192626953125,0.187255859375,0.192138671875,0.1795654296875,0.190185546875,0.193359375,0.190673828125,0.18701171875,0.1934814453125,0.1885986328125,0.181640625,0.192626953125,0.1727294921875,0.180908203125,0.1961669921875,0.1810302734375,0.1788330078125,0.1947021484375,0.1845703125,0.1800537109375,0.185791015625,0.1954345703125,0.197998046875,0.1922607421875,0.181640625,0.1890869140625,0.1873779296875,0.18603515625,0.1895751953125,0.1888427734375,0.193115234375,0.1907958984375,0.1817626953125,0.191650390625,0.1873779296875,0.1865234375,0.1915283203125,0.1884765625,0.1798095703125,0.1904296875,0.183837890625,0.1939697265625,0.186767578125,0.187255859375,0.1884765625,0.193359375,0.186279296875,0.1832275390625,0.1929931640625,0.1932373046875,0.1922607421875,0.1922607421875,0.177001953125,0.1888427734375,0.1849365234375,0.1865234375,0.179443359375,0.18359375,0.1890869140625,0.1905517578125,0.1893310546875,0.179931640625,0.1861572265625,0.1871337890625,0.1856689453125,0.186279296875,0.1851806640625,0.1923828125,0.1890869140625,0.18701171875,0.1790771484375,0.1790771484375,0.19140625,0.183837890625,0.1798095703125,0.1796875,0.1787109375,0.1900634765625,0.1846923828125,0.1842041015625,0.1805419921875,0.18212890625,0.193359375,0.18701171875,0.185302734375,0.1884765625,0.189697265625,0.1881103515625,0.1817626953125,0.1815185546875,0.18896484375,0.1866455078125,0.191162109375,0.1826171875,0.1849365234375,0.18017578125,0.181396484375,0.185302734375,0.18359375,0.177734375,0.185546875,0.1832275390625,0.1866455078125,0.1890869140625,0.1849365234375,0.194091796875,0.17822265625,0.185546875,0.186767578125,0.1883544921875,0.1890869140625,0.1796875,0.18994140625,0.18408203125,0.18896484375,0.1865234375,0.190185546875,0.1875,0.179443359375,0.180908203125,0.1834716796875,0.180908203125,0.1842041015625,0.177490234375,0.1839599609375,0.1842041015625,0.183349609375,0.180908203125,0.1787109375,0.1795654296875,0.184814453125,0.1873779296875,0.187744140625,0.1746826171875,0.1763916015625,0.1864013671875,0.1807861328125,0.188232421875,0.1788330078125,0.180908203125,0.1759033203125,0.1851806640625,0.176513671875,0.1800537109375,0.183349609375,0.185546875,0.179931640625,0.1868896484375,0.18701171875,0.184814453125,0.1815185546875,0.1827392578125,0.1705322265625,0.1776123046875,0.181396484375,0.18359375,0.1859130859375,0.18017578125,0.1773681640625,0.19140625,0.1942138671875,0.1866455078125,0.1842041015625,0.1885986328125,0.181884765625,0.1876220703125,0.1783447265625,0.1751708984375,0.1829833984375,0.1788330078125,0.18701171875,0.1839599609375,0.1790771484375,0.182861328125,0.1783447265625,0.1795654296875,0.1851806640625,0.1873779296875,0.1788330078125,0.178955078125,0.17822265625,0.1846923828125,0.1826171875,0.1776123046875,0.181396484375,0.182861328125,0.1832275390625,0.17333984375,0.18310546875,0.178466796875,0.167724609375,0.1702880859375,0.1845703125,0.1817626953125,0.179931640625,0.1683349609375,0.178955078125,0.17822265625,0.180419921875,0.167236328125,0.1768798828125,0.1763916015625,0.185546875,0.170654296875,0.181640625,0.1748046875,0.1766357421875,0.1800537109375,0.1851806640625,0.1708984375,0.180908203125,0.17822265625,0.1710205078125,0.1722412109375,0.17529296875,0.1728515625,0.1776123046875,0.1771240234375,0.176025390625,0.1734619140625,0.1727294921875,0.17138671875,0.1817626953125,0.1805419921875,0.1695556640625,0.172607421875,0.1739501953125,0.1693115234375,0.17236328125,0.1754150390625,0.160888671875,0.17041015625,0.177001953125,0.1668701171875,0.1844482421875,0.1749267578125,0.16455078125,0.1728515625,0.181884765625,0.17431640625,0.1756591796875,0.17333984375,0.1712646484375,0.1756591796875,0.1663818359375,0.1754150390625,0.1683349609375,0.1693115234375,0.16162109375,0.17333984375,0.170654296875,0.1705322265625,0.17822265625,0.165771484375,0.16552734375,0.1671142578125,0.1741943359375,0.1697998046875,0.1636962890625,0.16845703125,0.163818359375,0.1728515625,0.17041015625,0.16455078125,0.168212890625,0.1669921875,0.1695556640625,0.1732177734375,0.1676025390625,0.167236328125,0.1700439453125,0.1741943359375,0.16162109375,0.169921875,0.1710205078125,0.1715087890625,0.1793212890625,0.170654296875,0.1614990234375,0.175048828125,0.1763916015625,0.162841796875,0.160400390625,0.16796875,0.173828125,0.1700439453125,0.1617431640625,0.1611328125,0.1552734375,0.166259765625,0.165283203125,0.1702880859375,0.1712646484375,0.171630859375,0.161865234375,0.16162109375,0.1629638671875,0.167724609375,0.16943359375,0.1611328125,0.1708984375,0.177734375,0.1661376953125,0.1724853515625,0.162353515625,0.16943359375,0.17431640625,0.168701171875,0.171630859375,0.162353515625,0.158935546875,0.165283203125,0.161376953125,0.163330078125,0.1619873046875,0.1685791015625,0.161865234375,0.163818359375,0.1602783203125,0.16162109375,0.16357421875,0.1658935546875,0.160888671875,0.1658935546875,0.16259765625,0.1630859375,0.1668701171875,0.1661376953125,0.1580810546875,0.1717529296875,0.16064453125,0.1568603515625,0.1724853515625,0.175537109375,0.1632080078125,0.1646728515625,0.1527099609375,0.171142578125,0.1666259765625,0.1575927734375,0.1580810546875,0.161376953125,0.1656494140625,0.16357421875,0.1634521484375,0.1671142578125,0.157958984375,0.1602783203125,0.171142578125,0.1715087890625,0.162841796875,0.15673828125,0.1610107421875,0.1671142578125,0.1658935546875,0.1513671875,0.1622314453125,0.1702880859375,0.1712646484375,0.168212890625,0.1611328125,0.1719970703125,0.16650390625,0.1611328125,0.1611328125,0.1541748046875,0.1580810546875,0.1640625,0.1619873046875,0.1629638671875,0.169677734375,0.1639404296875,0.1646728515625,0.1558837890625,0.1546630859375,0.16357421875,0.1668701171875,0.16015625,0.15576171875,0.1614990234375,0.159423828125,0.1654052734375,0.1641845703125,0.154541015625,0.161376953125,0.1636962890625,0.1651611328125,0.160888671875,0.1617431640625,0.1610107421875,0.1622314453125,0.163818359375,0.15478515625,0.1629638671875,0.161865234375,0.1614990234375,0.1617431640625,0.162353515625,0.1630859375,0.1583251953125,0.167724609375,0.16796875,0.15478515625,0.153076171875,0.1563720703125,0.154296875,0.15576171875,0.1673583984375,0.162353515625,0.1663818359375,0.156494140625,0.1649169921875,0.1585693359375,0.1639404296875,0.155517578125,0.161865234375,0.1529541015625,0.161376953125,0.1583251953125,0.1658935546875,0.1610107421875,0.156494140625,0.155517578125,0.158203125,0.1588134765625,0.1561279296875,0.1534423828125,0.16796875,0.162109375,0.1558837890625,0.1634521484375,0.159423828125,0.157470703125,0.1534423828125,0.165283203125,0.15771484375,0.150634765625,0.15771484375,0.158935546875,0.15380859375,0.1649169921875,0.1572265625,0.1572265625,0.1678466796875,0.1600341796875,0.14892578125,0.1517333984375,0.1583251953125,0.158935546875,0.15185546875,0.15576171875,0.150634765625,0.16455078125,0.156494140625,0.15771484375,0.152587890625,0.162841796875,0.1583251953125,0.15478515625,0.1558837890625,0.170166015625,0.1522216796875,0.1693115234375,0.1650390625,0.1629638671875,0.1588134765625,0.15771484375,0.169677734375,0.1549072265625,0.1671142578125,0.1669921875,0.1689453125,0.148681640625,0.1539306640625,0.1534423828125,0.1612548828125,0.1588134765625,0.15869140625,0.159423828125,0.155029296875,0.1539306640625,0.1551513671875,0.1500244140625,0.1524658203125,0.1580810546875,0.1666259765625,0.15478515625,0.1571044921875,0.1614990234375,0.1558837890625,0.161376953125,0.1568603515625,0.159912109375,0.1651611328125,0.1595458984375,0.1566162109375,0.150146484375,0.1529541015625,0.14599609375,0.1556396484375,0.1602783203125,0.1492919921875,0.1546630859375,0.1558837890625,0.154052734375,0.1512451171875,0.154052734375,0.1484375,0.145751953125,0.1650390625,0.1627197265625,0.1431884765625,0.151611328125,0.158203125,0.1593017578125,0.15625,0.1531982421875,0.152587890625,0.1536865234375,0.15478515625,0.1573486328125,0.1461181640625,0.1563720703125,0.152587890625,0.1502685546875,0.1551513671875,0.1627197265625,0.163330078125,0.158935546875,0.150634765625,0.1632080078125,0.1552734375,0.1529541015625,0.1553955078125,0.14990234375,0.1585693359375,0.158447265625,0.1488037109375,0.14794921875,0.1650390625,0.1551513671875,0.1595458984375,0.153076171875,0.1512451171875,0.1529541015625,0.1513671875,0.1507568359375,0.1558837890625,0.1492919921875,0.1500244140625,0.14990234375,0.1474609375,0.1546630859375,0.1549072265625,0.158203125,0.150146484375,0.153564453125,0.1563720703125,0.156982421875,0.153076171875,0.1524658203125,0.152099609375,0.1595458984375,0.1512451171875,0.15478515625,0.1571044921875,0.1605224609375,0.1561279296875,0.15478515625,0.147705078125,0.1553955078125,0.1473388671875,0.157958984375,0.150390625,0.156005859375,0.152099609375,0.15673828125,0.1552734375,0.157470703125,0.1571044921875,0.141357421875,0.1578369140625,0.15234375,0.1546630859375,0.1551513671875,0.1533203125,0.156005859375,0.1546630859375,0.1571044921875,0.14599609375,0.146728515625,0.148681640625,0.1488037109375,0.1561279296875,0.145263671875,0.1492919921875,0.1475830078125,0.151611328125,0.1519775390625,0.1414794921875,0.146484375,0.15478515625,0.1575927734375,0.1568603515625,0.152099609375,0.1485595703125,0.1517333984375,0.1524658203125,0.157958984375,0.14794921875,0.148681640625,0.1546630859375,0.1475830078125,0.1468505859375,0.1505126953125,0.1494140625,0.1533203125,0.15380859375,0.145751953125,0.15625,0.154296875,0.15576171875,0.1434326171875,0.1568603515625,0.1474609375,0.1524658203125,0.1571044921875,0.1531982421875,0.1453857421875,0.1531982421875,0.1468505859375,0.1485595703125,0.1524658203125,0.1529541015625,0.15625,0.15283203125,0.1505126953125,0.13916015625,0.1500244140625,0.1512451171875,0.1475830078125,0.1495361328125,0.149169921875,0.1539306640625,0.1456298828125,0.1534423828125,0.14599609375,0.1475830078125,0.1566162109375,0.14990234375,0.15185546875,0.1484375,0.1553955078125,0.1514892578125,0.1497802734375,0.1510009765625,0.151123046875,0.1546630859375,0.1527099609375,0.149658203125,0.1522216796875,0.1436767578125,0.1455078125,0.15380859375,0.1483154296875,0.154052734375,0.14404296875,0.1448974609375,0.1419677734375,0.151123046875,0.1480712890625,0.1444091796875,0.150146484375,0.1536865234375,0.151123046875,0.1480712890625,0.1527099609375,0.1568603515625,0.149169921875,0.151611328125,0.151611328125,0.1490478515625,0.1484375,0.1497802734375,0.1505126953125,0.1470947265625,0.14697265625,0.15478515625,0.152099609375,0.153076171875,0.1484375,0.15771484375,0.143310546875,0.14697265625,0.146484375,0.158447265625,0.1444091796875,0.1505126953125,0.1463623046875,0.1578369140625,0.1553955078125,0.144287109375,0.1539306640625,0.15673828125,0.1446533203125,0.1485595703125,0.142333984375,0.1541748046875,0.14794921875,0.1458740234375,0.1458740234375,0.1473388671875,0.14697265625,0.143798828125,0.1488037109375,0.1480712890625,0.1446533203125,0.14111328125,0.149169921875,0.1448974609375,0.142333984375,0.1455078125,0.145751953125,0.14208984375,0.1451416015625,0.15380859375,0.138427734375,0.1497802734375,0.1455078125,0.14599609375,0.143798828125,0.143798828125,0.1461181640625,0.14794921875,0.14111328125,0.1572265625,0.140869140625,0.1365966796875,0.142578125,0.1500244140625,0.13720703125,0.14599609375,0.1455078125,0.1419677734375,0.1436767578125,0.1473388671875,0.143310546875,0.145263671875],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"mode\":\"lines+markers\",\"name\":\"Learning Rate\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232],\"y\":[0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"mode\":\"lines+markers\",\"name\":\"Dev STS Pearson Cosine\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232],\"y\":[0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7037153817934473],\"type\":\"scatter\",\"xaxis\":\"x5\",\"yaxis\":\"y5\"},{\"mode\":\"lines+markers\",\"name\":\"Dev STS Spearman Cosine\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232],\"y\":[0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.726539065588645],\"type\":\"scatter\",\"xaxis\":\"x5\",\"yaxis\":\"y5\"},{\"mode\":\"lines+markers\",\"name\":\"Test STS Pearson Cosine\",\"x\":[1232],\"y\":[0.6272570770065017],\"type\":\"scatter\",\"xaxis\":\"x6\",\"yaxis\":\"y6\"},{\"mode\":\"lines+markers\",\"name\":\"Test STS Spearman Cosine\",\"x\":[1232],\"y\":[0.6257119647399907],\"type\":\"scatter\",\"xaxis\":\"x6\",\"yaxis\":\"y6\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"autorange\":true},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.7777777777777778,1.0],\"autorange\":true},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"autorange\":true},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.7777777777777778,1.0],\"autorange\":true},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,0.45],\"autorange\":true},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.3888888888888889,0.6111111111111112],\"autorange\":true},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.55,1.0],\"autorange\":true},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.3888888888888889,0.6111111111111112],\"autorange\":true},\"xaxis5\":{\"anchor\":\"y5\",\"domain\":[0.0,0.45],\"autorange\":true},\"yaxis5\":{\"anchor\":\"x5\",\"domain\":[0.0,0.22222222222222224],\"autorange\":true},\"xaxis6\":{\"anchor\":\"y6\",\"domain\":[0.55,1.0],\"autorange\":true},\"yaxis6\":{\"anchor\":\"x6\",\"domain\":[0.0,0.22222222222222224],\"autorange\":true},\"annotations\":[{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=distilbert-base-uncased, batch_size=32, projection_depth=4, projection_size=6144, epochs=1, warmup_proportion=0.0, max_seq_length=64, aug_p=0.3,\\u003cbr\\u003elearning_rate=0.0001, model_save_path=train_stsb_bt-distilbert-2025-03-09_04-42-49, num_workers=2, weight_decay=1e-05, lambda_bt=0.0051, lambda_mixup=0.8,\\u003cbr\\u003euse_amp=True, patience=500, Augmenters: Synonym_Aug[substitute:0.3], RandomWord_Aug[swap:0.3], RandomWord_Aug[delete:0.3]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=distilbert-base-uncased, batch_size=32, projection_depth=4, projection_size=6144, epochs=1, warmup_proportion=0.0, max_seq_length=64, aug_p=0.3,\\u003cbr\\u003elearning_rate=0.0001, model_save_path=train_stsb_bt-distilbert-2025-03-09_04-42-49, num_workers=2, weight_decay=1e-05, lambda_bt=0.0051, lambda_mixup=0.8,\\u003cbr\\u003euse_amp=True, patience=500, Augmenters: Synonym_Aug[substitute:0.3], RandomWord_Aug[swap:0.3], RandomWord_Aug[delete:0.3]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=distilbert-base-uncased, batch_size=32, projection_depth=4, projection_size=6144, epochs=1, warmup_proportion=0.0, max_seq_length=64, aug_p=0.3,\\u003cbr\\u003elearning_rate=0.0001, model_save_path=train_stsb_bt-distilbert-2025-03-09_04-42-49, num_workers=2, weight_decay=1e-05, lambda_bt=0.0051, lambda_mixup=0.8,\\u003cbr\\u003euse_amp=True, patience=500, Augmenters: Synonym_Aug[substitute:0.3], RandomWord_Aug[swap:0.3], RandomWord_Aug[delete:0.3]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=distilbert-base-uncased, batch_size=32, projection_depth=4, projection_size=6144, epochs=1, warmup_proportion=0.0, max_seq_length=64, aug_p=0.3,\\u003cbr\\u003elearning_rate=0.0001, model_save_path=train_stsb_bt-distilbert-2025-03-09_04-42-49, num_workers=2, weight_decay=1e-05, lambda_bt=0.0051, lambda_mixup=0.8,\\u003cbr\\u003euse_amp=True, patience=500, Augmenters: Synonym_Aug[substitute:0.3], RandomWord_Aug[swap:0.3], RandomWord_Aug[delete:0.3]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=distilbert-base-uncased, batch_size=32, projection_depth=4, projection_size=6144, epochs=1, warmup_proportion=0.0, max_seq_length=64, aug_p=0.3,\\u003cbr\\u003elearning_rate=0.0001, model_save_path=train_stsb_bt-distilbert-2025-03-09_04-42-49, num_workers=2, weight_decay=1e-05, lambda_bt=0.0051, lambda_mixup=0.8,\\u003cbr\\u003euse_amp=True, patience=500, Augmenters: Synonym_Aug[substitute:0.3], RandomWord_Aug[swap:0.3], RandomWord_Aug[delete:0.3]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=distilbert-base-uncased, batch_size=32, projection_depth=4, projection_size=6144, epochs=1, warmup_proportion=0.0, max_seq_length=64, aug_p=0.3,\\u003cbr\\u003elearning_rate=0.0001, model_save_path=train_stsb_bt-distilbert-2025-03-09_04-42-49, num_workers=2, weight_decay=1e-05, lambda_bt=0.0051, lambda_mixup=0.8,\\u003cbr\\u003euse_amp=True, patience=500, Augmenters: Synonym_Aug[substitute:0.3], RandomWord_Aug[swap:0.3], RandomWord_Aug[delete:0.3]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"right\",\"font\":{\"color\":\"blue\",\"size\":12},\"showarrow\":false,\"text\":\"Best Spearman: 0.7417\\u003cbr\\u003eBest Pearson: 0.7261\",\"x\":1.0,\"xanchor\":\"right\",\"xref\":\"paper\",\"y\":0.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Training Metrics\"},\"margin\":{\"l\":50,\"r\":50,\"t\":100,\"b\":150},\"width\":1200,\"height\":800,\"showlegend\":true,\"sliders\":[{\"active\":0,\"currentvalue\":{\"prefix\":\"Epoch: \"},\"pad\":{\"t\":50},\"steps\":[{\"args\":[[\"0\"],{\"frame\":{\"duration\":0,\"redraw\":true},\"mode\":\"immediate\",\"transition\":{\"duration\":0}}],\"label\":\"0\",\"method\":\"animate\"}]}]},                        {\"toImageButtonOptions\": {\"filename\": \"train_stsb_bt-distilbert-2025-03-09_04-42-49\", \"format\": \"png\", \"width\": 1200, \"height\": 800, \"scale\": 1}, \"responsive\": true}                    ).then(function(){\n",
              "                            Plotly.addFrames('f238a3ba-b8bf-47f8-87f2-5c06438e2f2f', [{\"data\":[{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232],\"y\":[8041.51025390625,7232.7744140625,8159.3759765625,8302.6630859375,9230.3212890625,7842.63134765625,8334.623046875,8301.26171875,8591.384765625,7858.5068359375,8573.404296875,7841.314453125,8254.32421875,7856.70458984375,7835.4033203125,7540.98046875,8121.830078125,7864.88671875,7362.29541015625,7080.77880859375,7785.5380859375,8540.955078125,7735.212890625,8338.8740234375,7763.087890625,8035.83740234375,7479.54931640625,7552.02001953125,8185.86767578125,7943.72607421875,7168.22998046875,8236.6416015625,7580.77587890625,7295.30126953125,7136.51708984375,6896.0390625,7784.55517578125,7958.98828125,8125.58740234375,8189.11328125,8516.1787109375,7311.1044921875,7523.1611328125,7419.93603515625,7888.81494140625,7893.03076171875,7705.0126953125,7452.40185546875,8379.5869140625,7806.25390625,7750.64453125,7367.26708984375,7594.306640625,7284.154296875,7216.73583984375,7877.640625,7343.05517578125,7087.359375,7153.2939453125,7028.72265625,7188.78076171875,7720.955078125,7671.61181640625,7145.4814453125,7095.0732421875,7473.431640625,7734.8603515625,8358.9580078125,7277.822265625,7659.3369140625,7504.27099609375,7766.291015625,7392.69140625,7320.74169921875,6938.69091796875,7847.880859375,7426.03125,7324.19775390625,7064.5263671875,7372.26806640625,7628.69677734375,7516.0126953125,7837.29833984375,7352.587890625,7987.72509765625,7299.5244140625,8147.96142578125,7612.39697265625,7844.89990234375,8005.078125,8432.515625,8397.080078125,7170.68310546875,8199.7939453125,7710.6123046875,8418.2666015625,8037.251953125,8071.8642578125,7435.23291015625,7769.59765625,7983.66259765625,7936.02392578125,8337.6318359375,7724.19873046875,7713.52880859375,7141.79931640625,6954.50439453125,7328.6669921875,7684.6748046875,7551.693359375,7978.4111328125,7088.9873046875,8280.96875,8002.91552734375,7415.181640625,7825.41650390625,6993.84423828125,8118.841796875,8279.9716796875,7771.658203125,7994.26806640625,7644.376953125,7319.90771484375,7875.4169921875,8036.18994140625,8132.13134765625,7659.8408203125,7521.00244140625,7681.61083984375,7586.24609375,7269.6318359375,7785.7841796875,8413.6728515625,7029.58642578125,7192.43310546875,8104.556640625,7346.9306640625,8106.853515625,7178.27783203125,7259.9677734375,7121.6767578125,8249.1962890625,8098.912109375,7596.30419921875,7538.02099609375,7255.5625,7358.751953125,7453.603515625,8178.06103515625,8001.86376953125,8046.86767578125,6923.5595703125,7881.37939453125,7780.04345703125,7139.4150390625,7358.287109375,7878.02685546875,7412.16357421875,7407.244140625,7968.283203125,7619.24072265625,7093.59326171875,7773.22021484375,7990.03173828125,7738.845703125,7156.060546875,7358.951171875,7365.25927734375,7655.24609375,8153.81201171875,8080.83056640625,7839.74462890625,7268.09912109375,7779.31103515625,7988.9619140625,7944.951171875,7946.64892578125,7608.01513671875,7223.6181640625,7583.81982421875,7182.15380859375,7713.00341796875,7771.21044921875,7391.15478515625,7345.90185546875,7387.91015625,6954.9970703125,7493.3271484375,7831.82763671875,8027.3447265625,6923.3837890625,7979.755859375,8067.91357421875,7933.49560546875,8156.24755859375,7546.0322265625,7536.55712890625,7354.74609375,7783.3427734375,7727.70751953125,8025.7646484375,7221.03125,7228.1884765625,7465.3837890625,8017.01318359375,7223.13720703125,7423.77392578125,8167.27978515625,8004.578125,7856.630859375,7792.24072265625,7386.88916015625,7220.396484375,7204.595703125,8283.9609375,8042.740234375,7473.65576171875,8699.3642578125,7849.39501953125,7308.4404296875,7449.19482421875,7930.50146484375,7872.96142578125,7518.55419921875,7588.71435546875,8174.8994140625,7800.99365234375,8077.33984375,7946.359375,8542.552734375,7456.751953125,7768.87353515625,7699.765625,7965.6689453125,7380.85205078125,7991.5625,7743.90185546875,7864.2919921875,7953.63720703125,7976.7763671875,8016.77001953125,7546.19189453125,7972.6806640625,7897.95166015625,8323.0693359375,7642.31298828125,8267.625,7063.33251953125,7913.77978515625,7934.05517578125,7274.53271484375,7441.52880859375,7081.56689453125,7359.521484375,7029.44921875,7126.609375,7857.59619140625,8034.4033203125,7043.01806640625,7202.546875,7162.755859375,7822.9345703125,7726.96533203125,7859.919921875,7690.97216796875,7255.94873046875,7749.6796875,7832.0,6831.0712890625,7751.95068359375,7372.73486328125,7778.92138671875,7864.759765625,8107.47265625,7271.853515625,7543.63916015625,8262.4736328125,7794.2041015625,7423.1103515625,7906.89208984375,7878.4189453125,7207.5849609375,6977.5048828125,7165.0810546875,7481.53759765625,6981.3095703125,7968.4111328125,6754.4462890625,8057.3623046875,7984.1708984375,7051.49169921875,6815.095703125,6955.69384765625,7671.5712890625,7723.53076171875,7474.22705078125,7044.72607421875,7179.654296875,7618.4091796875,7097.83544921875,7961.36865234375,7867.9951171875,7585.5302734375,7934.92919921875,7634.8671875,6872.6611328125,6867.1259765625,7176.40576171875,7295.28271484375,6926.02685546875,8167.42822265625,7813.06689453125,7732.2998046875,7694.6318359375,7076.3291015625,7968.88916015625,7111.21044921875,7141.46240234375,7883.11865234375,7494.4990234375,6938.6123046875,6974.29296875,7904.6123046875,7267.47900390625,6989.03662109375,8081.818359375,6600.9228515625,7792.943359375,7489.3466796875,7338.36279296875,6827.84228515625,7630.66943359375,7838.47607421875,7392.82470703125,7176.5830078125,6837.67626953125,7161.27294921875,7360.5244140625,6814.97412109375,7784.21435546875,6960.1455078125,7100.0,7684.82275390625,7821.05029296875,7351.3759765625,7033.38671875,7687.80224609375,7827.3359375,7683.4912109375,7070.7451171875,7551.00390625,7179.65966796875,6750.03076171875,7409.15478515625,7960.6689453125,8202.3603515625,7701.39111328125,7585.4404296875,7171.05615234375,6987.92138671875,7184.60693359375,7867.46240234375,6890.31787109375,8063.947265625,7697.0390625,7413.09033203125,7454.9072265625,7112.26416015625,7115.2724609375,7475.16015625,7479.24658203125,7745.08837890625,7191.3212890625,7637.5458984375,6854.80078125,7260.58740234375,6730.65966796875,7113.5361328125,6763.32275390625,6895.658203125,7212.0390625,7236.939453125,7455.638671875,7098.86083984375,7240.876953125,6789.55810546875,7518.1611328125,7708.8583984375,6785.03369140625,7199.66162109375,7786.802734375,6981.23583984375,6797.662109375,7643.30126953125,6905.33984375,7513.1943359375,6993.12109375,6839.85498046875,7158.36328125,6687.375,6939.677734375,7740.07568359375,7980.26611328125,6699.33154296875,6741.6591796875,7116.490234375,7393.0439453125,7588.8857421875,7229.67822265625,7696.517578125,7696.046875,7457.67431640625,7006.9521484375,7023.83447265625,6850.34033203125,7419.607421875,6871.24853515625,7743.14892578125,6954.62158203125,7764.34326171875,7337.61474609375,7535.9189453125,6718.2841796875,7021.36962890625,7653.4580078125,7460.076171875,7797.21337890625,6930.537109375,7396.06103515625,7441.65625,7162.43798828125,7091.8583984375,6833.32080078125,7287.013671875,7211.9228515625,7554.904296875,7268.46630859375,6979.30224609375,6831.65576171875,7630.93017578125,7390.27392578125,6868.564453125,7021.09423828125,7247.6669921875,6796.8056640625,7958.1220703125,7424.0361328125,6712.6474609375,6805.38134765625,6477.15234375,7110.46923828125,7894.4462890625,7044.0986328125,6850.88720703125,7694.9189453125,7189.75390625,7762.82666015625,6866.8486328125,7689.4208984375,7603.7919921875,7089.9052734375,6871.2177734375,7478.5380859375,6632.25048828125,6687.6728515625,6889.9462890625,7589.3828125,7289.78466796875,7593.201171875,6894.439453125,6955.9609375,6571.7548828125,6910.7744140625,7500.4638671875,7226.193359375,6999.02001953125,7283.8798828125,6663.599609375,6989.2138671875,7208.18017578125,6602.06640625,6940.494140625,7736.93212890625,6703.29345703125,6921.04296875,6950.8486328125,7443.9658203125,7896.08349609375,7965.73291015625,7123.9619140625,6803.6494140625,6661.64404296875,7994.1083984375,7637.80615234375,6876.36328125,7482.37060546875,7511.42431640625,7416.72607421875,7564.4091796875,7555.01904296875,7494.3740234375,7548.8388671875,6775.591796875,7065.4755859375,6996.181640625,6512.28759765625,6640.36181640625,7757.47216796875,7079.79638671875,7051.990234375,7118.576171875,7699.66064453125,7018.7734375,6677.88037109375,7697.5263671875,6632.83544921875,6975.32470703125,7020.39892578125,6867.4052734375,6995.70166015625,7796.8896484375,6745.427734375,7671.81494140625,7263.81982421875,6917.81787109375,7501.42822265625,7270.08935546875,7614.2177734375,6988.7314453125,7361.32861328125,7071.87353515625,6824.98974609375,6882.30322265625,7496.96875,7168.0390625,7517.154296875,6678.20166015625,7337.65087890625,7056.142578125,7412.7724609375,7915.939453125,6888.78759765625,7110.52001953125,7492.033203125,7109.70166015625,6837.79345703125,6855.91162109375,7418.208984375,7247.169921875,7488.759765625,7070.30517578125,7505.32861328125,7173.78857421875,7201.23388671875,7216.3505859375,7580.04248046875,7037.99755859375,7228.98095703125,7199.1650390625,7200.9306640625,6994.33935546875,7607.4716796875,7478.013671875,7506.25537109375,7389.791015625,7384.5419921875,6786.162109375,7448.9423828125,7644.09765625,7123.2275390625,7294.2666015625,7316.79541015625,7690.63818359375,7102.0302734375,6939.18994140625,7630.9248046875,6851.17529296875,7241.92578125,7821.00244140625,7430.17138671875,7194.3720703125,6976.0888671875,7015.6630859375,7548.251953125,6817.71435546875,7521.931640625,7401.83837890625,7097.93408203125,7258.94091796875,7216.99462890625,7213.2607421875,7450.55712890625,6653.119140625,7334.1630859375,7199.79345703125,7720.4326171875,7020.33349609375,6700.57861328125,6801.4482421875,6841.52783203125,7382.658203125,7091.4990234375,7481.84765625,6809.5693359375,6956.5439453125,6704.57470703125,6844.38720703125,6934.29345703125,7669.51611328125,7717.94580078125,6931.16845703125,7584.98291015625,7303.92578125,7181.28271484375,7268.1201171875,6818.41796875,7538.16552734375,7344.96142578125,7112.1865234375,7059.185546875,7349.72900390625,6814.99365234375,6500.6748046875,6683.88037109375,7866.07958984375,7696.97998046875,6783.56201171875,7640.9384765625,6972.07373046875,7566.59521484375,7190.70947265625,7037.1181640625,6693.0185546875,7362.68359375,7026.4375,7442.86474609375,6671.6123046875,7105.7412109375,6854.06787109375,6833.87451171875,7057.82763671875,7413.2294921875,7191.77783203125,7127.14013671875,6868.46142578125,7148.583984375,7327.41796875,7148.88134765625,6926.01171875,6607.10546875,7824.60009765625,7869.5078125,7486.56787109375,6881.7080078125,6616.82763671875,6869.119140625,7387.85302734375,7288.27392578125,7336.00537109375,7651.77392578125,7542.0439453125,6853.201171875,6750.07958984375,7662.5185546875,7275.5693359375,7317.76611328125,7421.943359375,7409.275390625,6599.48388671875,7186.2001953125,6743.03369140625,7445.0703125,6811.70361328125,6464.4521484375,7206.87451171875,7562.8125,6683.11376953125,7100.33203125,7112.5068359375,6549.60546875,6965.95654296875,6543.240234375,6686.17236328125,6609.73046875,7100.162109375,6808.1640625,7037.6826171875,7576.21875,6452.572265625,6549.67041015625,7664.71875,7609.607421875,7209.759765625,7423.15234375,6725.43359375,6746.5166015625,6788.1318359375,7235.416015625,7638.69482421875,6849.8935546875,6868.30126953125,6615.2978515625,7450.341796875,7036.6708984375,7095.12744140625,7539.267578125,7378.04345703125,6955.33544921875,6569.95166015625,7805.55859375,7615.23486328125,7350.4287109375,7909.9072265625,6791.9228515625,7226.853515625,7591.50634765625,6745.47021484375,7347.5595703125,7481.1875,7376.04345703125,6641.1875,7437.7021484375,7089.22900390625,6438.3720703125,7228.99609375,6512.294921875,6732.20751953125,6984.302734375,7425.72802734375,7027.396484375,6763.591796875,7201.4970703125,6566.61083984375,6846.107421875,7483.16357421875,6833.7900390625,7225.05615234375,6673.7099609375,7344.4326171875,7232.111328125,7427.3759765625,7249.2783203125,6975.29345703125,6645.1005859375,6946.96826171875,7395.04931640625,7382.67626953125,7133.2578125,7173.42578125,7122.408203125,7528.86328125,6720.0634765625,7102.42236328125,6854.6943359375,7650.5634765625,7306.15673828125,6769.83642578125,7377.13232421875,7320.15625,7231.4228515625,6787.078125,7227.57470703125,7007.3203125,7700.140625,7051.3037109375,7322.767578125,7649.60595703125,6926.6416015625,6977.421875,7046.369140625,7226.7265625,6970.9580078125,7111.486328125,6712.1591796875,7202.89111328125,6741.18798828125,6540.4912109375,6914.9560546875,6746.5283203125,6871.22900390625,7588.44873046875,7253.14794921875,6917.21044921875,7272.41748046875,6885.123046875,7219.71484375,7159.49853515625,6993.345703125,6685.54248046875,6659.83935546875,6794.40087890625,6526.01953125,6451.861328125,7307.70751953125,6838.62451171875,6968.16796875,6945.7841796875,7121.3193359375,6657.298828125,7165.15185546875,7172.45703125,7011.93603515625,7208.09814453125,7129.5400390625,6710.880859375,7326.28662109375,7582.1171875,7398.2021484375,6679.69287109375,7243.3798828125,7448.00732421875,6789.77734375,7071.7783203125,6542.5703125,6662.48583984375,7096.2109375,6751.251953125,7197.6689453125,7588.62060546875,6705.31640625,7905.57958984375,7679.3759765625,7352.40625,7495.70654296875,6779.1640625,6527.4423828125,7361.4677734375,7097.9599609375,6833.48193359375,6518.568359375,7146.48291015625,7290.9384765625,6932.18310546875,6868.27099609375,6651.2490234375,7077.51025390625,7312.88330078125,6729.3388671875,6760.45556640625,6564.43603515625,7179.24462890625,6737.78076171875,6918.7333984375,6986.32861328125,6996.119140625,6833.27685546875,7006.8974609375,6650.63037109375,6742.599609375,7094.61669921875,7342.82861328125,7301.236328125,7141.0634765625,6781.9755859375,7682.6904296875,6712.416015625,7069.7392578125,6664.21484375,7386.84326171875,7410.146484375,6876.4248046875,7328.248046875,6921.5673828125,6946.185546875,7323.142578125,7026.0224609375,7355.5166015625,6853.1806640625,7419.59912109375,7079.91650390625,6457.42919921875,7506.29296875,7117.49169921875,6726.4609375,7266.88232421875,7240.21337890625,6384.21728515625,6692.73779296875,6952.73779296875,6920.8095703125,6871.5771484375,6626.48583984375,7585.90185546875,6863.27685546875,7046.91943359375,6875.42236328125,7286.068359375,6555.47705078125,7314.39453125,6522.15625,6930.39013671875,6904.3232421875,7043.71923828125,6591.83203125,7096.9697265625,7348.9111328125,6550.9267578125,7376.13134765625,7552.51318359375,7750.27001953125,6542.2138671875,7880.16455078125,7448.52490234375,7224.24951171875,7009.00830078125,6869.1318359375,7136.2734375,6852.75634765625,6881.20263671875,7652.990234375,6638.03515625,7761.744140625,6953.3095703125,6595.27197265625,6941.6123046875,7053.30322265625,7489.77978515625,7071.6953125,7241.2412109375,7427.37548828125,7389.8857421875,7661.14599609375,7026.29443359375,7362.876953125,7168.03759765625,7417.0966796875,6554.54443359375,7254.919921875,7573.1240234375,6738.44775390625,7080.82275390625,7230.76171875,7088.10693359375,7483.169921875,7335.79248046875,6990.84423828125,6621.8740234375,6673.25830078125,7085.7861328125,7126.42822265625,7675.4091796875,7046.72119140625,6609.6083984375,7651.255859375,6672.46337890625,6709.07861328125,7569.9814453125,7257.96435546875,7140.5986328125,7412.736328125,6725.43603515625,7334.57763671875,6249.080078125,6889.06689453125,6484.82373046875,7373.6015625,6345.4921875,6942.76416015625,7436.1826171875,7632.3623046875,7083.34814453125,7468.8642578125,7241.9462890625,7528.5615234375,6931.5107421875,7006.1611328125,6760.91162109375,6615.9892578125,7288.8330078125,7466.61328125,6359.18115234375,7040.02587890625,6717.25390625,7287.6904296875,7042.9189453125,6546.19189453125,7552.26123046875,6667.26708984375,6776.69091796875,6523.68408203125,7492.1513671875,7069.1630859375,6871.97314453125,7219.7685546875,6634.6845703125,6709.7275390625,6944.24365234375,6641.59765625,6854.8828125,6929.7734375,7078.923828125,6660.3076171875,7527.9599609375,7772.55810546875,6494.8203125,7432.87646484375,7199.59765625,6755.33935546875,7434.111328125,6388.67919921875,7547.13134765625,6842.5908203125,7481.3955078125,6555.736328125,7185.3408203125,7231.912109375,6548.96484375,6411.8955078125,7229.09326171875,6598.564453125,6810.71484375,7014.89990234375,6920.24609375,6383.5859375,7372.5703125,6810.64892578125,7539.58544921875,7325.8212890625,7247.03369140625,7550.6591796875,7235.74462890625,6973.4521484375,7020.1552734375,6681.3623046875,6764.6318359375,7380.0634765625,6899.76953125,7355.72119140625,7567.33642578125,7282.841796875,7683.6572265625,7203.66552734375,6750.9736328125,7206.505859375,7368.8349609375,7609.4384765625,6711.59228515625,7682.57470703125,7044.892578125,6489.8642578125,7269.015625,6641.25244140625,6775.8134765625,7584.2265625,6575.7373046875,6806.83349609375,7342.48388671875,6342.0458984375,6711.02734375,6825.779296875,6954.3193359375,7626.6806640625,7100.59521484375,6917.1875,7155.9599609375,6576.21484375,7124.759765625,6807.65380859375,7390.74365234375,6430.58935546875,6479.71923828125,6363.09716796875,7061.32666015625,7348.3828125,6795.10302734375,7170.84912109375,6991.6904296875,7635.904296875,6468.7890625,6654.76708984375,7333.009765625,7352.59521484375,6641.61474609375,6795.9716796875,6725.19921875,6838.7119140625,6871.51953125,6780.7607421875,7074.6240234375,6601.57958984375,7811.89208984375,7335.56298828125,6647.86767578125,6429.26611328125,7364.015625,6895.4326171875,7279.29296875,6839.248046875,6495.05810546875,6872.787109375,7284.046875,6513.35009765625,6371.47412109375,7289.13232421875,6845.23681640625,6441.17041015625,7306.8994140625,7416.0634765625,6563.42724609375,7080.5126953125,6764.1796875,7177.72705078125,6572.46875,7146.47607421875,6884.94384765625,7237.7431640625,6659.384765625,6623.3740234375,6709.38232421875,7022.22607421875,7219.3505859375,7562.14111328125,7155.90185546875,7193.46240234375,6849.5751953125,6937.93212890625,6542.6552734375,7525.1708984375,7444.23046875,6523.78515625,7123.4189453125,6729.876953125,7407.4501953125,6425.388671875,6727.21435546875,7710.90625,7554.966796875,7039.7158203125,6836.8955078125,7028.49267578125,7234.76806640625,7409.822265625,7339.533203125,7430.60546875,6573.4189453125,6750.91455078125,7007.9912109375,7225.34033203125,6853.16357421875,7384.99609375,6583.212890625,7484.0830078125,6953.3818359375,7134.6513671875,7480.4560546875,7087.12255859375,6817.9716796875,7415.4931640625,7257.568359375,6685.40576171875,6547.64013671875,7161.73779296875,6940.4873046875,7442.48583984375,7442.208984375,7087.06787109375,6680.41796875,7197.37548828125,6453.91015625,6836.75048828125,7162.2314453125,6772.2294921875,7540.5517578125,6705.0771484375,7077.1875,6358.54638671875,6556.79345703125,7443.89892578125,6519.8486328125,7100.7158203125,6745.59814453125,7171.3642578125,7244.27294921875,6387.62744140625,7679.4912109375,7030.20947265625,6809.818359375,7318.044921875,7433.71044921875,7133.18017578125,6662.37841796875,6960.7880859375,6819.13671875,6776.35986328125,7306.0947265625,6684.82080078125,6833.875,6722.6474609375,7269.529296875,6889.4462890625,7419.46875,6793.05078125,6970.60009765625,6825.6962890625,6669.81201171875,6681.771484375,7384.197265625,7276.9794921875,7287.3369140625,7281.318359375,6982.75537109375,6514.51953125,7447.7021484375,7144.1611328125,6572.7939453125,7484.181640625,7214.9169921875,7496.72900390625,7340.3447265625,6472.70068359375,6757.02880859375,7134.607421875,7243.73291015625,6636.884765625,7021.2939453125,7346.12060546875,6640.6396484375,7031.255859375,6505.5830078125,6575.0302734375,6878.75830078125,7302.72802734375,7120.01708984375,6586.1962890625,6589.197265625,7520.205078125,7123.75341796875,7323.427734375,7460.41162109375,7020.9208984375,6326.5771484375,7309.95947265625,7053.03271484375],\"type\":\"scatter\"},{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232],\"y\":[null,null,null,null,null,null,null,null,null,null,null,197.54425786062623,129.41263670429993,141.7085563880476,125.65508861915376,121.15158832452201,122.20539299655574,100.42889378488883,106.9160232833021,122.41863551852519,101.01496341535142,127.13051880958753,111.66552287593926,116.79781048562279,102.4973804673166,113.62455478877314,114.80584568972898,102.41257342417381,117.398248287318,94.03648578706351,102.30462077159501,106.60636597900671,111.21455390737916,96.61730099492411,94.2369996158678,91.60054504054419,101.54879484983117,108.96306788411235,105.8164235651127,130.67988559665693,119.83942354438392,97.07524980577115,94.61603017933602,96.23570891382701,94.75929652115438,103.98737836516169,103.60489851564064,88.78090080452716,110.5250017084137,95.48269362560382,109.5508634134454,92.92337369033781,89.0356815271032,96.10377810035828,92.23843348729235,108.43044338758587,92.68578265198495,87.3678827546611,92.30642259373838,85.23900468570223,102.46583483765916,99.92135260582987,103.20473631713674,84.07088451973895,80.3093597611536,88.3326628419081,101.34935203050645,114.7578676636863,77.07965680793497,86.53027413736423,85.56692312640116,103.76987954762431,100.52897029541643,109.13067858956487,91.27860046388123,115.13526562072971,90.68127699884698,85.52632291112305,80.88847407244519,91.1491521413287,102.13052822947049,100.18775633971872,105.02632827262555,92.66077079863095,90.32810379911047,91.97070180170272,93.98073267512717,78.33003047836898,97.33200950394148,98.45345902878789,113.28528144172266,null,89.27385586855658,94.31470533352336,99.80311087474534,111.4618937322062,85.70314466574236,100.96978587744121,91.10673566666306,84.58774163473382,93.83639115274273,103.21028009606978,98.62039986316697,102.42103459127114,87.2108911702896,91.08693554897658,74.7842803116598,93.62317516969858,100.75541467927516,79.27161974264223,108.36968255705129,80.1520237446556,103.50031477869108,87.55664574607749,80.95169864128724,91.8296187431305,79.97533108816693,102.02585138047,109.74067979766438,101.00282215403699,86.43259629593472,93.40739353923786,97.62777446662469,89.71140158490137,86.64874487433552,89.55303499689705,92.72104700584478,82.11557094083817,92.97616129655096,89.00303055496606,96.85101210296081,80.55206617786557,127.60853208360895,81.55185876347566,78.90288124596607,96.37060122442485,93.43299730707263,97.35965067651901,86.91171542527167,91.74945765239927,84.61652442074892,97.84885114062837,96.7583584778362,90.24243408199675,94.70224505906309,77.86131659747821,88.86253468965529,90.0830925410766,108.37159721964059,89.04596867471204,99.04849692996248,71.72645292536566,96.57365920823734,104.13611596188616,81.17461311887575,92.78759116739448,104.14865497309034,88.22898028719918,83.85286490237323,85.05272586994846,98.28944459514928,89.65399521585476,92.48214869122793,91.35137066028945,105.02395509227217,85.00431407455243,81.04833077509578,93.28815903809803,92.19049965318823,93.24452828108289,92.57670902283098,95.52051558895477,92.20938343396479,93.55286847728281,96.72931802571354,108.35554434976957,93.78328507253781,108.94198815413155,87.81486769657633,86.66701095987692,84.6424237314651,87.23628128215138,91.71363907461456,83.13700723688794,84.85581289862762,102.21262114452523,77.96005261423582,79.05278601238216,88.28842178430891,93.6313910108326,73.40507671850744,87.59935757584593,106.23452768446303,78.22755703869736,98.97527925518277,87.58797637034625,77.93045971010675,85.56889215607585,87.81097682910712,88.63893592013082,93.1495272088647,80.85321600007319,83.26740720094635,82.54574300391857,91.63860539835247,92.33466045107218,104.34372149299895,89.50675197547504,85.90835093625326,94.2645671692232,80.10739419164386,80.86014029577846,87.1954726209493,87.80617484575707,97.12012086228385,87.84075924690423,84.10552921829803,126.39021918995269,95.91392330673035,81.63981195472576,83.2336327981905,96.81775899928235,85.17790044895243,87.84052342707577,94.66204586881015,93.94806969163879,86.30094916346455,99.415534889296,91.73393279574867,122.22360811639356,94.11822187627912,73.83878769722143,97.55759317308294,91.7546908895352,100.78945298263793,99.85222343439693,83.85432388070959,82.1059539275705,90.50571516477206,92.38463447516017,169.7382324077525,87.4129404777795,93.94869967926437,105.37663379912983,91.82389018447927,84.61803643092533,98.04924466048573,72.50184964887431,83.20243871302135,97.72601132631769,94.99441202477175,82.13012498442214,72.6827778205374,74.22446693088247,80.14864401080925,84.6955908455297,99.28132757810093,105.18263997668653,78.89773566840955,76.6400865461101,70.62278207202415,79.6496368796046,94.05805544968673,102.6730816819934,79.69960674686702,71.32213287150998,77.66580600368118,90.57307651101904,68.0858455505442,81.22424104056397,74.73145092729216,77.71337555621916,79.33129279107315,108.88095357536768,88.0443756533039,80.24900807675486,102.36873855021261,83.03353888363955,71.19512141680626,88.49444863528778,79.29058997198007,79.14701441388439,63.38171768036433,71.4406058670487,73.87982843214503,68.08119607995151,76.75858564275012,59.896052003906114,87.6334647068634,83.07920452391603,69.30269890739642,61.68003836139718,63.12048218122756,74.99088316438973,65.21948022156654,76.09935373455377,68.63398974857456,72.93050120981056,71.78742402770551,62.20580299232456,83.72393048012232,73.18103047283863,92.46236843898878,72.98192124305584,79.63818038318222,72.66431948577853,64.76455266553174,85.29421239306456,74.50622521380808,69.96521281526073,93.06059040622243,71.66843992149268,69.89170493594315,73.17076460403956,65.43792744867619,86.96509896379297,80.11198019793825,67.8612560638919,79.58600214522319,71.58247003318382,63.88190396564713,64.61785501374901,93.11679090032555,65.06713534329282,64.50791222630195,92.51733216537866,52.500669106416574,79.23842432288382,83.90401004605977,74.82436852484989,66.8725833164119,69.46618282311174,77.09001086360252,68.61938566536091,65.09464346102716,63.38412174767835,76.35852914380214,64.43852588970077,55.93908604817491,79.60437623936802,64.31387499740985,76.52390945563846,84.11196523100375,71.5778673533837,89.71593319962044,66.69179748539112,66.21739002356036,79.37138608117561,72.06724573273175,89.77267143488531,76.61233670318468,70.41519037587638,60.61755249378125,74.5220081509107,90.58957994549192,82.55898844402505,68.13837895608918,81.71817244336799,79.39157059939745,66.62102680469754,78.36305651703952,77.53534911471759,63.99505817421921,88.11999895881841,66.84507923146735,68.46845953690081,65.81079379945676,72.5312657616497,70.27146702056315,67.97784076716631,62.66709846497593,67.37051234582309,69.4168869594587,60.06749092183357,68.52830673233738,65.22775054767182,55.15757638463205,74.80374139470568,58.797501885832474,65.55392533814586,62.497986344295285,62.63195106058652,80.57586346600085,64.40641935048127,68.77939615822739,61.828120974054634,69.88857690230769,63.79056717801263,60.955529789521,69.48078446513082,79.87461282849694,59.69067510302402,61.31234725368292,72.45540921561097,69.90194959459167,60.98252443897629,72.47194653232067,55.81726878038608,64.20366192095673,58.65077750346563,63.7511215046046,72.19381947281659,78.88862168889113,58.06303704921246,57.02551826047525,54.16121529790508,74.36322301893073,62.04496168819202,79.44107396644324,78.5110097139579,87.2468308516488,69.1466388697713,67.46876173279132,62.75541316130942,62.97480700555819,67.734348996304,61.69680108618996,63.19886854269106,58.246015674725555,70.35699527517866,73.8501365994605,63.80524299406159,58.108220111449384,61.41838083613583,67.88773435370292,69.72317793230947,74.07484134758809,61.14417558017145,64.32836163059892,59.41720272067383,70.6942024946042,57.99483389837777,60.01526816314151,68.12383289046055,56.23371752598007,78.40209484868318,60.05603097312388,62.64191722263729,60.473938365075156,75.67687293661403,64.68733417530086,59.77037155425834,71.90102583223411,63.32932086321007,61.05710188076782,99.67223467046169,71.80784392950156,58.85023762683684,65.98379688342008,48.03682827553883,72.12068975118153,77.99142259639649,59.119925243235755,63.941371731580716,68.18188896091407,60.517444184150456,77.88429817875965,57.78082225128999,64.76255994100592,73.42907162176978,50.983025998345504,60.71946574153864,59.318666715848174,56.22107855774562,56.77478350797716,76.99900418253169,72.81282575810069,64.02238135552965,67.23075672309557,62.308743731716,60.26603768420578,53.70237691073893,60.49987616301232,61.83671367753726,72.51590651309112,68.10856461848816,62.22799150749191,55.04280215397377,68.49320168754889,67.81215202345068,57.01355429953799,59.55960912612803,71.65166205807242,60.84633438246621,66.16639133822646,65.46347371018024,69.37037222462068,72.47348554440829,74.28505212730099,55.25522123340574,47.54491451390887,51.27175219066093,78.45960110387914,71.56539733981215,67.00593190099443,66.14541198284147,68.36065791048522,77.05846818807531,66.21312563114891,58.47296048088457,67.50202960688945,68.82674253665556,51.16345612063975,72.44396062819526,74.87527916272374,53.91202640419434,51.104277356385836,74.49984534714535,57.35157899147242,57.68896779391541,50.159491831293224,61.85542042378761,75.43845051700136,57.592510542289475,61.94610206644401,58.36289105866962,53.23216767408854,74.54081740610079,57.67422731474414,62.28884829799302,93.42276064852697,54.7360791665641,73.70156961279552,58.808561309627564,60.75165870700176,83.18584519881472,67.21032804647118,59.58873646691069,59.92599520029237,77.53902771378439,58.87434270639824,60.417013468474124,61.732570594818114,68.0343695778107,62.140562087086515,56.882543577424165,52.47644888203962,79.8313595699357,69.48578783841634,72.93800707574567,125.50231909196123,58.23881786133463,59.28178306699057,60.9166904216502,60.55111859141692,58.46525366210909,60.76470612448037,60.711165598092975,57.791100274387716,66.27566175031009,63.76543148712955,69.05334686528246,56.55525877837586,69.84350292356197,79.38407295150242,71.51021392432538,60.19572972223537,65.51198028953291,67.33896309248311,59.690128579520724,62.72958057058461,75.16735984039293,67.95473863182922,72.24408357646733,56.330041821883654,65.37016781303171,55.20595313742925,68.8517784865953,60.4501188382584,80.96013973951098,72.79168381029035,52.36948834675838,64.23506644721469,77.11491242093078,68.09113913468327,68.36540482692591,57.128564763892214,55.7676363655702,67.63812724727283,63.3183612202211,63.25616331741041,57.76488489739558,51.10462227690721,67.3039505001834,61.010361148375935,56.95257733132841,54.45108866290647,65.7679851295114,57.75045410064337,75.44554780355388,53.53751930917055,68.65083396424008,53.53815339604521,54.798622956188716,67.43678777496397,74.03835517716566,69.28839326861134,64.48956038991453,57.363066130932694,57.384477677285616,58.991185802324246,49.29135247593285,68.9674186653906,61.01401968252217,62.43144457020957,59.954733714409564,56.17771863572616,59.1786487991184,85.36427221495505,67.38277981444213,61.300783881213434,77.42359963638388,65.67429766795436,76.62732480289682,66.63308202002102,51.37135179786061,56.93948719981998,56.54589234430499,62.38366966455478,50.736283401462906,68.87392037600478,58.07258081427405,45.665244908469184,53.14899874023553,72.95435366471334,70.8921635432765,48.96927874935614,66.23176707594158,61.60763776072254,67.37347418474438,54.80043464848089,61.36750241436054,55.40185406095894,59.99087156909008,71.06082685183618,56.7557561946718,59.67538000175608,80.46778990343626,56.05523169575734,58.50615869295373,62.66005890944735,58.79951613540424,60.95128184163308,59.94523972753729,60.73309644234544,52.23381027355799,59.033216224673865,54.605181561548775,54.24376905058866,54.216059553045596,79.86294358488045,85.78573262798314,74.71849810285758,61.58192104394816,52.41007041231684,52.42637409085674,58.42224832183939,58.223798267475,76.39089411247333,63.43588450457237,57.60525728764626,52.13268840076715,54.67323185812613,65.98378250591239,66.15851342333258,50.064385144406245,56.70440025661256,64.59490303397307,58.29031001875804,50.87322759748379,49.603728323179986,54.50864679402072,57.28968735023566,46.4563065200676,52.139604257793714,62.7395766307995,58.32242137212872,64.02760051832226,62.79404574437933,52.1779769366346,66.12994491871683,50.96942148967603,51.328437175016376,58.1291388217244,72.02424502507432,55.876289295674965,66.37144220812391,59.97791606665358,43.840528410666764,44.409541359622956,62.339807873032306,79.64690483093698,63.08740160517761,66.5865519124786,53.16552057766711,49.170446005736856,51.666755638015154,60.815162093982664,69.11428842404165,60.62887643505946,55.623783191950245,52.64942227796046,54.64510674431191,51.20947933179418,59.73133981080411,62.516649507334165,59.11994031624921,60.65856288950363,63.13472251893887,77.09776457250375,72.38658992937718,60.15495402182718,73.3471261495819,53.68729060137338,58.948911843094784,61.777441604431374,62.06908079285216,61.98106803491151,66.62642911346566,54.46738087829709,46.20778964854973,67.87947786097764,58.8018387726676,45.98868554238016,54.222258869206684,48.91622061636016,55.46498484359987,60.971900534011525,58.94477087231736,57.86456501325789,67.56333812509024,52.23507530677822,48.15090534689532,57.84638481576707,73.87496953843596,57.80893497435167,56.78879741583265,43.7345992357093,63.45991859307121,56.725081552101265,59.54480459659,64.52670958059275,63.51410402655856,58.98671918197263,61.77837769252702,66.29248496290288,48.349186335312254,58.07218424581871,51.40402563512556,58.22795393349242,61.17900107114342,58.0354112705007,67.21729536852284,62.661048077470575,67.8114190998161,53.749493687116235,48.67675605318795,76.72811430252898,54.41804787839384,53.54029655270641,47.33903380887241,52.823726046399905,47.1659346290673,68.51564966651603,53.94547509130301,56.75203352392585,65.45458936211837,56.0746556008877,61.7132068638344,49.187776422288785,82.29197879307245,62.262713257042805,50.63618565031349,50.573820983442445,52.37356092696226,52.518395280888534,45.89274402599935,49.651939497110284,47.65034378507315,59.37533108818046,71.52823791047472,62.716931389215794,63.79137957861325,53.50525316012037,59.01941105267069,53.12062132244008,45.45347842068411,46.91027144753232,41.811703126369316,54.690266867531925,66.27205626717219,46.322696077154,45.58285185725738,49.255644052910085,39.249748736517894,64.47417762769916,60.74751176234393,55.73230335000164,52.25051939997673,61.03864725177585,46.79710882710668,51.53525812535809,48.44311424462329,74.099702246872,59.52538310428157,60.310567394357925,65.46668650905077,63.27429726547562,55.70778373783855,50.524858762830455,54.308098334183,63.511462366648416,53.419588352300266,50.978383266626395,49.254465555043375,55.62853176176079,58.87478041705358,41.37620341516225,62.43486909893436,49.695689817829866,82.74141614434606,72.45358148102422,51.13457449286878,60.35867818198074,54.6999111356781,48.869798265635794,57.33033714783803,53.92768063232397,53.05059264334095,47.105441819151714,69.36529641418969,52.25559930423383,50.32390331967565,43.527616484964064,50.86279731927442,57.78337688590506,53.58934948413658,60.93195957098854,45.30734558187716,55.688778342052444,56.144743013553914,58.628890800178,45.55417044427868,53.039214262932816,58.69850552871053,61.362458778628564,44.737112725368874,56.13697094076711,53.15857116896794,53.47844520353723,58.11325279535485,54.78007803093552,54.664374619305725,51.04751356165201,64.8435608745719,39.43278355189462,44.98082942608055,49.199710306078316,61.96059484391338,63.253374588311765,63.22414385832532,49.88104585914905,65.4638532671304,62.65712470228664,55.40372750574431,56.554886399074725,55.51291567478866,55.97615735564284,54.390049463491174,49.10180711916559,56.942975143782,56.544912907557,63.97241662225634,61.6297344195953,53.18845881024354,54.379667325508194,49.672706143600465,50.32258103342927,59.69834532817739,56.634998233747254,57.23710189629255,49.74989188140387,61.57076806694868,53.47421740755502,53.92679570199959,42.32029136828292,54.969307408026026,48.69252743199131,62.59806012383528,49.62134840244037,55.51308682288896,65.87992927297314,50.380239086411635,47.9244009142804,47.71147040209768,53.713870500303294,50.552485067853944,63.27432546945699,66.68793819074637,69.05117551260038,49.462219314342406,71.93246194516446,60.295798908882375,50.7778909568875,44.52182761273625,60.80960742574336,88.47575364107983,51.77722085921076,50.373394898999216,68.87367129558804,52.39381201778827,82.76405430658568,46.62902329994058,50.83658430504225,50.40086530459465,65.17516993376559,72.7998760018095,58.61891884659461,49.19290602583538,62.532318027599125,53.84996712844345,65.09280541401884,50.287420689250446,63.089733839767725,49.962491920809015,52.1987541641041,38.693582217493436,46.40662657713035,61.57815794605358,57.83145494152696,61.67289319030449,59.03791459333477,59.17934562288992,66.42648282912606,59.450369817420714,47.75020831103764,55.04795074264528,44.025294810257826,51.86585274220107,61.67003530990145,73.40468268810322,47.48292277977819,47.85352955092064,70.92692643380707,52.526016823111405,45.43121872776486,68.78435877352301,50.165711210142376,48.66092540534773,65.12607557532792,46.58041880690204,61.826024509793335,34.346496368017775,63.372290554420026,51.19108306874685,47.833454487084566,46.65557096982245,59.66817337780781,55.138600431676316,71.16533870241804,50.206479570392105,58.92039162231998,56.13718618320957,68.34864666174661,58.868510650123355,45.85435172041811,50.07435601406813,53.41989273049385,57.0348124067069,65.48165614152663,46.3628610067785,41.535636183714296,55.81028630372438,55.96132990257646,52.6317770320837,46.53607208216291,58.28474281277635,55.14806502563733,62.78233713907684,50.34123671205965,64.06841368326606,53.33863028870042,63.81803588489864,62.89133436588245,54.90814918808958,61.128274354633824,59.040826192658216,46.364095415198015,55.585433722623854,60.7120554531017,51.72905982665491,56.81875476707168,67.61345001199982,65.03828912198009,52.45966528774677,62.4045786910435,65.84704319079269,51.43544045105877,60.646258571941104,45.70387020319368,78.21215467089192,47.98941826069808,59.035290622187794,47.29179321338889,61.445456546400564,50.14323347300652,49.92290675279991,43.297825276474065,46.85546997022054,46.03733856759183,50.43866876075909,47.397439200955304,47.24712373272686,46.1171432069782,69.19343525798996,45.37859676525589,54.23324216263129,63.22962076680546,51.29096596741021,54.959080797256206,61.51299076451292,56.74564245553554,62.306504874076204,50.195297661646684,54.24708692112095,61.491149240095154,57.712756774429074,55.59989629121224,68.56689739286236,58.745745799535946,61.48476036737154,89.60830321168893,61.19729191630153,62.83453289955066,58.997483707752934,70.93172591223772,55.28782530489431,72.13615477744077,51.301243393909246,47.12561889107727,53.467490207339516,54.49459064488008,45.322867324902674,76.42642115843785,48.66775648198222,60.79022644692237,51.09841538010611,45.65432603044665,52.457790528152884,56.01231675647977,49.326400341824666,61.672855299708125,48.26188302200406,72.49632306157332,54.066365547944514,47.38510570405743,48.10912608028124,45.87868077692408,61.720848457594236,44.077477847520406,49.107702200212664,41.872782406120066,46.51316629238216,53.51004767071263,61.21079542493837,68.10275787010409,49.478095017690904,62.08428269476561,39.58130435718979,62.11213251345752,65.50233196532474,53.5974789256437,53.98243351322408,40.212326781509866,57.513488704877915,50.992485869059486,53.096321889615616,64.23972189344168,51.08003270097247,56.45146257763425,71.45018418402769,56.172067201489405,46.99011245598849,47.514121432056086,57.9907397831025,57.95250032557185,57.677140448769755,48.295571995464,46.798888421001735,53.82699921901174,62.82614964809777,48.2625729138076,42.340470138292545,50.19366431570604,44.187067452883056,41.9021241061093,46.23524710594969,60.2002522485451,47.43973578913216,54.947049846291584,63.84806993600266,50.415067835117874,47.17147681013487,76.20699080496472,42.45129737173864,42.29962534838547,51.305602498671334,57.08088351872554,46.10238797977813,56.02312961376447,48.67252928016776,68.75227849184122,56.08542199415383,58.35010140689414,44.130988678301335,44.09418435893636,53.043444715865405,58.04733672951603,57.887454161481394,52.87499568326798,52.71885253692893,55.754460086799554,77.38819411379683,44.40067149828201,41.868134477192235,60.45116221564465,63.7757481623782,58.27270207797584,60.992303031236915,59.05233661830268,43.79794937263268,70.56310915705615,72.59806086466342,57.349758926087404,56.99217135879335,44.867873961128254,64.97438842446918,55.90232452491329,52.482745752884775,62.716195592890145,53.493898820642826,55.45425379415956,41.99711639617865,53.551330887300786,55.997650788690926,43.521224408211765,56.77514812629626,65.9297084554783,45.96215566245749,49.02760755625352,53.35149938760826,50.9665273156695,46.59456563711551,58.23050436036724,56.40201795616759,51.09002045960827,54.45428183339645,46.81566712920027,43.9999817778016,53.473844539186885,62.087094228166976,48.633139744311045,62.05121071747154,44.16357251916018,68.54054760264685,40.27238623985476,49.6170567100173,71.26264641518861,50.3710441249435,56.22055646726713,45.4329404875777,57.42446421741917,51.39221589632727,44.464222108438165,68.81023306938603,57.14204068818976,45.49478454157438,62.331879710866794,58.596319958121484,53.08982240346269,38.657750784565344,47.62277776061633,49.17268911750469,56.51162287203905,68.39305832213509,64.16765015964864,34.150304615440916,49.413607573822006,50.978247508469266,47.03433344876044,55.618773359354314,50.18795745083572,53.86489982883949,47.736756818356895,49.06375665239787,56.34248213689872,49.045112535569956,48.804367786094566,61.968151244551386,61.302755945830945,49.44428060195067,47.635970834124166,57.061309822618,44.67503044912149,49.842576281754795,59.943100599969036,44.4773169885268,56.51154911857505,57.53444212673711,44.9067678258435,53.342813302933095,72.31198146790206,63.87131935480875,39.202860294548174,45.30176292596078,50.221045068123985,52.38394929802755,54.98722083561695,40.90600655698265,51.41740650192286,49.3507511244127,47.604088527724585,59.70336988325289,45.19475066540726,45.18846980853156,62.10070076364215,55.65053016334941,58.00886312839358,61.08893489511226,55.91837869498991,39.51574843929011,50.48070593000241,60.50824772252758],\"type\":\"scatter\"},{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232],\"y\":[0.1173095703125,0.1175537109375,0.11761474609375,0.1170654296875,0.11810302734375,0.11724853515625,0.11700439453125,0.116943359375,0.1170654296875,0.1177978515625,0.1168212890625,0.11712646484375,0.11834716796875,0.1220703125,0.12408447265625,0.1275634765625,0.13232421875,0.13330078125,0.1396484375,0.13916015625,0.1348876953125,0.1409912109375,0.1416015625,0.14111328125,0.1417236328125,0.1446533203125,0.144775390625,0.147216796875,0.145263671875,0.14794921875,0.1484375,0.1485595703125,0.14892578125,0.150146484375,0.153076171875,0.1556396484375,0.1553955078125,0.1544189453125,0.1600341796875,0.1611328125,0.156982421875,0.160888671875,0.1595458984375,0.16064453125,0.16064453125,0.1580810546875,0.1646728515625,0.1646728515625,0.161865234375,0.164306640625,0.170166015625,0.164306640625,0.167236328125,0.1676025390625,0.172119140625,0.1676025390625,0.1751708984375,0.1705322265625,0.17529296875,0.169677734375,0.169921875,0.1734619140625,0.1737060546875,0.1744384765625,0.1795654296875,0.176513671875,0.1761474609375,0.17626953125,0.1754150390625,0.1810302734375,0.1763916015625,0.175048828125,0.180419921875,0.17919921875,0.1785888671875,0.1767578125,0.178466796875,0.17822265625,0.177001953125,0.1785888671875,0.17431640625,0.178466796875,0.18359375,0.1815185546875,0.182373046875,0.1805419921875,0.1812744140625,0.181396484375,0.181884765625,0.18310546875,0.1832275390625,0.1798095703125,0.187744140625,0.1795654296875,0.184814453125,0.1893310546875,0.1934814453125,0.1942138671875,0.189697265625,0.187744140625,0.1898193359375,0.1875,0.189697265625,0.2000732421875,0.1956787109375,0.1915283203125,0.1944580078125,0.19970703125,0.192626953125,0.1947021484375,0.193359375,0.1988525390625,0.1925048828125,0.192138671875,0.1968994140625,0.2017822265625,0.201171875,0.195556640625,0.2000732421875,0.200927734375,0.2042236328125,0.2005615234375,0.1990966796875,0.203857421875,0.19873046875,0.203125,0.206298828125,0.2039794921875,0.205810546875,0.201416015625,0.19921875,0.20556640625,0.1998291015625,0.20849609375,0.2078857421875,0.208984375,0.2095947265625,0.202880859375,0.212158203125,0.208740234375,0.201904296875,0.213134765625,0.21044921875,0.2060546875,0.2115478515625,0.213623046875,0.2125244140625,0.205322265625,0.2115478515625,0.2093505859375,0.212646484375,0.213134765625,0.21484375,0.2164306640625,0.2158203125,0.220458984375,0.210693359375,0.2093505859375,0.2144775390625,0.2139892578125,0.2120361328125,0.2169189453125,0.2216796875,0.2132568359375,0.2176513671875,0.2149658203125,0.218017578125,0.2117919921875,0.2159423828125,0.213134765625,0.216796875,0.2213134765625,0.2183837890625,0.2176513671875,0.2200927734375,0.2237548828125,0.2191162109375,0.22509765625,0.219970703125,0.2254638671875,0.2276611328125,0.2176513671875,0.2225341796875,0.2261962890625,0.215087890625,0.2337646484375,0.22412109375,0.21630859375,0.2242431640625,0.2254638671875,0.221435546875,0.223388671875,0.2164306640625,0.2275390625,0.2213134765625,0.2255859375,0.222412109375,0.2220458984375,0.2235107421875,0.218017578125,0.2181396484375,0.220947265625,0.219482421875,0.2230224609375,0.2303466796875,0.2230224609375,0.2261962890625,0.219970703125,0.223876953125,0.2259521484375,0.2315673828125,0.227294921875,0.2332763671875,0.2249755859375,0.23046875,0.22314453125,0.230224609375,0.2335205078125,0.2279052734375,0.2333984375,0.2288818359375,0.2330322265625,0.2335205078125,0.2261962890625,0.2218017578125,0.229248046875,0.24267578125,0.23046875,0.2342529296875,0.226318359375,0.240966796875,0.2332763671875,0.2291259765625,0.235595703125,0.23974609375,0.230224609375,0.2451171875,0.2366943359375,0.232421875,0.2449951171875,0.2347412109375,0.2391357421875,0.2327880859375,0.23291015625,0.2322998046875,0.2425537109375,0.2337646484375,0.2388916015625,0.233154296875,0.2330322265625,0.237060546875,0.238037109375,0.238037109375,0.24462890625,0.2320556640625,0.2347412109375,0.2366943359375,0.2261962890625,0.2352294921875,0.24169921875,0.2388916015625,0.2366943359375,0.2349853515625,0.2230224609375,0.2359619140625,0.226318359375,0.2276611328125,0.2291259765625,0.2274169921875,0.2352294921875,0.224365234375,0.2200927734375,0.2264404296875,0.2293701171875,0.2197265625,0.2230224609375,0.2208251953125,0.2264404296875,0.220947265625,0.2174072265625,0.223876953125,0.2293701171875,0.2286376953125,0.229248046875,0.220947265625,0.226806640625,0.2274169921875,0.232421875,0.224853515625,0.23095703125,0.22607421875,0.2181396484375,0.2239990234375,0.2236328125,0.22802734375,0.2139892578125,0.218505859375,0.2275390625,0.220458984375,0.2196044921875,0.220703125,0.2247314453125,0.217529296875,0.223388671875,0.2244873046875,0.2164306640625,0.223876953125,0.209716796875,0.2156982421875,0.2230224609375,0.212646484375,0.2147216796875,0.22021484375,0.221923828125,0.2196044921875,0.2200927734375,0.212158203125,0.2213134765625,0.2135009765625,0.220703125,0.218505859375,0.2208251953125,0.225830078125,0.209716796875,0.2108154296875,0.2135009765625,0.2130126953125,0.213623046875,0.222412109375,0.2139892578125,0.2110595703125,0.2130126953125,0.2093505859375,0.2138671875,0.2110595703125,0.2137451171875,0.210693359375,0.2149658203125,0.202880859375,0.2086181640625,0.210693359375,0.2098388671875,0.2161865234375,0.207275390625,0.2025146484375,0.2000732421875,0.212646484375,0.2091064453125,0.2139892578125,0.2247314453125,0.21484375,0.2039794921875,0.2059326171875,0.206298828125,0.20654296875,0.202880859375,0.2095947265625,0.2054443359375,0.200439453125,0.2105712890625,0.2115478515625,0.1951904296875,0.2030029296875,0.1982421875,0.210205078125,0.202392578125,0.2098388671875,0.204345703125,0.2099609375,0.2059326171875,0.2030029296875,0.2037353515625,0.210205078125,0.211181640625,0.215576171875,0.20361328125,0.20751953125,0.21044921875,0.20849609375,0.1978759765625,0.202880859375,0.2059326171875,0.206787109375,0.2030029296875,0.202392578125,0.206787109375,0.19921875,0.2012939453125,0.2064208984375,0.203369140625,0.2000732421875,0.1981201171875,0.1937255859375,0.2025146484375,0.202880859375,0.2034912109375,0.207275390625,0.19677734375,0.2076416015625,0.2015380859375,0.1962890625,0.19970703125,0.1883544921875,0.1978759765625,0.20166015625,0.2039794921875,0.19287109375,0.1910400390625,0.197998046875,0.18603515625,0.205078125,0.2066650390625,0.197021484375,0.1971435546875,0.1932373046875,0.1988525390625,0.19970703125,0.1943359375,0.2054443359375,0.194091796875,0.1964111328125,0.2010498046875,0.20458984375,0.194091796875,0.195068359375,0.2030029296875,0.1953125,0.2005615234375,0.20166015625,0.2021484375,0.19482421875,0.1968994140625,0.1939697265625,0.192138671875,0.2025146484375,0.1956787109375,0.196533203125,0.199951171875,0.194580078125,0.189208984375,0.1944580078125,0.197265625,0.194091796875,0.1917724609375,0.2015380859375,0.187744140625,0.1866455078125,0.1883544921875,0.1962890625,0.1956787109375,0.189208984375,0.1884765625,0.19189453125,0.18701171875,0.1920166015625,0.1920166015625,0.1944580078125,0.193603515625,0.19580078125,0.19189453125,0.193359375,0.1859130859375,0.1885986328125,0.18310546875,0.1903076171875,0.19287109375,0.184326171875,0.1978759765625,0.1846923828125,0.1947021484375,0.192626953125,0.18896484375,0.1944580078125,0.1978759765625,0.197021484375,0.1864013671875,0.1932373046875,0.197509765625,0.1888427734375,0.1866455078125,0.190185546875,0.1895751953125,0.198974609375,0.193603515625,0.186279296875,0.192626953125,0.187255859375,0.192138671875,0.1795654296875,0.190185546875,0.193359375,0.190673828125,0.18701171875,0.1934814453125,0.1885986328125,0.181640625,0.192626953125,0.1727294921875,0.180908203125,0.1961669921875,0.1810302734375,0.1788330078125,0.1947021484375,0.1845703125,0.1800537109375,0.185791015625,0.1954345703125,0.197998046875,0.1922607421875,0.181640625,0.1890869140625,0.1873779296875,0.18603515625,0.1895751953125,0.1888427734375,0.193115234375,0.1907958984375,0.1817626953125,0.191650390625,0.1873779296875,0.1865234375,0.1915283203125,0.1884765625,0.1798095703125,0.1904296875,0.183837890625,0.1939697265625,0.186767578125,0.187255859375,0.1884765625,0.193359375,0.186279296875,0.1832275390625,0.1929931640625,0.1932373046875,0.1922607421875,0.1922607421875,0.177001953125,0.1888427734375,0.1849365234375,0.1865234375,0.179443359375,0.18359375,0.1890869140625,0.1905517578125,0.1893310546875,0.179931640625,0.1861572265625,0.1871337890625,0.1856689453125,0.186279296875,0.1851806640625,0.1923828125,0.1890869140625,0.18701171875,0.1790771484375,0.1790771484375,0.19140625,0.183837890625,0.1798095703125,0.1796875,0.1787109375,0.1900634765625,0.1846923828125,0.1842041015625,0.1805419921875,0.18212890625,0.193359375,0.18701171875,0.185302734375,0.1884765625,0.189697265625,0.1881103515625,0.1817626953125,0.1815185546875,0.18896484375,0.1866455078125,0.191162109375,0.1826171875,0.1849365234375,0.18017578125,0.181396484375,0.185302734375,0.18359375,0.177734375,0.185546875,0.1832275390625,0.1866455078125,0.1890869140625,0.1849365234375,0.194091796875,0.17822265625,0.185546875,0.186767578125,0.1883544921875,0.1890869140625,0.1796875,0.18994140625,0.18408203125,0.18896484375,0.1865234375,0.190185546875,0.1875,0.179443359375,0.180908203125,0.1834716796875,0.180908203125,0.1842041015625,0.177490234375,0.1839599609375,0.1842041015625,0.183349609375,0.180908203125,0.1787109375,0.1795654296875,0.184814453125,0.1873779296875,0.187744140625,0.1746826171875,0.1763916015625,0.1864013671875,0.1807861328125,0.188232421875,0.1788330078125,0.180908203125,0.1759033203125,0.1851806640625,0.176513671875,0.1800537109375,0.183349609375,0.185546875,0.179931640625,0.1868896484375,0.18701171875,0.184814453125,0.1815185546875,0.1827392578125,0.1705322265625,0.1776123046875,0.181396484375,0.18359375,0.1859130859375,0.18017578125,0.1773681640625,0.19140625,0.1942138671875,0.1866455078125,0.1842041015625,0.1885986328125,0.181884765625,0.1876220703125,0.1783447265625,0.1751708984375,0.1829833984375,0.1788330078125,0.18701171875,0.1839599609375,0.1790771484375,0.182861328125,0.1783447265625,0.1795654296875,0.1851806640625,0.1873779296875,0.1788330078125,0.178955078125,0.17822265625,0.1846923828125,0.1826171875,0.1776123046875,0.181396484375,0.182861328125,0.1832275390625,0.17333984375,0.18310546875,0.178466796875,0.167724609375,0.1702880859375,0.1845703125,0.1817626953125,0.179931640625,0.1683349609375,0.178955078125,0.17822265625,0.180419921875,0.167236328125,0.1768798828125,0.1763916015625,0.185546875,0.170654296875,0.181640625,0.1748046875,0.1766357421875,0.1800537109375,0.1851806640625,0.1708984375,0.180908203125,0.17822265625,0.1710205078125,0.1722412109375,0.17529296875,0.1728515625,0.1776123046875,0.1771240234375,0.176025390625,0.1734619140625,0.1727294921875,0.17138671875,0.1817626953125,0.1805419921875,0.1695556640625,0.172607421875,0.1739501953125,0.1693115234375,0.17236328125,0.1754150390625,0.160888671875,0.17041015625,0.177001953125,0.1668701171875,0.1844482421875,0.1749267578125,0.16455078125,0.1728515625,0.181884765625,0.17431640625,0.1756591796875,0.17333984375,0.1712646484375,0.1756591796875,0.1663818359375,0.1754150390625,0.1683349609375,0.1693115234375,0.16162109375,0.17333984375,0.170654296875,0.1705322265625,0.17822265625,0.165771484375,0.16552734375,0.1671142578125,0.1741943359375,0.1697998046875,0.1636962890625,0.16845703125,0.163818359375,0.1728515625,0.17041015625,0.16455078125,0.168212890625,0.1669921875,0.1695556640625,0.1732177734375,0.1676025390625,0.167236328125,0.1700439453125,0.1741943359375,0.16162109375,0.169921875,0.1710205078125,0.1715087890625,0.1793212890625,0.170654296875,0.1614990234375,0.175048828125,0.1763916015625,0.162841796875,0.160400390625,0.16796875,0.173828125,0.1700439453125,0.1617431640625,0.1611328125,0.1552734375,0.166259765625,0.165283203125,0.1702880859375,0.1712646484375,0.171630859375,0.161865234375,0.16162109375,0.1629638671875,0.167724609375,0.16943359375,0.1611328125,0.1708984375,0.177734375,0.1661376953125,0.1724853515625,0.162353515625,0.16943359375,0.17431640625,0.168701171875,0.171630859375,0.162353515625,0.158935546875,0.165283203125,0.161376953125,0.163330078125,0.1619873046875,0.1685791015625,0.161865234375,0.163818359375,0.1602783203125,0.16162109375,0.16357421875,0.1658935546875,0.160888671875,0.1658935546875,0.16259765625,0.1630859375,0.1668701171875,0.1661376953125,0.1580810546875,0.1717529296875,0.16064453125,0.1568603515625,0.1724853515625,0.175537109375,0.1632080078125,0.1646728515625,0.1527099609375,0.171142578125,0.1666259765625,0.1575927734375,0.1580810546875,0.161376953125,0.1656494140625,0.16357421875,0.1634521484375,0.1671142578125,0.157958984375,0.1602783203125,0.171142578125,0.1715087890625,0.162841796875,0.15673828125,0.1610107421875,0.1671142578125,0.1658935546875,0.1513671875,0.1622314453125,0.1702880859375,0.1712646484375,0.168212890625,0.1611328125,0.1719970703125,0.16650390625,0.1611328125,0.1611328125,0.1541748046875,0.1580810546875,0.1640625,0.1619873046875,0.1629638671875,0.169677734375,0.1639404296875,0.1646728515625,0.1558837890625,0.1546630859375,0.16357421875,0.1668701171875,0.16015625,0.15576171875,0.1614990234375,0.159423828125,0.1654052734375,0.1641845703125,0.154541015625,0.161376953125,0.1636962890625,0.1651611328125,0.160888671875,0.1617431640625,0.1610107421875,0.1622314453125,0.163818359375,0.15478515625,0.1629638671875,0.161865234375,0.1614990234375,0.1617431640625,0.162353515625,0.1630859375,0.1583251953125,0.167724609375,0.16796875,0.15478515625,0.153076171875,0.1563720703125,0.154296875,0.15576171875,0.1673583984375,0.162353515625,0.1663818359375,0.156494140625,0.1649169921875,0.1585693359375,0.1639404296875,0.155517578125,0.161865234375,0.1529541015625,0.161376953125,0.1583251953125,0.1658935546875,0.1610107421875,0.156494140625,0.155517578125,0.158203125,0.1588134765625,0.1561279296875,0.1534423828125,0.16796875,0.162109375,0.1558837890625,0.1634521484375,0.159423828125,0.157470703125,0.1534423828125,0.165283203125,0.15771484375,0.150634765625,0.15771484375,0.158935546875,0.15380859375,0.1649169921875,0.1572265625,0.1572265625,0.1678466796875,0.1600341796875,0.14892578125,0.1517333984375,0.1583251953125,0.158935546875,0.15185546875,0.15576171875,0.150634765625,0.16455078125,0.156494140625,0.15771484375,0.152587890625,0.162841796875,0.1583251953125,0.15478515625,0.1558837890625,0.170166015625,0.1522216796875,0.1693115234375,0.1650390625,0.1629638671875,0.1588134765625,0.15771484375,0.169677734375,0.1549072265625,0.1671142578125,0.1669921875,0.1689453125,0.148681640625,0.1539306640625,0.1534423828125,0.1612548828125,0.1588134765625,0.15869140625,0.159423828125,0.155029296875,0.1539306640625,0.1551513671875,0.1500244140625,0.1524658203125,0.1580810546875,0.1666259765625,0.15478515625,0.1571044921875,0.1614990234375,0.1558837890625,0.161376953125,0.1568603515625,0.159912109375,0.1651611328125,0.1595458984375,0.1566162109375,0.150146484375,0.1529541015625,0.14599609375,0.1556396484375,0.1602783203125,0.1492919921875,0.1546630859375,0.1558837890625,0.154052734375,0.1512451171875,0.154052734375,0.1484375,0.145751953125,0.1650390625,0.1627197265625,0.1431884765625,0.151611328125,0.158203125,0.1593017578125,0.15625,0.1531982421875,0.152587890625,0.1536865234375,0.15478515625,0.1573486328125,0.1461181640625,0.1563720703125,0.152587890625,0.1502685546875,0.1551513671875,0.1627197265625,0.163330078125,0.158935546875,0.150634765625,0.1632080078125,0.1552734375,0.1529541015625,0.1553955078125,0.14990234375,0.1585693359375,0.158447265625,0.1488037109375,0.14794921875,0.1650390625,0.1551513671875,0.1595458984375,0.153076171875,0.1512451171875,0.1529541015625,0.1513671875,0.1507568359375,0.1558837890625,0.1492919921875,0.1500244140625,0.14990234375,0.1474609375,0.1546630859375,0.1549072265625,0.158203125,0.150146484375,0.153564453125,0.1563720703125,0.156982421875,0.153076171875,0.1524658203125,0.152099609375,0.1595458984375,0.1512451171875,0.15478515625,0.1571044921875,0.1605224609375,0.1561279296875,0.15478515625,0.147705078125,0.1553955078125,0.1473388671875,0.157958984375,0.150390625,0.156005859375,0.152099609375,0.15673828125,0.1552734375,0.157470703125,0.1571044921875,0.141357421875,0.1578369140625,0.15234375,0.1546630859375,0.1551513671875,0.1533203125,0.156005859375,0.1546630859375,0.1571044921875,0.14599609375,0.146728515625,0.148681640625,0.1488037109375,0.1561279296875,0.145263671875,0.1492919921875,0.1475830078125,0.151611328125,0.1519775390625,0.1414794921875,0.146484375,0.15478515625,0.1575927734375,0.1568603515625,0.152099609375,0.1485595703125,0.1517333984375,0.1524658203125,0.157958984375,0.14794921875,0.148681640625,0.1546630859375,0.1475830078125,0.1468505859375,0.1505126953125,0.1494140625,0.1533203125,0.15380859375,0.145751953125,0.15625,0.154296875,0.15576171875,0.1434326171875,0.1568603515625,0.1474609375,0.1524658203125,0.1571044921875,0.1531982421875,0.1453857421875,0.1531982421875,0.1468505859375,0.1485595703125,0.1524658203125,0.1529541015625,0.15625,0.15283203125,0.1505126953125,0.13916015625,0.1500244140625,0.1512451171875,0.1475830078125,0.1495361328125,0.149169921875,0.1539306640625,0.1456298828125,0.1534423828125,0.14599609375,0.1475830078125,0.1566162109375,0.14990234375,0.15185546875,0.1484375,0.1553955078125,0.1514892578125,0.1497802734375,0.1510009765625,0.151123046875,0.1546630859375,0.1527099609375,0.149658203125,0.1522216796875,0.1436767578125,0.1455078125,0.15380859375,0.1483154296875,0.154052734375,0.14404296875,0.1448974609375,0.1419677734375,0.151123046875,0.1480712890625,0.1444091796875,0.150146484375,0.1536865234375,0.151123046875,0.1480712890625,0.1527099609375,0.1568603515625,0.149169921875,0.151611328125,0.151611328125,0.1490478515625,0.1484375,0.1497802734375,0.1505126953125,0.1470947265625,0.14697265625,0.15478515625,0.152099609375,0.153076171875,0.1484375,0.15771484375,0.143310546875,0.14697265625,0.146484375,0.158447265625,0.1444091796875,0.1505126953125,0.1463623046875,0.1578369140625,0.1553955078125,0.144287109375,0.1539306640625,0.15673828125,0.1446533203125,0.1485595703125,0.142333984375,0.1541748046875,0.14794921875,0.1458740234375,0.1458740234375,0.1473388671875,0.14697265625,0.143798828125,0.1488037109375,0.1480712890625,0.1446533203125,0.14111328125,0.149169921875,0.1448974609375,0.142333984375,0.1455078125,0.145751953125,0.14208984375,0.1451416015625,0.15380859375,0.138427734375,0.1497802734375,0.1455078125,0.14599609375,0.143798828125,0.143798828125,0.1461181640625,0.14794921875,0.14111328125,0.1572265625,0.140869140625,0.1365966796875,0.142578125,0.1500244140625,0.13720703125,0.14599609375,0.1455078125,0.1419677734375,0.1436767578125,0.1473388671875,0.143310546875,0.145263671875],\"type\":\"scatter\"},{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232],\"y\":[0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.00005,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.000025,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125,0.0000125],\"type\":\"scatter\"},{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232],\"y\":[0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.6598772557492791,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.7068745437409153,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.726118031360284,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7141277739686651,0.7037153817934473],\"type\":\"scatter\"},{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232],\"y\":[0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.6764375854767235,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7214633248291069,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7417269994158159,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.7344106716768866,0.726539065588645],\"type\":\"scatter\"}],\"name\":\"0\"}]);\n",
              "                        }).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f238a3ba-b8bf-47f8-87f2-5c06438e2f2f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/1::   4%|▍         | 1232/30804 [09:17<3:43:10,  2.21it/s, loss=7.05e+3, pearson_cosine=0.704, spearman_cosine=0.727, mean_grad_norm=60.5, learning_rate=1.25e-5]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping triggered at epoch 1, iteration 1232.\n",
            "Best Spearman Correlation: 0.7417269994158159\n",
            "Best Pearson Correlation: 0.726118031360284\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "import os\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from sentence_transformers import SentenceTransformer, models, util\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "import copy\n",
        "import gc\n",
        "import math\n",
        "import random\n",
        "import nlpaug.augmenter.word as naw\n",
        "from IPython.display import display, clear_output\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "from torch import GradScaler\n",
        "from torch.amp import autocast\n",
        "\n",
        "class BarlowTwinsNCSE:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        torch.backends.cudnn.benchmark = True  # Enable cuDNN benchmarking\n",
        "        self._prepare_datasets()\n",
        "        self._initialize_models()\n",
        "        self._initialize_optimizer_scheduler()\n",
        "        self.scaler = GradScaler(\"cuda\", enabled=self.config.get(\"use_amp\", True))  # GradScaler for AMP\n",
        "        self.best_spearman = -float(\"inf\")\n",
        "        self.best_pearson = -float(\"inf\")\n",
        "        self.patience_counter = 0\n",
        "        self.augmenters = [\n",
        "            naw.SynonymAug(aug_src='wordnet', aug_p=self.config[\"aug_p\"]),\n",
        "            naw.RandomWordAug(action=\"swap\", aug_p=self.config[\"aug_p\"]),\n",
        "            naw.RandomWordAug(aug_p=self.config[\"aug_p\"]),\n",
        "        ]\n",
        "        self.test_sts_pearson_cosine_values = []\n",
        "        self.test_sts_spearman_cosine_values = []\n",
        "        self.test_iterations = []\n",
        "        self._create_plot()\n",
        "\n",
        "    def _create_plot(self):\n",
        "        self.loss_values = []\n",
        "        self.sts_pearson_cosine_values = []\n",
        "        self.sts_spearman_cosine_values = []\n",
        "        self.mean_grad_norm_values = []\n",
        "        self.variance_values = []\n",
        "        self.learning_rate_values = []\n",
        "        self.iterations = []\n",
        "        self.epochs = []\n",
        "\n",
        "        # 3 rows x 2 columns grid with 6 subplots\n",
        "        self.fig = make_subplots(\n",
        "            rows=3, cols=2,\n",
        "            subplot_titles=(\n",
        "                \"Loss vs Iterations\",\n",
        "                \"Mean Gradient Norm vs Iterations\",\n",
        "                \"Variance vs Iterations\",\n",
        "                \"Learning Rate vs Iterations\",\n",
        "                \"Dev STS Cosine (Pearson & Spearman) vs Iterations\",\n",
        "                \"Test STS Cosine (Pearson & Spearman) vs Iterations\"\n",
        "            )\n",
        "        )\n",
        "        # Dev metrics\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Loss'), row=1, col=1)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Mean Gradient Norm'), row=1, col=2)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Variance'), row=2, col=1)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Learning Rate'), row=2, col=2)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Dev STS Pearson Cosine'), row=3, col=1)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Dev STS Spearman Cosine'), row=3, col=1)\n",
        "        # Test metrics\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Test STS Pearson Cosine'), row=3, col=2)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Test STS Spearman Cosine'), row=3, col=2)\n",
        "\n",
        "        # Prepare footer text\n",
        "        footer_text = \", \".join([f\"{key}={value}\" for key, value in self.config.items()])\n",
        "        augmenters_text = \", Augmenters: \" + \", \".join([f\"{aug.name}[{aug.action}:{aug.aug_p}]\" for aug in self.augmenters])\n",
        "        footer_text += augmenters_text\n",
        "        wrapped_footer = \"<br>\".join(textwrap.wrap(footer_text, width=160))\n",
        "\n",
        "        # Configure the download button\n",
        "        self.plot_config = {\n",
        "            'toImageButtonOptions': {\n",
        "                'filename': self.config[\"model_save_path\"],\n",
        "                'format': 'png',\n",
        "                'width': 1200,\n",
        "                'height': 800,\n",
        "                'scale': 1\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.fig.update_layout(\n",
        "            width=1200,\n",
        "            height=800,\n",
        "            title_text='Training Metrics',\n",
        "            showlegend=True,\n",
        "            margin=dict(l=50, r=50, t=100, b=150),\n",
        "            annotations=[\n",
        "                dict(\n",
        "                    text=wrapped_footer,\n",
        "                    showarrow=False,\n",
        "                    xref=\"paper\",\n",
        "                    yref=\"paper\",\n",
        "                    x=0.5,\n",
        "                    y=-0.15,\n",
        "                    xanchor='center',\n",
        "                    yanchor='top',\n",
        "                    align=\"center\",\n",
        "                    font=dict(size=10, color=\"gray\")\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Annotation for best metrics\n",
        "        self.best_metrics_annotation_index = len(self.fig.layout.annotations)\n",
        "        self.fig.add_annotation(\n",
        "            text=\"Best Spearman: N/A<br>Best Pearson: N/A\",\n",
        "            showarrow=False,\n",
        "            xref=\"paper\",\n",
        "            yref=\"paper\",\n",
        "            x=1.0,\n",
        "            y=0.0,\n",
        "            xanchor='right',\n",
        "            yanchor='bottom',\n",
        "            align=\"right\",\n",
        "            font=dict(size=12, color=\"blue\")\n",
        "        )\n",
        "\n",
        "        self.fig.show(config=self.plot_config)\n",
        "\n",
        "    def _update_traces(self):\n",
        "        with self.fig.batch_update():\n",
        "            self.fig.data[0].x = self.iterations\n",
        "            self.fig.data[0].y = self.loss_values\n",
        "            self.fig.data[1].x = self.iterations\n",
        "            self.fig.data[1].y = self.mean_grad_norm_values\n",
        "            self.fig.data[2].x = self.iterations\n",
        "            self.fig.data[2].y = self.variance_values\n",
        "            self.fig.data[3].x = self.iterations\n",
        "            self.fig.data[3].y = self.learning_rate_values\n",
        "            self.fig.data[4].x = self.iterations\n",
        "            self.fig.data[4].y = self.sts_pearson_cosine_values\n",
        "            self.fig.data[5].x = self.iterations\n",
        "            self.fig.data[5].y = self.sts_spearman_cosine_values\n",
        "            self.fig.data[6].x = self.test_iterations\n",
        "            self.fig.data[6].y = self.test_sts_pearson_cosine_values\n",
        "            self.fig.data[7].x = self.test_iterations\n",
        "            self.fig.data[7].y = self.test_sts_spearman_cosine_values\n",
        "\n",
        "            for i in range(1, 4):\n",
        "                for j in range(1, 3):\n",
        "                    self.fig.update_yaxes(autorange=True, row=i, col=j)\n",
        "                    self.fig.update_xaxes(autorange=True, row=i, col=j)\n",
        "\n",
        "    def _update_plot(self):\n",
        "        self._update_traces()\n",
        "\n",
        "        unique_epochs = sorted(set(self.epochs))\n",
        "        frames = []\n",
        "        for ep in unique_epochs:\n",
        "            indices = [i for i, e in enumerate(self.epochs) if e == ep]\n",
        "            frame = go.Frame(\n",
        "                data=[\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.loss_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.mean_grad_norm_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.variance_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.learning_rate_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.sts_pearson_cosine_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.sts_spearman_cosine_values[i] for i in indices])\n",
        "                ],\n",
        "                name=str(ep)\n",
        "            )\n",
        "            frames.append(frame)\n",
        "\n",
        "        self.fig.frames = frames\n",
        "\n",
        "        slider_steps = [\n",
        "            {\"args\": [[str(ep)], {\"frame\": {\"duration\": 0, \"redraw\": True},\n",
        "                                   \"mode\": \"immediate\", \"transition\": {\"duration\": 0}}],\n",
        "             \"label\": str(ep), \"method\": \"animate\"} for ep in unique_epochs\n",
        "        ]\n",
        "\n",
        "        self.fig.update_layout(\n",
        "            sliders=[{\n",
        "                \"active\": len(unique_epochs) - 1 if unique_epochs else 0,\n",
        "                \"currentvalue\": {\"prefix\": \"Epoch: \"},\n",
        "                \"pad\": {\"t\": 50},\n",
        "                \"steps\": slider_steps\n",
        "            }]\n",
        "        )\n",
        "\n",
        "        self.fig.layout.annotations[self.best_metrics_annotation_index].text = (\n",
        "            f\"Best Spearman: {self.best_spearman:.4f}<br>Best Pearson: {self.best_pearson:.4f}\"\n",
        "        )\n",
        "\n",
        "        for i in range(1, 4):\n",
        "            for j in range(1, 3):\n",
        "                self.fig.update_yaxes(autorange=True, row=i, col=j)\n",
        "                self.fig.update_xaxes(autorange=True, row=i, col=j)\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        self.fig.show(config=self.plot_config)\n",
        "\n",
        "    def _prepare_datasets(self):\n",
        "        wikipedia_url = \"https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse/resolve/main/wiki1m_for_simcse.txt\"\n",
        "        wikipedia_dataset_path = \"data/wiki1m_for_simcse.txt\"\n",
        "\n",
        "        if not os.path.exists(wikipedia_dataset_path):\n",
        "            util.http_get(wikipedia_url, wikipedia_dataset_path)\n",
        "        train_sentences = []\n",
        "        with open(wikipedia_dataset_path, encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if len(line) >= 10:\n",
        "                    train_sentences.append(line)\n",
        "        self.train_sentences = train_sentences\n",
        "\n",
        "        self.train_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"train\")\n",
        "        self.eval_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n",
        "        self.test_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"test\")\n",
        "\n",
        "        self.train_data_loader = DataLoader(\n",
        "            SentenceDataset(self.train_sentences),\n",
        "            batch_size=self.config[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            num_workers=self.config[\"num_workers\"],\n",
        "            pin_memory=True\n",
        "        )\n",
        "        self.test_evaluator = EmbeddingSimilarityEvaluator(\n",
        "            sentences1=self.test_dataset[\"sentence1\"],\n",
        "            sentences2=self.test_dataset[\"sentence2\"],\n",
        "            scores=self.test_dataset[\"score\"]\n",
        "        )\n",
        "\n",
        "        self.evaluate_steps = max(len(self.train_data_loader) // 100, 1)\n",
        "\n",
        "    def _get_random_augmentation(self):\n",
        "        return random.choice(self.augmenters)\n",
        "\n",
        "    def _apply_augmentation(self, sentences, aug):\n",
        "        return aug.augment(sentences)\n",
        "\n",
        "    def _initialize_models(self):\n",
        "        word_embedding_model = models.Transformer(\n",
        "            self.config[\"model_name\"],\n",
        "            max_seq_length=self.config[\"max_seq_length\"],\n",
        "            config_args={\"attention_dropout\": self.config[\"aug_p\"], \"dropout\": self.config[\"aug_p\"]}\n",
        "        )\n",
        "        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "        projection_layers = [torch.nn.Linear(768, self.config[\"projection_size\"])]\n",
        "        for _ in range(self.config[\"projection_depth\"] - 1):\n",
        "            projection_layers.append(torch.nn.BatchNorm1d(self.config[\"projection_size\"]))\n",
        "            projection_layers.append(torch.nn.ReLU())\n",
        "            projection_layers.append(torch.nn.Linear(self.config[\"projection_size\"], self.config[\"projection_size\"]))\n",
        "        projection_head = torch.nn.Sequential(*projection_layers)\n",
        "        self.online_model = SentenceTransformer(modules=[word_embedding_model, pooling_model, projection_head]).to(self.device)\n",
        "        encoder_modules = [copy.deepcopy(self.online_model[i]) for i in range(2)]\n",
        "        self.encoder = SentenceTransformer(modules=encoder_modules).to(self.device)\n",
        "\n",
        "    def _update_encoder(self):\n",
        "        for i in range(len(self.encoder)):\n",
        "            self.encoder[i].load_state_dict(self.online_model[i].state_dict())\n",
        "\n",
        "    def _initialize_optimizer_scheduler(self):\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.online_model.parameters(),\n",
        "            lr=self.config[\"learning_rate\"],\n",
        "            betas=(0.9, 0.999),\n",
        "            eps=1e-8,\n",
        "            weight_decay=self.config[\"weight_decay\"]\n",
        "        )\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=200, verbose=True)\n",
        "\n",
        "    def _forward_pass(self, model, sentences, train):\n",
        "        if train:\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "        features = model.tokenize(sentences)\n",
        "        features = {k: v.to(self.device, non_blocking=True) for k, v in features.items()}\n",
        "        with autocast(\"cuda\", enabled=self.config.get(\"use_amp\", True)):\n",
        "            embeddings = model[0](features)[\"token_embeddings\"]\n",
        "            pooled = model[1]({\"token_embeddings\": embeddings})[\"sentence_embedding\"]\n",
        "            if len(model) > 2:\n",
        "                return model[2](pooled)\n",
        "            else:\n",
        "                return pooled\n",
        "\n",
        "    def _mixed_barlow_twins_loss(self, z_a, z_b):\n",
        "        N, D = z_a.size()\n",
        "\n",
        "        z_a_norm = (z_a - z_a.mean(dim=0)) / (z_a.std(dim=0) + 1e-6)\n",
        "        z_b_norm = (z_b - z_b.mean(dim=0)) / (z_b.std(dim=0) + 1e-6)\n",
        "\n",
        "        c = torch.matmul(z_a_norm.T, z_b_norm) / N\n",
        "        I = torch.eye(D, device=z_a.device)\n",
        "        c_diff = (c - I).pow(2)\n",
        "        off_diag_mask = ~torch.eye(D, dtype=torch.bool, device=z_a.device)\n",
        "        c_diff[off_diag_mask] *= self.config[\"lambda_bt\"]\n",
        "        loss_bt = c_diff.sum()\n",
        "\n",
        "        # MixUp Regularization\n",
        "        idx = torch.randperm(N)\n",
        "        alpha = torch.tensor(np.random.beta(1.0, 1.0), device=z_a.device, dtype=z_a.dtype)\n",
        "        # Instead of mixing raw inputs (not applicable for text), mix the embeddings\n",
        "        z_m = alpha * z_a + (1 - alpha) * z_b[idx, :]\n",
        "        z_m_norm = (z_m - z_m.mean(dim=0)) / (z_m.std(dim=0) + 1e-6)\n",
        "        cc_m_a = torch.matmul(z_m_norm.T, z_a_norm) / N\n",
        "        cc_m_b = torch.matmul(z_m_norm.T, z_b_norm) / N\n",
        "        cc_m_a_gt = alpha * torch.matmul(z_a_norm.T, z_a_norm) / N + (1 - alpha) * torch.matmul(z_b_norm[idx, :].T, z_a_norm) / N\n",
        "        cc_m_b_gt = alpha * torch.matmul(z_a_norm.T, z_b_norm) / N + (1 - alpha) * torch.matmul(z_b_norm[idx, :].T, z_b_norm) / N\n",
        "        loss_mix = self.config[\"lambda_mixup\"] * self.config[\"lambda_bt\"] * (\n",
        "            (cc_m_a - cc_m_a_gt).pow(2).sum() + (cc_m_b - cc_m_b_gt).pow(2).sum()\n",
        "        )\n",
        "        return loss_bt + loss_mix\n",
        "\n",
        "    def _evaluate_without_heads(self):\n",
        "        self._update_encoder()\n",
        "        self.encoder.eval()\n",
        "        indices = list(range(len(self.eval_dataset[\"sentence1\"])))\n",
        "        random.shuffle(indices)\n",
        "        sentences1 = [self.eval_dataset[\"sentence1\"][i] for i in indices]\n",
        "        sentences2 = [self.eval_dataset[\"sentence2\"][i] for i in indices]\n",
        "        scores = [self.eval_dataset[\"score\"][i] for i in indices]\n",
        "        evaluator = EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)\n",
        "        return evaluator(self.encoder)\n",
        "\n",
        "    def fit(self):\n",
        "        latest_eval_metrics = {}\n",
        "        eval_count = 0\n",
        "        for epoch in range(self.config[\"epochs\"]):\n",
        "            early_stop = False\n",
        "            epoch_loss = 0\n",
        "            pbar = tqdm(self.train_data_loader, desc=f\"Epoch {epoch+1}/{self.config['epochs']}:\")\n",
        "            for idx, sentences in enumerate(pbar):\n",
        "                s1 = self._apply_augmentation(sentences, self._get_random_augmentation())\n",
        "                s2 = self._apply_augmentation(sentences, self._get_random_augmentation())\n",
        "\n",
        "                with autocast(\"cuda\", enabled=self.config.get(\"use_amp\", True)):\n",
        "                    z_a = self._forward_pass(self.online_model, s1, train=True)\n",
        "                    z_b = self._forward_pass(self.online_model, s2, train=True)\n",
        "                    loss = self._mixed_barlow_twins_loss(z_a, z_b)\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                self.scaler.scale(loss).backward()\n",
        "\n",
        "                scale = self.scaler.get_scale()\n",
        "                scale = scale if scale != 0 else 1e-8\n",
        "                total_norm_scaled = 0.0\n",
        "                for param in self.online_model.parameters():\n",
        "                    if param.grad is not None:\n",
        "                        param_norm = param.grad.data.norm(2).item()\n",
        "                        total_norm_scaled += param_norm ** 2\n",
        "                total_norm_scaled = math.sqrt(total_norm_scaled)\n",
        "                total_norm = total_norm_scaled / scale\n",
        "                mean_grad_norm = total_norm / len(list(self.online_model.parameters()))\n",
        "\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "                self.scheduler.step(loss)\n",
        "\n",
        "                if idx % self.evaluate_steps == 0 or idx in [0, len(self.train_data_loader)-1]:\n",
        "                    latest_eval_metrics = self._evaluate_without_heads()\n",
        "                    last_spearman = latest_eval_metrics.get('spearman_cosine', -float('inf'))\n",
        "                    last_pearson = latest_eval_metrics.get('pearson_cosine', -float('inf'))\n",
        "                    if last_spearman > self.best_spearman:\n",
        "                        self.best_spearman = last_spearman\n",
        "                        self.best_pearson = last_pearson\n",
        "                        self.patience_counter = 0\n",
        "                    else:\n",
        "                        self.patience_counter += self.evaluate_steps\n",
        "\n",
        "                    eval_count += 1\n",
        "                    # Every 50 evaluation steps, compute test evaluator metrics.\n",
        "                    if eval_count % 5 == 0 or eval_count == 0:\n",
        "                        self._update_encoder()\n",
        "                        self.encoder.eval()\n",
        "                        test_metrics = self.test_evaluator(self.encoder)\n",
        "                        self.test_sts_pearson_cosine_values.append(test_metrics.get('pearson_cosine', np.nan))\n",
        "                        self.test_sts_spearman_cosine_values.append(test_metrics.get('spearman_cosine', np.nan))\n",
        "                        self.test_iterations.append(idx)\n",
        "\n",
        "                pbar.set_postfix({\n",
        "                    \"loss\": loss.item(),\n",
        "                    **latest_eval_metrics,\n",
        "                    \"mean_grad_norm\": mean_grad_norm,\n",
        "                    \"learning_rate\": self.optimizer.param_groups[0]['lr']\n",
        "                })\n",
        "\n",
        "                self.loss_values.append(loss.item())\n",
        "                self.sts_pearson_cosine_values.append(latest_eval_metrics.get('pearson_cosine', np.nan))\n",
        "                self.sts_spearman_cosine_values.append(latest_eval_metrics.get('spearman_cosine', np.nan))\n",
        "                self.mean_grad_norm_values.append(mean_grad_norm)\n",
        "                self.variance_values.append(torch.var(z_a, dim=0).mean().item())\n",
        "                self.learning_rate_values.append(self.optimizer.param_groups[0]['lr'])\n",
        "                self.iterations.append(idx)\n",
        "                self.epochs.append(epoch)\n",
        "\n",
        "                if idx % self.evaluate_steps == 0 or idx in [0, len(self.train_data_loader)-1]:\n",
        "                    self._update_plot()\n",
        "                    if self.patience_counter >= self.config[\"patience\"]:\n",
        "                        early_stop = True\n",
        "                        print(f\"Early stopping triggered at epoch {epoch+1}, iteration {idx}.\")\n",
        "                        print(f\"Best Spearman Correlation: {self.best_spearman}\")\n",
        "                        print(f\"Best Pearson Correlation: {self.best_pearson}\")\n",
        "                        break\n",
        "\n",
        "            if early_stop:\n",
        "                break\n",
        "            avg_loss = epoch_loss / len(self.train_data_loader)\n",
        "            pbar.set_description(f\"Epoch {epoch+1} Loss: {avg_loss}\")\n",
        "\n",
        "    def cleanup(self):\n",
        "        del self.online_model, self.optimizer, self.scheduler, self.scaler\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "class SentenceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sentences):\n",
        "        self.sentences = sentences\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentences[idx]\n",
        "\n",
        "config = {\n",
        "    \"model_name\": \"distilbert-base-uncased\",\n",
        "    \"batch_size\": 256,\n",
        "    \"projection_depth\": 4,\n",
        "    \"projection_size\": 6144,\n",
        "    \"epochs\": 1,\n",
        "    \"warmup_proportion\": 0.0,\n",
        "    \"max_seq_length\": 64,\n",
        "    \"aug_p\": 0.3,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"model_save_path\": f\"train_stsb_bt-distilbert-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\",\n",
        "    \"num_workers\": 2,\n",
        "    \"weight_decay\": 1e-5,\n",
        "    \"lambda_bt\": 0.0051,\n",
        "    \"lambda_mixup\": 0.8,\n",
        "    \"use_amp\": True,\n",
        "    \"patience\": 500\n",
        "}\n",
        "\n",
        "trainer = BarlowTwinsNCSE(config)\n",
        "trainer.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GewS0FAjqepf"
      },
      "source": [
        "## Testing Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gnsh36-KovT4",
        "outputId": "3456bfd0-3a9f-4783-8208-54b4c1b4fcd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'pearson_cosine': 0.6668615350342543, 'spearman_cosine': 0.66859453666296}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer._update_encoder()\n",
        "trainer.encoder.eval()\n",
        "trainer.test_evaluator(trainer.encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_1moXSFqiDu"
      },
      "source": [
        "## Saving Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnCVyIj6rSre",
        "outputId": "b32f91e2-6471-499e-b3d9-18156c2fd86f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder weights saved to train_stsb_bt-distilbert-2025-02-12_16-13-57\n"
          ]
        }
      ],
      "source": [
        "# Ensure the encoder is in evaluation mode before saving\n",
        "trainer.encoder.eval()\n",
        "\n",
        "# Save the state dictionary of the encoder\n",
        "torch.save(trainer.encoder.state_dict(), trainer.config[\"model_save_path\"])\n",
        "print(f\"Encoder weights saved to {trainer.config['model_save_path']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5z5zD4tp6v1"
      },
      "source": [
        "## Hyperparameter Tuning using Optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p7WLVawIu_G"
      },
      "source": [
        "Initial testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JTMmN2LIuaY"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "import os\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from sentence_transformers import SentenceTransformer, models, util\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "import copy\n",
        "import gc\n",
        "import math\n",
        "import random\n",
        "import nlpaug.augmenter.word as naw\n",
        "from IPython.display import display, clear_output\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "from torch import GradScaler\n",
        "from torch.amp import autocast\n",
        "import optuna\n",
        "\n",
        "class BarlowTwinsNCSE:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        torch.backends.cudnn.benchmark = True  # Enable cuDNN benchmarking\n",
        "        self._prepare_datasets()\n",
        "        self._initialize_models()\n",
        "        self._initialize_optimizer_scheduler()\n",
        "        self.scaler = GradScaler(\"cuda\", enabled=self.config.get(\"use_amp\", True))  # GradScaler for AMP\n",
        "        self.best_spearman = -float(\"inf\")\n",
        "        self.best_pearson = -float(\"inf\")\n",
        "        self.patience_counter = 0\n",
        "        self.augmenters = [\n",
        "            naw.SynonymAug(aug_src='wordnet', aug_p=self.config[\"aug_p\"]),\n",
        "            naw.RandomWordAug(action=\"swap\", aug_p=self.config[\"aug_p\"]),\n",
        "            naw.RandomWordAug(aug_p=self.config[\"aug_p\"]),\n",
        "        ]\n",
        "        self.test_sts_pearson_cosine_values = []\n",
        "        self.test_sts_spearman_cosine_values = []\n",
        "        self.test_iterations = []\n",
        "        self._create_plot()\n",
        "\n",
        "    def _create_plot(self):\n",
        "        self.loss_values = []\n",
        "        self.sts_pearson_cosine_values = []\n",
        "        self.sts_spearman_cosine_values = []\n",
        "        self.mean_grad_norm_values = []\n",
        "        self.variance_values = []\n",
        "        self.learning_rate_values = []\n",
        "        self.iterations = []\n",
        "        self.epochs = []\n",
        "\n",
        "        # 3 rows x 2 columns grid with 6 subplots\n",
        "        self.fig = make_subplots(\n",
        "            rows=3, cols=2,\n",
        "            subplot_titles=(\n",
        "                \"Loss vs Iterations\",\n",
        "                \"Mean Gradient Norm vs Iterations\",\n",
        "                \"Variance vs Iterations\",\n",
        "                \"Learning Rate vs Iterations\",\n",
        "                \"Dev STS Cosine (Pearson & Spearman) vs Iterations\",\n",
        "                \"Test STS Cosine (Pearson & Spearman) vs Iterations\"\n",
        "            )\n",
        "        )\n",
        "        # Dev metrics\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Loss'), row=1, col=1)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Mean Gradient Norm'), row=1, col=2)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Variance'), row=2, col=1)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Learning Rate'), row=2, col=2)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Dev STS Pearson Cosine'), row=3, col=1)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Dev STS Spearman Cosine'), row=3, col=1)\n",
        "        # Test metrics\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Test STS Pearson Cosine'), row=3, col=2)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Test STS Spearman Cosine'), row=3, col=2)\n",
        "\n",
        "        # Prepare footer text\n",
        "        footer_text = \", \".join([f\"{key}={value}\" for key, value in self.config.items()])\n",
        "        augmenters_text = \", Augmenters: \" + \", \".join([f\"{aug.name}[{aug.action}:{aug.aug_p}]\" for aug in self.augmenters])\n",
        "        footer_text += augmenters_text\n",
        "        wrapped_footer = \"<br>\".join(textwrap.wrap(footer_text, width=160))\n",
        "\n",
        "        # Configure the download button\n",
        "        self.plot_config = {\n",
        "            'toImageButtonOptions': {\n",
        "                'filename': self.config[\"model_save_path\"],\n",
        "                'format': 'png',\n",
        "                'width': 1200,\n",
        "                'height': 800,\n",
        "                'scale': 1\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.fig.update_layout(\n",
        "            width=1200,\n",
        "            height=800,\n",
        "            title_text='Training Metrics',\n",
        "            showlegend=True,\n",
        "            margin=dict(l=50, r=50, t=100, b=150),\n",
        "            annotations=[\n",
        "                dict(\n",
        "                    text=wrapped_footer,\n",
        "                    showarrow=False,\n",
        "                    xref=\"paper\",\n",
        "                    yref=\"paper\",\n",
        "                    x=0.5,\n",
        "                    y=-0.15,\n",
        "                    xanchor='center',\n",
        "                    yanchor='top',\n",
        "                    align=\"center\",\n",
        "                    font=dict(size=10, color=\"gray\")\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Annotation for best metrics\n",
        "        self.best_metrics_annotation_index = len(self.fig.layout.annotations)\n",
        "        self.fig.add_annotation(\n",
        "            text=\"Best Spearman (Test): N/A<br>Best Pearson (Test): N/A<br>Best Spearman (Val): N/A<br>Best Pearson (Val): N/A\",\n",
        "            showarrow=False,\n",
        "            xref=\"paper\",\n",
        "            yref=\"paper\",\n",
        "            x=1.0,\n",
        "            y=0.0,\n",
        "            xanchor='right',\n",
        "            yanchor='bottom',\n",
        "            align=\"right\",\n",
        "            font=dict(size=12, color=\"blue\")\n",
        "        )\n",
        "\n",
        "        self.fig.show(config=self.plot_config)\n",
        "\n",
        "    def _update_traces(self):\n",
        "        with self.fig.batch_update():\n",
        "            self.fig.data[0].x = self.iterations\n",
        "            self.fig.data[0].y = self.loss_values\n",
        "            self.fig.data[1].x = self.iterations\n",
        "            self.fig.data[1].y = self.mean_grad_norm_values\n",
        "            self.fig.data[2].x = self.iterations\n",
        "            self.fig.data[2].y = self.variance_values\n",
        "            self.fig.data[3].x = self.iterations\n",
        "            self.fig.data[3].y = self.learning_rate_values\n",
        "            self.fig.data[4].x = self.iterations\n",
        "            self.fig.data[4].y = self.sts_pearson_cosine_values\n",
        "            self.fig.data[5].x = self.iterations\n",
        "            self.fig.data[5].y = self.sts_spearman_cosine_values\n",
        "            self.fig.data[6].x = self.test_iterations\n",
        "            self.fig.data[6].y = self.test_sts_pearson_cosine_values\n",
        "            self.fig.data[7].x = self.test_iterations\n",
        "            self.fig.data[7].y = self.test_sts_spearman_cosine_values\n",
        "\n",
        "            for i in range(1, 4):\n",
        "                for j in range(1, 3):\n",
        "                    self.fig.update_yaxes(autorange=True, row=i, col=j)\n",
        "                    self.fig.update_xaxes(autorange=True, row=i, col=j)\n",
        "\n",
        "    def _update_plot(self):\n",
        "        self._update_traces()\n",
        "        unique_epochs = sorted(set(self.epochs))\n",
        "        frames = []\n",
        "        for ep in unique_epochs:\n",
        "            indices = [i for i, e in enumerate(self.epochs) if e == ep]\n",
        "            frame = go.Frame(\n",
        "                data=[\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.loss_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.mean_grad_norm_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.variance_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.learning_rate_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.sts_pearson_cosine_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.sts_spearman_cosine_values[i] for i in indices])\n",
        "                ],\n",
        "                name=str(ep)\n",
        "            )\n",
        "            frames.append(frame)\n",
        "        self.fig.frames = frames\n",
        "\n",
        "        slider_steps = [\n",
        "            {\"args\": [[str(ep)], {\"frame\": {\"duration\": 0, \"redraw\": True},\n",
        "                                   \"mode\": \"immediate\", \"transition\": {\"duration\": 0}}],\n",
        "             \"label\": str(ep), \"method\": \"animate\"} for ep in unique_epochs\n",
        "        ]\n",
        "\n",
        "        self.fig.update_layout(\n",
        "            sliders=[{\n",
        "                \"active\": len(unique_epochs) - 1 if unique_epochs else 0,\n",
        "                \"currentvalue\": {\"prefix\": \"Epoch: \"},\n",
        "                \"pad\": {\"t\": 50},\n",
        "                \"steps\": slider_steps\n",
        "            }]\n",
        "        )\n",
        "\n",
        "        # Compute best test metrics if available\n",
        "        if self.test_sts_spearman_cosine_values:\n",
        "            best_test_spearman = max(self.test_sts_spearman_cosine_values)\n",
        "        else:\n",
        "            best_test_spearman = float('nan')\n",
        "        if self.test_sts_pearson_cosine_values:\n",
        "            best_test_pearson = max(self.test_sts_pearson_cosine_values)\n",
        "        else:\n",
        "            best_test_pearson = float('nan')\n",
        "\n",
        "        self.fig.layout.annotations[self.best_metrics_annotation_index].text = (\n",
        "            f\"Best Spearman (Test): {best_test_spearman:.4f}<br>\"\n",
        "            f\"Best Pearson (Test): {best_test_pearson:.4f}<br>\"\n",
        "            f\"Best Spearman (Val): {self.best_spearman:.4f}<br>\"\n",
        "            f\"Best Pearson (Val): {self.best_pearson:.4f}\"\n",
        "        )\n",
        "\n",
        "        for i in range(1, 4):\n",
        "            for j in range(1, 3):\n",
        "                self.fig.update_yaxes(autorange=True, row=i, col=j)\n",
        "                self.fig.update_xaxes(autorange=True, row=i, col=j)\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        self.fig.show(config=self.plot_config)\n",
        "\n",
        "    def _prepare_datasets(self):\n",
        "        wikipedia_url = \"https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse/resolve/main/wiki1m_for_simcse.txt\"\n",
        "        wikipedia_dataset_path = \"data/wiki1m_for_simcse.txt\"\n",
        "        if not os.path.exists(wikipedia_dataset_path):\n",
        "            util.http_get(wikipedia_url, wikipedia_dataset_path)\n",
        "        train_sentences = []\n",
        "        with open(wikipedia_dataset_path, encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if len(line) >= 10:\n",
        "                    train_sentences.append(line)\n",
        "        self.train_sentences = train_sentences\n",
        "\n",
        "        self.train_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"train\")\n",
        "        self.eval_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n",
        "        self.test_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"test\")\n",
        "\n",
        "        self.train_data_loader = DataLoader(\n",
        "            SentenceDataset(self.train_sentences),\n",
        "            batch_size=self.config[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            num_workers=self.config[\"num_workers\"],\n",
        "            pin_memory=True\n",
        "        )\n",
        "        self.test_evaluator = EmbeddingSimilarityEvaluator(\n",
        "            sentences1=self.test_dataset[\"sentence1\"],\n",
        "            sentences2=self.test_dataset[\"sentence2\"],\n",
        "            scores=self.test_dataset[\"score\"]\n",
        "        )\n",
        "\n",
        "        self.evaluate_steps = max(len(self.train_data_loader) // 50, 1)\n",
        "\n",
        "        # Ensure patience is at least twice as large as evaluate_steps\n",
        "        if self.config.get(\"patience\", 0) < self.evaluate_steps * 5:\n",
        "            print(f\"Warning: Patience ({self.config['patience']}) is less than evaluation steps x5 ({self.evaluate_steps * 5}). Adjusting patience to {self.evaluate_steps * 5}.\")\n",
        "            self.config[\"patience\"] = self.evaluate_steps * 5\n",
        "\n",
        "\n",
        "    def _get_random_augmentation(self):\n",
        "        return random.choice(self.augmenters)\n",
        "\n",
        "    def _apply_augmentation(self, sentences, aug):\n",
        "        return aug.augment(sentences)\n",
        "\n",
        "    def _initialize_models(self):\n",
        "        word_embedding_model = models.Transformer(\n",
        "            self.config[\"model_name\"],\n",
        "            max_seq_length=self.config[\"max_seq_length\"],\n",
        "            config_args={\"attention_dropout\": self.config[\"aug_p\"], \"dropout\": self.config[\"aug_p\"]}\n",
        "        )\n",
        "        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "        projection_layers = [torch.nn.Linear(768, self.config[\"projection_size\"])]\n",
        "        for _ in range(self.config[\"projection_depth\"] - 1):\n",
        "            projection_layers.append(torch.nn.BatchNorm1d(self.config[\"projection_size\"]))\n",
        "            projection_layers.append(torch.nn.ReLU())\n",
        "            projection_layers.append(torch.nn.Linear(self.config[\"projection_size\"], self.config[\"projection_size\"]))\n",
        "        projection_head = torch.nn.Sequential(*projection_layers)\n",
        "        self.online_model = SentenceTransformer(modules=[word_embedding_model, pooling_model, projection_head]).to(self.device)\n",
        "        encoder_modules = [copy.deepcopy(self.online_model[i]) for i in range(2)]\n",
        "        self.encoder = SentenceTransformer(modules=encoder_modules).to(self.device)\n",
        "\n",
        "    def _update_encoder(self):\n",
        "        for i in range(len(self.encoder)):\n",
        "            self.encoder[i].load_state_dict(self.online_model[i].state_dict())\n",
        "\n",
        "    def _initialize_optimizer_scheduler(self):\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.online_model.parameters(),\n",
        "            lr=self.config[\"learning_rate\"],\n",
        "            betas=(0.9, 0.999),\n",
        "            eps=1e-8,\n",
        "            weight_decay=self.config[\"weight_decay\"]\n",
        "        )\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=200, verbose=True)\n",
        "\n",
        "    def _forward_pass(self, model, sentences, train):\n",
        "        if train:\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "        features = model.tokenize(sentences)\n",
        "        features = {k: v.to(self.device, non_blocking=True) for k, v in features.items()}\n",
        "        with autocast(\"cuda\", enabled=self.config.get(\"use_amp\", True)):\n",
        "            embeddings = model[0](features)[\"token_embeddings\"]\n",
        "            pooled = model[1]({\"token_embeddings\": embeddings})[\"sentence_embedding\"]\n",
        "            if len(model) > 2:\n",
        "                return model[2](pooled)\n",
        "            else:\n",
        "                return pooled\n",
        "\n",
        "    def _mixed_barlow_twins_loss(self, z_a, z_b):\n",
        "        N, D = z_a.size()\n",
        "        z_a_norm = (z_a - z_a.mean(dim=0)) / (z_a.std(dim=0) + 1e-6)\n",
        "        z_b_norm = (z_b - z_b.mean(dim=0)) / (z_b.std(dim=0) + 1e-6)\n",
        "        c = torch.matmul(z_a_norm.T, z_b_norm) / N\n",
        "        I = torch.eye(D, device=z_a.device)\n",
        "        c_diff = (c - I).pow(2)\n",
        "        off_diag_mask = ~torch.eye(D, dtype=torch.bool, device=z_a.device)\n",
        "        c_diff[off_diag_mask] *= self.config[\"lambda_bt\"]\n",
        "        loss_bt = c_diff.sum()\n",
        "        # MixUp Regularization\n",
        "        idx = torch.randperm(N)\n",
        "        alpha = torch.tensor(np.random.beta(1.0, 1.0), device=z_a.device, dtype=z_a.dtype)\n",
        "        z_m = alpha * z_a + (1 - alpha) * z_b[idx, :]\n",
        "        z_m_norm = (z_m - z_m.mean(dim=0)) / (z_m.std(dim=0) + 1e-6)\n",
        "        cc_m_a = torch.matmul(z_m_norm.T, z_a_norm) / N\n",
        "        cc_m_b = torch.matmul(z_m_norm.T, z_b_norm) / N\n",
        "        cc_m_a_gt = alpha * torch.matmul(z_a_norm.T, z_a_norm) / N + (1 - alpha) * torch.matmul(z_b_norm[idx, :].T, z_a_norm) / N\n",
        "        cc_m_b_gt = alpha * torch.matmul(z_a_norm.T, z_b_norm) / N + (1 - alpha) * torch.matmul(z_b_norm[idx, :].T, z_b_norm) / N\n",
        "        loss_mix = self.config[\"lambda_mixup\"] * self.config[\"lambda_bt\"] * (\n",
        "            (cc_m_a - cc_m_a_gt).pow(2).sum() + (cc_m_b - cc_m_b_gt).pow(2).sum()\n",
        "        )\n",
        "        return loss_bt + loss_mix\n",
        "\n",
        "    def _evaluate_without_heads(self):\n",
        "        self._update_encoder()\n",
        "        self.encoder.eval()\n",
        "        indices = list(range(len(self.eval_dataset[\"sentence1\"])))\n",
        "        random.shuffle(indices)\n",
        "        sentences1 = [self.eval_dataset[\"sentence1\"][i] for i in indices]\n",
        "        sentences2 = [self.eval_dataset[\"sentence2\"][i] for i in indices]\n",
        "        scores = [self.eval_dataset[\"score\"][i] for i in indices]\n",
        "        evaluator = EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)\n",
        "        return evaluator(self.encoder)\n",
        "\n",
        "    def fit(self):\n",
        "        latest_eval_metrics = {}\n",
        "        for epoch in range(self.config[\"epochs\"]):\n",
        "            early_stop = False\n",
        "            epoch_loss = 0\n",
        "            pbar = tqdm(self.train_data_loader, desc=f\"Epoch {epoch+1}/{self.config['epochs']}:\")\n",
        "            for idx, sentences in enumerate(pbar):\n",
        "                s1 = self._apply_augmentation(sentences, self._get_random_augmentation())\n",
        "                s2 = self._apply_augmentation(sentences, self._get_random_augmentation())\n",
        "                with autocast(\"cuda\", enabled=self.config.get(\"use_amp\", True)):\n",
        "                    z_a = self._forward_pass(self.online_model, s1, train=True)\n",
        "                    z_b = self._forward_pass(self.online_model, s2, train=True)\n",
        "                    loss = self._mixed_barlow_twins_loss(z_a, z_b)\n",
        "                epoch_loss += loss.item()\n",
        "                self.optimizer.zero_grad()\n",
        "                self.scaler.scale(loss).backward()\n",
        "                scale = self.scaler.get_scale()\n",
        "                scale = scale if scale != 0 else 1e-8\n",
        "                total_norm_scaled = 0.0\n",
        "                for param in self.online_model.parameters():\n",
        "                    if param.grad is not None:\n",
        "                        param_norm = param.grad.data.norm(2).item()\n",
        "                        total_norm_scaled += param_norm ** 2\n",
        "                total_norm_scaled = math.sqrt(total_norm_scaled)\n",
        "                total_norm = total_norm_scaled / scale\n",
        "                mean_grad_norm = total_norm / len(list(self.online_model.parameters()))\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "                self.scheduler.step(loss)\n",
        "                if idx % self.evaluate_steps == 0 or idx in [0, len(self.train_data_loader)-1]:\n",
        "                    latest_eval_metrics = self._evaluate_without_heads()\n",
        "                    last_spearman = latest_eval_metrics.get('spearman_cosine', -float('inf'))\n",
        "                    last_pearson = latest_eval_metrics.get('pearson_cosine', -float('inf'))\n",
        "                    self._update_encoder()\n",
        "                    self.encoder.eval()\n",
        "                    test_metrics = self.test_evaluator(self.encoder)\n",
        "                    self.test_sts_pearson_cosine_values.append(test_metrics.get('pearson_cosine', np.nan))\n",
        "                    self.test_sts_spearman_cosine_values.append(test_metrics.get('spearman_cosine', np.nan))\n",
        "                    self.test_iterations.append(idx)\n",
        "                    self._update_plot()\n",
        "\n",
        "                    if last_spearman > self.best_spearman:\n",
        "                        self.best_spearman = last_spearman\n",
        "                        self.best_pearson = last_pearson\n",
        "                        self.patience_counter = 0\n",
        "                    else:\n",
        "                        self.patience_counter += self.evaluate_steps\n",
        "                pbar.set_postfix({\n",
        "                    \"loss\": loss.item(),\n",
        "                    **latest_eval_metrics,\n",
        "                    \"mean_grad_norm\": mean_grad_norm,\n",
        "                    \"learning_rate\": self.optimizer.param_groups[0]['lr']\n",
        "                })\n",
        "                self.loss_values.append(loss.item())\n",
        "                self.sts_pearson_cosine_values.append(latest_eval_metrics.get('pearson_cosine', np.nan))\n",
        "                self.sts_spearman_cosine_values.append(latest_eval_metrics.get('spearman_cosine', np.nan))\n",
        "                self.mean_grad_norm_values.append(mean_grad_norm)\n",
        "                self.variance_values.append(torch.var(z_a, dim=0).mean().item())\n",
        "                self.learning_rate_values.append(self.optimizer.param_groups[0]['lr'])\n",
        "                self.iterations.append(idx)\n",
        "                self.epochs.append(epoch)\n",
        "                if idx % self.evaluate_steps == 0 or idx in [0, len(self.train_data_loader)-1]:\n",
        "                    if self.patience_counter >= self.config[\"patience\"]:\n",
        "                        early_stop = True\n",
        "                        print(f\"Early stopping triggered at epoch {epoch+1}, iteration {idx}.\")\n",
        "                        print(f\"Best Spearman Correlation: {self.best_spearman}\")\n",
        "                        print(f\"Best Pearson Correlation: {self.best_pearson}\")\n",
        "                        break\n",
        "            if early_stop:\n",
        "                break\n",
        "            avg_loss = epoch_loss / len(self.train_data_loader)\n",
        "            pbar.set_description(f\"Epoch {epoch+1} Loss: {avg_loss}\")\n",
        "\n",
        "    def cleanup(self):\n",
        "        del self.online_model, self.optimizer, self.scheduler, self.scaler\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "class SentenceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sentences):\n",
        "        self.sentences = sentences\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentences[idx]\n",
        "\n",
        "# Mount Google Drive for checkpoint and study persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/violet_checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "STUDY_DB_PATH = os.path.join(CHECKPOINT_DIR, \"llm_finetuning_study.db\")\n",
        "\n",
        "def save_checkpoint(trial_number, trainer):\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_trial_{trial_number}.pt\")\n",
        "    torch.save({\n",
        "        'online_model_state_dict': trainer.online_model.state_dict(),\n",
        "        'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
        "        'scheduler_state_dict': trainer.scheduler.state_dict(),\n",
        "        'best_spearman': trainer.best_spearman,\n",
        "        'best_pearson': trainer.best_pearson,\n",
        "        'epochs': trainer.epochs,\n",
        "        'iterations': trainer.iterations,\n",
        "    }, checkpoint_path)\n",
        "\n",
        "    # Also save the graph to a subfolder called \"graphs\"\n",
        "    graphs_dir = os.path.join(CHECKPOINT_DIR, \"graphs\")\n",
        "    os.makedirs(graphs_dir, exist_ok=True)\n",
        "    graph_path = os.path.join(graphs_dir, f\"checkpoint_trial_{trial_number}.png\")\n",
        "    trainer.fig.write_image(graph_path)\n",
        "    print(f\"Checkpoint and graph saved to {checkpoint_path} and {graph_path}\")\n",
        "\n",
        "    return checkpoint_path\n",
        "\n",
        "def load_checkpoint(trainer, checkpoint_path):\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        trainer.online_model.load_state_dict(checkpoint['online_model_state_dict'])\n",
        "        trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        trainer.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        trainer.best_spearman = checkpoint.get('best_spearman', -float(\"inf\"))\n",
        "        trainer.best_pearson = checkpoint.get('best_pearson', -float(\"inf\"))\n",
        "        trainer.epochs = checkpoint.get('epochs', [])\n",
        "        trainer.iterations = checkpoint.get('iterations', [])\n",
        "        print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
        "\n",
        "def objective(trial):\n",
        "    # Experiment with various hyperparameters\n",
        "    config = {\n",
        "        \"model_name\": \"distilbert-base-uncased\",\n",
        "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512]),\n",
        "        \"projection_depth\": trial.suggest_int(\"projection_depth\", 2, 6),\n",
        "        \"projection_size\": trial.suggest_categorical(\"projection_size\", [2048, 4096, 6144, 8192]),\n",
        "        \"epochs\": 1,  # fixed at 1\n",
        "        \"warmup_proportion\": 0.0,\n",
        "        \"max_seq_length\": trial.suggest_categorical(\"max_seq_length\", [32, 64, 75]),\n",
        "        \"aug_p\": trial.suggest_uniform(\"aug_p\", 0.2, 0.4),\n",
        "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 3e-5, 1e-3),\n",
        "        \"model_save_path\": os.path.join(CHECKPOINT_DIR, f\"train_stsb_bt-distilbert-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_trial{trial.number}\"),\n",
        "        \"num_workers\": 10,\n",
        "        \"weight_decay\": trial.suggest_uniform(\"weight_decay\", 0.1, 0.2),\n",
        "        \"lambda_bt\": trial.suggest_uniform(\"lambda_bt\", 0.001, 0.2),\n",
        "        \"lambda_mixup\": trial.suggest_uniform(\"lambda_mixup\", 0.6, 1.5),\n",
        "        \"use_amp\": True,\n",
        "        \"patience\": 500\n",
        "    }\n",
        "\n",
        "    trainer = BarlowTwinsNCSE(config)\n",
        "    # Resume from checkpoint if available\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_trial_{trial.number}.pt\")\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        load_checkpoint(trainer, checkpoint_path)\n",
        "\n",
        "    try:\n",
        "        trainer.fit()\n",
        "    except KeyboardInterrupt:\n",
        "        save_checkpoint(trial.number, trainer)\n",
        "        raise optuna.TrialPruned(\"Trial interrupted and checkpoint saved.\")\n",
        "\n",
        "    # Save checkpoint at end of trial\n",
        "    save_checkpoint(trial.number, trainer)\n",
        "\n",
        "    # Use the STS-B validation metric (spearman cosine) as objective.\n",
        "    val_metrics = trainer._evaluate_without_heads()\n",
        "    return val_metrics.get(\"spearman_cosine\", -float(\"inf\"))\n",
        "\n",
        "# Create or resume an Optuna study persisted on Google Drive\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=\"llm_finetuning_study\",\n",
        "    storage=f\"sqlite:///{STUDY_DB_PATH}\",\n",
        "    load_if_exists=True\n",
        ")\n",
        "study.optimize(objective, n_trials=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "29kySzsbp6Z9",
        "outputId": "78bfe7b5-c8a0-493a-c0c1-7142474f962f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"45d58fbd-bf86-4d1a-8262-402bc1021648\" class=\"plotly-graph-div\" style=\"height:800px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"45d58fbd-bf86-4d1a-8262-402bc1021648\")) {                    Plotly.newPlot(                        \"45d58fbd-bf86-4d1a-8262-402bc1021648\",                        [{\"mode\":\"lines+markers\",\"name\":\"Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923],\"y\":[2184.04931640625,1983.4031982421875,2353.45263671875,2254.07666015625,2084.581787109375,2133.153564453125,2503.723388671875,2078.739013671875,2005.5802001953125,2266.14697265625,2104.712646484375,3401.214111328125,3609.413330078125,2888.32763671875,3574.197021484375,3122.470703125,2397.123046875,4627.8798828125,3877.140625,8705.0146484375,5476.33056640625,6641.5224609375,5605.20751953125,4989.2197265625,6068.1494140625,5906.90625,7050.85107421875,5772.2763671875,5937.7392578125,5983.46875,4966.58642578125,5047.068359375,5064.64111328125,4171.52783203125,4845.81640625,4704.21533203125,4271.83642578125,4129.44775390625,4225.4052734375,3870.098388671875,4036.8369140625,4191.96337890625,3985.055419921875,3870.5546875,3672.560791015625,3871.08447265625,3605.823486328125,3602.1943359375,3504.866455078125,3269.451171875,3242.383056640625,3500.841552734375,3260.43017578125,3194.70166015625,3089.480224609375,3261.708251953125,2973.025146484375,3248.0244140625,3132.086669921875,2972.9873046875,3015.1328125,3062.911376953125,2897.294921875,2905.134521484375,3083.473388671875,2883.169921875,3007.33447265625,2855.205322265625,2749.44482421875,2665.544189453125,2795.4677734375,2898.389892578125,2539.48095703125,2657.467041015625,2672.58447265625,2807.129638671875,2626.128662109375,2635.3115234375,2603.81884765625,2538.813720703125,2629.49755859375,2585.46875,2729.25390625,2889.599365234375,2571.7509765625,2546.88232421875,2639.643310546875,2677.061279296875,2613.91748046875,2542.6005859375,2387.650146484375,2551.9814453125,2625.17431640625,2288.15234375,2543.691650390625,2299.156494140625,2452.7109375,2413.29150390625,2422.348876953125,2410.810302734375,2441.215087890625,2360.94921875,2312.332763671875,2253.322265625,2313.64013671875,2221.976318359375,2369.6708984375,2344.638671875,2372.512451171875,2344.41748046875,2391.457275390625,2298.00732421875,2366.61962890625,2425.797607421875,2383.840087890625,2473.2294921875,2344.06982421875,2277.3046875,2162.002685546875,2240.65771484375,2090.579345703125,2160.350830078125,2187.04638671875,2222.5771484375,2342.02783203125,2140.847900390625,2374.008544921875,2212.7919921875,2404.049560546875,2151.045654296875,2162.59716796875,2167.560546875,2292.27197265625,2179.55859375,2102.42041015625,2118.402587890625,2123.3642578125,2138.322021484375,2250.0400390625,2078.17822265625,2177.81396484375,2006.2635498046875,2052.32666015625,2146.7099609375,2148.5205078125,2104.013916015625,2160.99267578125,1984.7083740234375,2044.4686279296875,2115.667724609375,2120.58544921875,1979.99560546875,2006.0946044921875,1957.8602294921875,2156.751708984375,2191.641845703125,1942.1488037109375,2018.19677734375,1958.3203125,2024.4345703125,2068.90380859375,2025.610595703125,2019.18603515625,1931.1683349609375,1973.284423828125,1996.4876708984375,1923.44384765625,2019.7890625,1999.924072265625,1989.446533203125,2035.8707275390625,1963.87255859375,2075.565185546875,1843.8394775390625,2055.10205078125,2073.451171875,2063.59228515625,1982.7244873046875,2066.51806640625,2153.405517578125,1933.9688720703125,1999.5731201171875,2122.982177734375,2003.0164794921875,2039.780517578125,1887.970458984375,1996.6365966796875,1970.2550048828125,2181.506591796875,1941.0601806640625,2212.856201171875,1863.734619140625,1991.4134521484375,1986.8101806640625,2062.920166015625,1849.864501953125,1870.833740234375,1964.9537353515625,2091.39453125,2100.997802734375,1884.25830078125,2044.0526123046875,1936.533935546875,1932.615478515625,1844.2413330078125,1928.952880859375,2046.32666015625,1942.5894775390625,1924.6131591796875,1875.9683837890625,1823.730712890625,1824.0518798828125,1844.1058349609375,1881.4888916015625,2040.17333984375,1945.9327392578125,1858.8936767578125,1933.2437744140625,1872.61962890625,2024.79443359375,2040.5474853515625,1942.0767822265625,1888.067138671875,1758.4969482421875,1922.0318603515625,1880.570556640625,1851.70654296875,1946.4593505859375,2136.203369140625,2063.121337890625,1772.0777587890625,1884.97314453125,2060.81005859375,1788.6715087890625,1864.437744140625,1810.082763671875,2000.97802734375,1800.813232421875,1968.85546875,1891.5966796875,1899.931884765625,1815.623779296875,1957.54736328125,1907.1517333984375,1915.7181396484375,1935.09912109375,1857.7066650390625,1961.375,1759.22998046875,2015.0150146484375,1999.161865234375,1779.4990234375,1885.5833740234375,1958.8857421875,1964.07373046875,1864.060546875,1960.498291015625,1860.5762939453125,1770.842529296875,1780.5047607421875,1808.705078125,1843.1641845703125,1863.455322265625,1819.564697265625,2007.3016357421875,1856.6392822265625,2040.84521484375,1856.37841796875,1962.5206298828125,1872.885498046875,1787.365966796875,1705.6119384765625,1715.3443603515625,1812.4407958984375,1827.584228515625,1754.346923828125,1798.8155517578125,1844.5072021484375,1817.0223388671875,1987.3961181640625,1721.669189453125,1792.4080810546875,1689.3326416015625,2030.359375,1935.2386474609375,1718.1163330078125,1818.704833984375,1988.37060546875,1947.5478515625,1657.03125,1698.67724609375,1947.4478759765625,1856.3809814453125,1764.092529296875,1818.61865234375,1690.2720947265625,1782.00244140625,1706.0611572265625,1780.5560302734375,1787.09716796875,1943.7528076171875,2044.2430419921875,1710.953125,1844.69873046875,1792.9075927734375,1783.7379150390625,1916.963623046875,1847.9573974609375,1701.2054443359375,1629.361572265625,1671.542236328125,1736.3023681640625,1866.7186279296875,1626.2861328125,1724.7769775390625,1680.0926513671875,1691.5908203125,2005.597900390625,1915.6448974609375,1659.0665283203125,1632.9630126953125,1805.438232421875,1977.181640625,1755.7713623046875,1780.0418701171875,1768.3831787109375,1815.577392578125,1796.5587158203125,1630.0545654296875,1870.797119140625,1827.4119873046875,1790.0096435546875,1751.2828369140625,1714.6748046875,1939.12255859375,1737.22216796875,1778.3409423828125,1631.6680908203125,1767.0589599609375,1753.0928955078125,1697.05908203125,1797.8033447265625,1793.31494140625,1762.67919921875,1850.1617431640625,1807.800048828125,1888.177490234375,1908.947509765625,1724.9886474609375,1712.3734130859375,1819.3165283203125,1921.387451171875,1809.8427734375,1802.9951171875,1801.5897216796875,1701.462646484375,1751.3837890625,1786.652587890625,1707.57080078125,1846.1514892578125,1809.2276611328125,1645.0750732421875,1790.6729736328125,1670.666259765625,1610.7281494140625,1675.9735107421875,1812.88720703125,1619.8223876953125,1720.11376953125,1959.7518310546875,1730.88427734375,1625.269287109375,1682.1165771484375,1682.94873046875,1771.28125,1632.952392578125,1668.057373046875,1779.508544921875,1863.0364990234375,1858.46240234375,1646.5313720703125,1895.7626953125,1690.8944091796875,1880.478271484375,1729.5030517578125,1851.7313232421875,1938.54052734375,1734.018310546875,1688.9522705078125,1660.7005615234375,1889.736083984375,1698.5377197265625,1765.724365234375,1713.4193115234375,1774.1422119140625,1766.2008056640625,1649.6806640625,1650.837890625,1837.87548828125,1725.1451416015625,1802.7637939453125,1685.5811767578125,1610.7413330078125,1658.5269775390625,1761.90673828125,1648.1513671875,1650.2696533203125,1941.12158203125,1853.60107421875,1648.056396484375,1648.9647216796875,1691.39599609375,1725.3314208984375,1664.8206787109375,1579.3760986328125,1768.5262451171875,1694.365478515625,1575.0360107421875,1710.62841796875,1749.15673828125,1672.88671875,1641.230224609375,1638.2042236328125,1788.3424072265625,1710.8719482421875,1700.314697265625,1720.8782958984375,1844.096923828125,1932.069580078125,1791.4766845703125,1756.2938232421875,1811.583984375,1622.189453125,1608.297607421875,1633.774658203125,1658.5855712890625,1729.297119140625,1679.865234375,1640.606689453125,1734.8829345703125,1694.4578857421875,1662.9088134765625,1767.036865234375,1682.9365234375,1881.9139404296875,1704.71484375,1604.936767578125,1687.24169921875,1599.4986572265625,1874.391357421875,1749.858642578125,1666.9678955078125,1742.6834716796875,1714.3948974609375,1724.53466796875,1730.8233642578125,1742.1802978515625,1675.146240234375,1687.826904296875,1667.973388671875,1741.0731201171875,1568.669921875,1855.1732177734375,1581.217529296875,1703.8634033203125,1651.5819091796875,1643.476318359375,1795.859130859375,1726.215576171875,1637.117919921875,1665.7601318359375,1595.8302001953125,1526.0782470703125,1759.78515625,1673.4251708984375,1730.4197998046875,1740.5770263671875,1649.74169921875,1673.830810546875,1651.449462890625,1799.7156982421875,1609.931396484375,1677.3193359375,1544.12841796875,1608.07177734375,1652.18896484375,1537.2838134765625,1668.48193359375,1589.5181884765625,1659.6143798828125,1864.2921142578125,1576.16162109375,1643.4678955078125,1851.994384765625,1723.4219970703125,1695.15771484375,1667.596923828125,1791.7008056640625,1739.633544921875,1660.8448486328125,1714.7882080078125,1595.81396484375,1680.565185546875,1735.9949951171875,1658.9197998046875,1781.302001953125,1772.92919921875,1647.8841552734375,1563.7022705078125,1610.2608642578125,1644.480224609375,1573.8887939453125,1818.3568115234375,1825.7406005859375,1544.078857421875,1820.8056640625,1722.438232421875,1574.5010986328125,1748.9564208984375,1600.0245361328125,1611.709716796875,1550.394775390625,1767.21826171875,1681.5467529296875,1644.09033203125,1712.6585693359375,1551.542724609375,1545.43701171875,1712.346923828125,1795.7347412109375,1816.9864501953125,1662.9892578125,1584.65673828125,1612.86865234375,1660.620849609375,1684.9063720703125,1616.299072265625,1633.3564453125,1510.9185791015625,1771.213623046875,1777.3662109375,1530.2935791015625,1749.89794921875,1694.1256103515625,1841.9532470703125,1632.1435546875,1626.1676025390625,1748.9197998046875,1730.97314453125,1665.6622314453125,1708.891357421875,1622.9208984375,1755.0118408203125,1678.03076171875,1799.8338623046875,1648.1695556640625,1588.328857421875,1676.1573486328125,1583.5804443359375,1866.173828125,1540.6219482421875,1513.43359375,1552.37451171875,1611.457275390625,1524.7220458984375,1680.0928955078125,1687.6571044921875,1693.9561767578125,1742.46337890625,1633.2938232421875,1794.4957275390625,1691.961181640625,1612.406494140625,1618.7484130859375,1627.1107177734375,1653.280029296875,1733.948486328125,1811.252197265625,1774.274169921875,1639.3599853515625,1721.7550048828125,1643.200927734375,1780.7841796875,1725.0399169921875,1572.7352294921875,1529.957763671875,1671.322509765625,1528.382080078125,1636.955810546875,1626.020263671875,1804.8883056640625,1700.280029296875,1765.4384765625,1648.707763671875,1664.78125,1770.2423095703125,1632.2452392578125,1627.826416015625,1637.997314453125,1802.9091796875,1712.6427001953125,1612.778564453125,1635.864501953125,1657.6328125,1650.10205078125,1858.840576171875,1805.4468994140625,1597.86328125,1613.146240234375,1629.639892578125,1708.8358154296875,1626.736572265625,1612.7337646484375,1606.506591796875,1576.52294921875,1646.4681396484375,1548.4552001953125,1690.074951171875,1861.4100341796875,1702.587158203125,1605.5899658203125,1795.1888427734375,1680.474365234375,1710.9532470703125,1641.439697265625,1601.8741455078125,1686.92431640625,1567.973388671875,1700.46240234375,1594.6082763671875,1653.652587890625,1686.740966796875,1654.4967041015625,1561.6160888671875,1641.1441650390625,1713.9815673828125,1606.5009765625,1560.056884765625,1798.01171875,1650.976318359375,1738.92919921875,1629.9254150390625,1761.7381591796875,1788.0682373046875,1696.8671875,1650.7532958984375,1597.110595703125,1671.492431640625,1752.4537353515625,1545.0106201171875,1555.508544921875,1723.8768310546875,1634.0345458984375,1588.721923828125,1633.80712890625,1744.7935791015625,1660.8818359375,1630.013916015625,1592.0196533203125,1710.1221923828125,1762.029541015625,1696.931884765625,1709.3885498046875,1709.27197265625,1721.071044921875,1534.6181640625,1530.628173828125,1818.1724853515625,1529.1168212890625,1542.11572265625,1697.0540771484375,1642.0443115234375,1718.1134033203125,1734.6785888671875,1955.7269287109375,1612.0548095703125,1773.982421875,1778.07958984375,1585.538818359375,1599.7490234375,1726.688232421875,1602.88671875,1841.69482421875,1618.055419921875,1711.3094482421875,1793.0,1535.4974365234375,1542.757568359375,1576.6749267578125,1773.0750732421875,1676.835693359375,1643.352294921875,1683.1326904296875,1595.1279296875,1575.72216796875,1624.6778564453125,1517.22265625,1600.0406494140625,1534.5665283203125,1628.044189453125,1764.3499755859375,1651.2735595703125,1512.8416748046875,1721.7869873046875,1596.7655029296875,1615.77880859375,1573.3056640625,1677.6065673828125,1615.9671630859375,1610.1622314453125,1690.5177001953125,1549.9678955078125,1627.4439697265625,1595.456298828125,1659.22802734375,1579.873779296875,1540.76806640625,1646.6473388671875,1659.3668212890625,1695.214111328125,1755.4193115234375,1865.73583984375,1608.6171875,1806.2353515625,1708.7181396484375,1633.813232421875,1529.949462890625,1736.136474609375,1717.490966796875,1537.7904052734375,1655.935546875,1613.2894287109375,1557.215576171875,1549.6387939453125,1669.3841552734375,1652.816162109375,1883.6767578125,1475.8017578125,1776.657470703125,1535.2059326171875,1667.9176025390625,1697.70068359375,1682.02587890625,1573.8223876953125,1747.4732666015625,1588.5858154296875,1683.8055419921875,1684.2923583984375,1593.50634765625,1493.4482421875,1722.9737548828125,1628.6561279296875,1833.414306640625,1651.8326416015625,1620.477783203125,1702.2911376953125,1567.3885498046875,1684.05615234375,1783.7095947265625,1566.0726318359375,1640.2757568359375,1541.9832763671875,1492.8621826171875,1559.2955322265625,1608.587158203125,1796.85009765625,1608.3258056640625,1627.50048828125,1584.294921875,1605.550048828125,1592.84716796875,1572.8023681640625,1503.39697265625,1520.3907470703125,1607.875,1460.3983154296875,1533.839111328125,1545.7391357421875,1557.441650390625,1637.578369140625,1523.206787109375,1542.3277587890625,1657.6065673828125,1580.796630859375,1768.15869140625,1672.0279541015625,1599.3695068359375,1497.514404296875,1693.6761474609375,1615.431884765625,1663.1387939453125,1810.5506591796875,1661.30126953125,1698.3651123046875,1553.73095703125,1677.7958984375,1692.6414794921875,1711.646240234375,1580.0029296875,1688.468505859375,1687.14013671875,1555.5155029296875,1659.143798828125,1693.280029296875,1548.0535888671875,1683.82763671875,1718.7196044921875,1557.8599853515625,1692.7822265625,1647.867431640625,1591.0401611328125,1678.7830810546875,1638.5780029296875,1517.833740234375,1646.35546875,1749.5714111328125,1751.5517578125,1587.82421875,1505.72802734375,1666.2259521484375,1701.48828125,1739.3485107421875,1682.9757080078125,1641.47412109375,1725.18994140625,1625.9686279296875,1683.45703125,1571.336669921875,1541.7601318359375,1609.5576171875,1531.2562255859375,1670.834228515625,1728.03759765625,1607.4444580078125,1687.792236328125,1585.863037109375,1700.5504150390625,1625.1669921875,1675.2762451171875,1664.1427001953125,1573.153076171875,1759.489501953125,1674.961669921875,1689.684814453125,1508.6636962890625,1537.14208984375,1676.8367919921875,1587.60107421875,1575.19970703125,1582.2796630859375,1531.721435546875,1646.64013671875,1558.443359375,1586.7362060546875,1733.926025390625,1552.8822021484375,1663.404541015625,1584.673583984375,1753.7603759765625,1718.53125,1725.0479736328125,1753.980224609375,1646.6717529296875,1624.05224609375,1501.5184326171875,1554.085693359375,1746.2984619140625,1760.218994140625,1623.32177734375,1612.3231201171875,1548.0450439453125,1572.1817626953125,1533.09228515625,1527.5013427734375,1655.37060546875,1602.680908203125,1573.357421875,1651.0838623046875,1643.25439453125,1549.3087158203125,1656.6068115234375,1535.1982421875,1664.527099609375,1539.7725830078125,1655.2696533203125,1655.29443359375,1757.6768798828125,1636.640869140625,1736.9434814453125,1641.236083984375,1634.2689208984375,1654.700927734375,1647.0391845703125,1566.8438720703125,1705.439208984375,1662.5692138671875,1565.0712890625,1657.4320068359375,1591.241943359375,1668.448974609375,1727.3717041015625,1698.8590087890625,1657.8038330078125,1622.94287109375,1637.342529296875,1631.055419921875,1552.53369140625,1573.811767578125,1554.42724609375,1560.052490234375,1528.4698486328125,1736.9091796875,1653.656005859375,1648.004638671875,1758.7308349609375,1688.8389892578125,1618.995361328125,1643.1900634765625,1628.1287841796875,1563.1634521484375,1646.097412109375,1594.6884765625,1568.364501953125,1521.877685546875],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"lines+markers\",\"name\":\"Mean Gradient Norm\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923],\"y\":[null,null,null,null,null,null,null,null,null,null,59.35167831746244,null,null,null,null,306.5546045709554,72.61591212651632,489.3573525786012,263.6552233903265,960.9733253900438,371.4001178161901,349.0667201027965,248.92315954116174,372.5783160467275,708.2256565313642,289.33608116331317,841.3395286712109,524.7248677425353,566.6775362666447,502.91771030184685,146.917826149838,126.52501221094883,143.58779166490064,83.86337658662343,140.48900015416578,103.66774653247911,92.88343862080819,110.1174509803741,90.94526483220314,63.15189715055965,69.89080149308793,78.71314986062082,87.28684072537038,72.8254882707523,70.73483012969983,77.70276596833638,55.897425901333406,57.914243547439504,63.1630449607388,51.118454082138825,54.122100748506995,68.4505376090699,50.87320102453999,48.46543306668106,46.49335581348463,45.680150623943966,44.066624521551226,55.06500161567298,62.13202090828731,43.17003397958166,36.40101980710659,42.63483891853614,42.25891426652258,35.81359104591229,42.022015246048824,34.35230011160471,43.83310923772364,41.76800949657681,35.855944540346115,30.896380880414434,31.25549702732283,35.38330757208764,24.819259130022118,30.07271841163393,25.87718452581091,31.179479924012448,32.10428105760644,30.300288143479023,28.911834906051045,25.572476278043297,28.00564926482603,32.97649456381382,27.316423372946975,39.10005248389424,26.99570390878185,23.65735977878964,26.16770763506326,32.44284903515792,25.275287708104702,35.36355160459561,24.030872802454887,26.61063036332636,28.676134584421806,23.936065120915256,27.007760212699438,21.94705075717484,20.827193486846244,22.77617980059793,24.065880778314504,24.00520221866148,22.304742109115956,20.35226195929253,22.421690044064608,23.469635990457697,19.883542671162925,19.174236868430427,24.147884885813557,23.513872712263485,23.127027814569246,25.002410774688315,22.020046339717545,20.019821696611192,22.321824692557716,23.19840866260517,25.1125348483023,22.54649183153815,24.361715892051066,19.48016345774146,18.761138284344046,19.876765991934505,19.487428281445137,19.574450896783926,19.87048564143823,21.204156235047794,22.611338943490324,18.953734916583215,27.79261319475205,21.69333266837987,25.70610454580254,27.00553152851499,19.64044205285247,19.321783541704256,20.495627709076594,18.09206882457383,17.26363174670825,20.829564786656462,18.629393632228673,20.658954746090988,24.04818120601673,16.116160823782828,20.527043385240933,15.780795402681292,18.078103868882188,18.75774330085104,21.4851621193212,17.583824707027198,16.578097428786275,19.564667245467966,16.54373953236175,16.547041088353833,15.8813218966271,17.57534175564776,16.04253746656879,14.965285180897729,24.20751149267438,17.1191152521693,15.082735385832425,17.768574392584004,16.596856315005496,18.85545735111945,23.77196587619382,18.60653070989076,14.9450128180208,16.885048517940454,19.094838786141736,16.193214891599666,17.58020127421265,17.794120134137103,15.315698071392534,16.558055135897977,18.000650080878962,16.36491174280205,19.697236581282567,15.362788399193887,15.27049383011941,18.27363344543651,21.819562447504936,17.47579632500409,20.54505447178388,19.626770474923703,17.754186913108843,16.255399995409444,19.943940443684237,16.582470815647984,17.108474799949356,14.711217286289068,16.7605406416572,16.554187942721775,21.34768396715195,19.154606905090844,19.98349705865133,13.748692052138402,16.10634188483536,17.430118726414168,17.36981702932686,15.660138587098016,13.89342110735946,17.60655633940754,17.46976497600602,18.640534842643728,15.426293512694425,20.029831840634746,14.949202888595126,19.392764496257907,15.12345112757001,17.849152444532397,16.035736590649076,16.53009361700838,17.02941622950352,13.108884936064905,14.682066825349589,14.296728504857935,14.300782675446747,14.37010159806794,16.067616597057025,13.828744421791987,14.10305805033886,15.781987294386186,14.912670399423984,15.443716780188769,17.34265077428923,15.358702664915313,15.072223032793367,16.203686779560268,17.18398849394286,18.159055060554618,15.13525222257343,15.410815864355126,19.384127755255182,19.329871941737874,14.937076370463087,15.699508041244778,18.444550915487074,15.322488182466227,13.019406022565919,14.3333264924755,16.425669238536493,13.718624976650066,18.130816162945887,17.135276490186005,14.461488998862043,14.456279061180172,16.982887934835087,16.76285589438988,15.834617682303328,15.381747926578546,15.85929528058026,13.494879212300148,15.259907943793703,16.480606675174833,19.00057713378309,17.158889374789254,14.094688036389634,17.118756436681135,17.414836320467185,16.537032941446626,16.174189761692592,14.122131635840754,14.446401193353626,19.257438887239605,13.720008872903607,12.20455070129343,14.886760399366423,16.4090322643977,15.933423023682682,14.068018103754032,16.604893144648944,15.474632968350999,15.2730167808193,19.051777591321024,13.86759284409592,12.872435655449543,13.376508985218864,14.953289892278997,14.32463987933782,13.459106160652299,11.705859540205646,16.261443341075925,13.955746537116726,16.23584098878221,14.114566055667556,13.388893314860299,11.286333503182256,16.312554536161723,17.28991285455913,13.16764921957219,13.679088256467894,17.277606742428418,17.17414405192574,11.962449670689757,14.871222399426088,16.471579334755525,16.115659594624415,12.417630735015036,14.933560997853453,14.621313652854122,16.278455375719656,12.147900895301497,12.694329264727529,12.861870448507974,15.893836865567218,18.696017987630523,12.819056561363224,12.844753775593022,14.906080716554586,12.316166205791117,17.11316043836268,15.044024920190619,13.351255571311722,12.593079133443949,11.467427343919525,14.63292632770299,16.80879950880307,11.324897313132302,11.077463280842476,13.81668915266181,13.34396143260978,16.147155659776452,17.51421423111112,12.472168276179977,12.519589686904512,15.59309535285485,18.553001636081767,16.673310169959137,13.75376179031301,12.105140400024132,15.701887909607082,14.442250343350727,12.307553352678944,15.365757118484428,15.855817480742703,15.046211599252839,13.880416895612141,12.810346499393798,15.574883483708991,14.470202545212443,14.80593575200785,11.50939332742055,17.04179800236674,14.598176329532912,13.409198383576713,12.85249223537345,13.700024400722302,14.759504678230106,14.622628905154071,12.665505063477408,15.769726014457838,15.886274006067366,12.220625520784768,10.789369419945245,13.321627735855383,15.509309067291722,13.116741292237027,13.433928732154358,13.569991501901264,12.285846430719545,14.63071091410939,13.184132982517005,11.85706602055917,13.855902234755698,12.569406802860634,13.063405393246407,12.518397843821226,13.032570578947023,13.16701145853364,11.037656174171659,17.029010013174602,11.760412074440277,15.821450966724056,18.639601828328768,13.133110089687465,10.27881469379214,11.880710567592285,12.02287194093318,13.220069023494975,11.31754746203441,16.521256698466253,14.189707511153232,15.310333567585248,14.788777094955298,11.270261031679707,16.35197123490947,11.449790217345143,14.031833371022936,11.87218735088465,14.122356619230823,16.16234920709738,12.869615880265505,12.18136600511357,11.97442305284719,14.413993137938599,11.72444415504553,13.710097233987096,12.272130740338946,14.433060298469428,12.879609994464,10.742414682933802,13.162215070352893,13.344911275835726,13.253253718909079,16.974524522138157,11.374704691022437,13.521892306054514,11.5433317971455,14.52503701950329,12.368618187606632,12.007794409492467,17.87648228479681,14.487208387863387,12.210844019441124,12.461943002624318,11.5540353460123,11.172473031011098,15.354490474054948,10.496494263148733,14.96777461406754,11.247501523384603,10.496857068913302,13.080062705815894,15.476509053983838,11.195565963686931,11.516265267952031,10.63463849577094,14.710007142388813,11.66620144604864,11.929644060574237,13.762648344572373,12.924126282266654,15.745511617050507,15.89045747150036,11.791105171384352,14.767329767233885,16.825196226910634,10.736094144229218,10.928005675340145,12.885949133932627,11.23090236968173,11.931165944082625,12.179379670498157,12.64671086199947,14.767474341036955,11.543409697892713,14.31963608832238,11.280266313918512,16.585405943632036,15.44411616321985,10.019821424020911,11.665672715990087,9.882157439431998,14.870700019578447,10.91243090330858,12.07938158484801,12.486880086998726,15.567968261334697,16.268982531493524,12.482972885423898,14.227691533573005,12.99243639963452,13.685404625874064,13.3478963544975,12.022537167352356,12.963420497100099,16.100935907020354,11.351873628417582,12.847351963063986,18.33128418223075,12.367848573668807,16.19630901098356,11.80961864227213,12.951581670419213,12.819857664431396,11.51019846342992,11.618246750965081,12.566081884950291,10.137228421670978,13.896507701692169,15.27121589869902,10.604237630532735,12.31247924594042,11.370960691197068,14.877027361821456,11.470835643358214,12.059149183351616,11.698852337247866,9.869982908059422,13.269969962061074,15.422281151292337,12.509611440957462,10.791366041363855,11.512435485701294,16.4306076314058,10.177561157521101,12.240180967454357,15.551756723573993,12.384507089954885,12.494501871520576,11.012276740656361,12.867717489921576,12.78656494727358,11.11165899974939,12.080451522074718,11.182143002916298,13.53931069903831,13.250644440581658,18.30358987701666,13.321373009000279,12.996375445222984,11.800059966300784,10.804805689700878,12.730179029882132,10.350095852240063,10.768754251048719,14.292539675263264,14.845594034507286,10.195412895227415,14.199599683370135,11.964801949427814,9.667579070469372,14.413216735437034,10.236346499678367,10.562735941236923,9.20382198111472,12.846279059505795,12.32003825521722,10.92341050170777,10.779139500393514,10.721006737951901,12.092211106216812,13.875691830338132,14.159905316745492,15.892125683651038,12.926425156294211,13.012205475225066,10.515633428131322,12.821662432645024,12.6290349082857,10.099530181438386,11.072847026590455,10.784477188733007,15.574610075949973,11.736514978278768,11.116858880561104,13.317436288789875,12.94443618847568,15.14701762856848,11.864000781351672,10.306080197160988,12.989615405230694,13.231668892027773,12.110598079081825,12.909791050245595,10.816730607152701,17.32126050282405,11.954291555659317,12.806628009653727,10.643324402269986,10.091346146100589,10.953344240460885,9.895602401385636,14.176384846477232,11.092263811249941,9.757562932316516,11.028465905766282,12.960543411580316,12.023141263168588,10.728385284456694,11.33043099178672,12.05440995470622,11.748037944145207,10.618355627565538,13.379258703990251,13.46726468508406,12.058466434054695,11.538316386929397,10.788898109890242,11.806949158022285,12.39766976730945,15.763095197621054,14.993350010950431,14.143545127504714,13.517919552576766,11.853458258578755,12.818124838334297,12.34321822588146,10.23705681106729,9.423955042889673,11.374954407636526,11.278325983958128,11.211903364850016,11.04136659445136,16.290116789885943,13.962216918049927,13.501710679414032,12.348770906711199,11.985740984536228,12.836052437036226,10.767314756018383,10.143545489297171,10.371995163895317,13.263546107171768,13.040948240159882,12.712013721082725,11.15155214981606,12.309605008504443,10.21895076416583,13.61592258768324,14.799160634854395,10.329202917812827,11.241650789869356,13.20691272484238,11.653292089152623,10.623294747179639,11.551388703901912,13.793824967293897,11.930807218526535,12.100801952637712,9.901282831556996,13.127428248668615,16.098521861066324,10.636890436427773,12.208867209472912,13.648782638846805,17.617164362086278,11.689027813219655,10.360856746151343,12.216713922408358,12.4149915204378,11.591305576870775,10.318705512774741,11.456761158603646,9.603217599371893,13.312126304568633,10.647670662848338,11.728312967129746,12.898764682436557,11.864705625083452,11.598446265916104,10.998270397296565,13.031869669564594,9.851510582110608,12.161944315643384,9.867584432486124,13.438925278126359,13.052895139646555,11.25219500005792,10.876188455791501,11.146353246215519,12.268240343695748,12.605223303134258,11.617556714698484,9.348086031714265,13.525399186281023,9.658506744667521,9.362132413832285,10.13396139337125,14.821928389615016,12.138919725204117,11.58764345920539,13.325276546589308,12.019997227086728,13.162874593490264,11.63772150382274,12.8557503349505,13.52872440490896,12.655463731454711,9.729088125783806,10.255957153221313,13.844337801484409,9.467842304965439,10.071938136151017,10.996860384495694,12.57341214881289,14.412519533222827,11.846843960745463,18.67993449577431,12.373774248627136,12.640551387120379,11.990454873484573,10.771050864106229,10.285132925419944,12.553423986031461,10.023383877742841,12.86131758529609,10.631271352031499,11.590694247766416,11.471627068618407,11.490768528978153,12.82080357577394,10.582944482476995,13.887466300159941,12.712993153914073,11.351899783179986,9.921073920118188,9.821374778308268,10.443474285649346,10.086427735136878,8.546639296890541,10.193310834306045,10.465620098719942,9.63646143258475,12.770510868839036,10.732740255388139,9.945626217006291,10.777521244550005,9.25455590004991,10.14202265620456,10.581402957015639,10.645383299082633,9.89435296520108,9.598104957502528,11.589951744268628,9.997523303504648,9.967864101634726,9.672510507141826,12.19007390747825,9.516268968963354,10.516795181498496,11.246693858677016,11.30694463860327,12.887090816053812,15.614986535027217,15.870742936335121,10.415827175418475,15.37274302441243,10.04065889414053,10.670327136662959,10.191270863737126,12.50566462208292,11.793225162746465,10.056774101327939,12.047489278315108,10.479435907272979,9.593703123108886,10.26918855359712,11.722312793452582,11.482818929285376,16.776531824379955,8.96627369340615,11.602723699410571,11.061640118103261,14.859108337317293,11.769555277423052,11.372799712200433,10.169951559484153,11.753775079073051,8.215259580160179,10.973615839214835,12.560191296760044,9.827292651017167,10.539027106950144,11.525854660179263,10.812420050600693,15.622180159285199,10.922722018704771,10.963034200171569,11.057143773446992,11.104811025409441,12.41217979873561,12.914106350018983,9.51705417549785,9.47500666479296,9.852213238634917,9.720503795547641,9.081102881209885,11.07556522466358,13.77457151368353,9.834240754748016,11.086395161227848,9.261121330623402,10.470136394626039,10.606487871671689,10.044629461918568,8.471906385414924,9.112559052428452,12.22859735621129,7.9450563333167254,9.537410020607773,10.66329196035001,9.599568024730111,9.716415388877284,9.596692223895465,9.274949536179285,9.793415713160963,9.425108670753456,14.597207246757666,10.029869262995343,10.866915504108754,10.3950505876543,12.492908854748016,9.344903597161084,11.875719875651583,13.9026251131023,12.072368779836902,11.18544704105556,10.212527632203532,11.018686450897862,10.931305634324527,23.247536889102136,10.040310164826163,12.125668122275659,14.280900699780766,9.354011386209796,10.47424590130088,12.681468121355548,11.063506080395033,12.037163184127643,11.353429158771128,10.811624437548593,10.75911550587176,10.949367584973501,9.721661634086562,12.216438462366702,11.459443828906736,9.178929372573313,10.455895271157994,12.880822856093863,11.64742931523044,11.110396278313196,8.742419447725679,9.89322920944598,13.709518682503925,13.35763350564543,11.92280370804145,11.974040805339605,10.679930327121248,10.471555709243026,11.963765598529843,9.750002642726848,9.604106315693311,9.28081165014074,9.865138207523012,10.205985712980484,12.540324048330447,10.362575014662896,12.906225355192126,11.86853763387871,10.218028842567287,10.17691969328726,13.246148572416624,9.82787284115349,8.836646013977097,13.074544834234725,11.027550131798868,12.643742978935395,9.173886555567131,11.60735812178298,11.911507521329707,9.831243677359002,10.348120667901162,8.561524854902828,9.983590174586102,10.989226887435903,10.16721849947884,10.902769225872953,10.80956071548065,9.33754078989781,13.298944689008529,9.152926969264254,11.285670804112769,11.395057756441224,10.891388035970484,11.906744302528706,11.07070770676767,11.011830355753172,9.633323721109011,12.531244614555577,10.371853028075284,14.52627659463303,11.516588221983199,11.026736603215848,11.488956357407448,9.991065442594067,8.325157152589005,8.54707886744255,10.529331972772434,9.365876170032132,9.46691569680368,11.385962421650357,9.170384693586623,9.46875982822292,9.204130136270306,9.438769737423096,11.648523041599189,8.636908260649916,9.12994785303068,9.487988966784775,12.967599068091523,10.744829446405031,12.76086725862514,10.665013304581866,10.828546735687114,10.018354870532756,10.565653617117125,8.853437296192148,12.53811346604392,10.885662677814125,8.890710534163125,8.737297032648494,7.919108103078744,11.777094970918846,13.202844433390576,10.549505468861286,12.341616502057905,10.677138972217687,11.617562056382726,11.683871644514742,10.87791037516505,8.849832779652534,8.986279503072614,9.277847749390304,11.455695000251112,11.616137677967282,10.010497286233004,9.68676411432611,11.350080244524273,11.669307151870406,11.997807258611601,12.06829106314947,10.978465151950944,9.287174648029398,10.604643642370188,12.132075633442478,10.295826359007224,8.373975686629548],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"mode\":\"lines+markers\",\"name\":\"Variance\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923],\"y\":[0.1153564453125,0.1156005859375,0.11541748046875,0.1158447265625,0.11529541015625,0.115234375,0.11553955078125,0.11602783203125,0.11553955078125,0.115478515625,0.11566162109375,0.134765625,0.1339111328125,0.1358642578125,0.1343994140625,0.1357421875,0.159423828125,0.2054443359375,0.2239990234375,0.314697265625,0.29296875,0.48388671875,0.38720703125,0.432373046875,0.5029296875,0.5302734375,0.58984375,0.8515625,0.77978515625,0.8857421875,0.90185546875,0.98486328125,1.015625,1.0576171875,1.146484375,1.216796875,1.33203125,1.3447265625,1.4375,1.439453125,1.482421875,1.5263671875,1.5576171875,1.6044921875,1.541015625,1.5732421875,1.609375,1.5947265625,1.65234375,1.6181640625,1.62890625,1.609375,1.689453125,1.6572265625,1.6884765625,1.673828125,1.666015625,1.6357421875,1.6533203125,1.6640625,1.6162109375,1.62890625,1.630859375,1.6064453125,1.6201171875,1.583984375,1.6103515625,1.5859375,1.583984375,1.5791015625,1.58203125,1.58984375,1.623046875,1.6259765625,1.5986328125,1.5986328125,1.611328125,1.609375,1.599609375,1.5439453125,1.5576171875,1.5595703125,1.52734375,1.546875,1.54296875,1.517578125,1.5458984375,1.4912109375,1.513671875,1.494140625,1.466796875,1.4833984375,1.4951171875,1.4794921875,1.4697265625,1.4697265625,1.4736328125,1.46484375,1.4560546875,1.4599609375,1.435546875,1.4326171875,1.435546875,1.451171875,1.42578125,1.408203125,1.396484375,1.3935546875,1.4052734375,1.3837890625,1.3916015625,1.3955078125,1.3828125,1.3798828125,1.375,1.3623046875,1.3662109375,1.3642578125,1.3349609375,1.33203125,1.3115234375,1.3330078125,1.3359375,1.333984375,1.32421875,1.3193359375,1.3203125,1.2880859375,1.2919921875,1.2841796875,1.296875,1.3173828125,1.306640625,1.28125,1.2958984375,1.29296875,1.2880859375,1.2841796875,1.28125,1.283203125,1.2607421875,1.279296875,1.2822265625,1.26953125,1.2734375,1.2607421875,1.26171875,1.25390625,1.2373046875,1.244140625,1.2578125,1.2470703125,1.2255859375,1.2255859375,1.2412109375,1.2314453125,1.21875,1.2138671875,1.2060546875,1.1953125,1.20703125,1.19140625,1.208984375,1.1982421875,1.1748046875,1.1982421875,1.1748046875,1.173828125,1.1650390625,1.1689453125,1.1640625,1.16796875,1.1689453125,1.1533203125,1.1640625,1.1474609375,1.1494140625,1.150390625,1.1416015625,1.1474609375,1.1298828125,1.138671875,1.1396484375,1.1435546875,1.1357421875,1.1298828125,1.1181640625,1.1181640625,1.1328125,1.1103515625,1.1181640625,1.142578125,1.1171875,1.1201171875,1.1181640625,1.125,1.1083984375,1.111328125,1.09765625,1.1171875,1.08984375,1.1015625,1.099609375,1.0986328125,1.0810546875,1.091796875,1.0625,1.06640625,1.076171875,1.0732421875,1.05078125,1.0537109375,1.05859375,1.0517578125,1.046875,1.0673828125,1.04296875,1.0478515625,1.0615234375,1.025390625,1.029296875,1.0244140625,1.03515625,1.0390625,1.0341796875,1.052734375,1.037109375,1.041015625,1.0244140625,1.0380859375,1.0078125,1.021484375,1.021484375,1.0185546875,1.0166015625,1.0244140625,1.0126953125,1.005859375,1.0146484375,1.021484375,1.005859375,1.013671875,1.0048828125,0.99755859375,1.0,0.9853515625,0.9921875,0.9990234375,0.99658203125,0.9931640625,0.9853515625,0.97607421875,0.97900390625,0.99072265625,1.0048828125,0.9775390625,0.98046875,0.9833984375,0.98291015625,0.97998046875,0.9765625,0.9833984375,0.98095703125,0.9716796875,0.97119140625,0.970703125,0.974609375,0.97509765625,0.970703125,0.97265625,0.95654296875,0.9619140625,0.955078125,0.94189453125,0.939453125,0.93603515625,0.91455078125,0.93701171875,0.92578125,0.9384765625,0.93505859375,0.9404296875,0.9345703125,0.947265625,0.94384765625,0.9208984375,0.923828125,0.935546875,0.91552734375,0.91748046875,0.91796875,0.916015625,0.92626953125,0.923828125,0.93359375,0.912109375,0.92724609375,0.92333984375,0.90771484375,0.91064453125,0.896484375,0.90478515625,0.9033203125,0.90380859375,0.90087890625,0.8974609375,0.89599609375,0.90087890625,0.89892578125,0.89404296875,0.89208984375,0.8916015625,0.892578125,0.89404296875,0.8896484375,0.89208984375,0.8876953125,0.8857421875,0.87890625,0.87255859375,0.86328125,0.8779296875,0.8828125,0.8642578125,0.8720703125,0.875,0.87646484375,0.86962890625,0.8935546875,0.86669921875,0.89111328125,0.87451171875,0.869140625,0.87451171875,0.87646484375,0.86767578125,0.87451171875,0.86962890625,0.857421875,0.8583984375,0.87109375,0.859375,0.861328125,0.865234375,0.857421875,0.8798828125,0.8525390625,0.85107421875,0.8662109375,0.8515625,0.8623046875,0.8603515625,0.853515625,0.86474609375,0.8701171875,0.8564453125,0.85546875,0.86376953125,0.8505859375,0.85205078125,0.859375,0.85693359375,0.8466796875,0.8486328125,0.84765625,0.85107421875,0.833984375,0.83544921875,0.84033203125,0.81494140625,0.82666015625,0.8291015625,0.8193359375,0.841796875,0.8193359375,0.8369140625,0.84375,0.83837890625,0.8359375,0.8212890625,0.82763671875,0.83447265625,0.828125,0.8359375,0.8330078125,0.81982421875,0.828125,0.81982421875,0.81103515625,0.82177734375,0.814453125,0.83740234375,0.82958984375,0.826171875,0.8173828125,0.83544921875,0.830078125,0.8076171875,0.81298828125,0.80419921875,0.8173828125,0.80810546875,0.81494140625,0.80712890625,0.806640625,0.80517578125,0.82080078125,0.81103515625,0.8095703125,0.81591796875,0.81689453125,0.8125,0.818359375,0.8203125,0.8115234375,0.822265625,0.8017578125,0.8154296875,0.80908203125,0.81591796875,0.80078125,0.8046875,0.80517578125,0.81591796875,0.8134765625,0.8076171875,0.82177734375,0.81689453125,0.79638671875,0.80126953125,0.8115234375,0.79296875,0.79248046875,0.81103515625,0.80517578125,0.80322265625,0.79931640625,0.80712890625,0.80322265625,0.79052734375,0.80908203125,0.79541015625,0.798828125,0.8037109375,0.7890625,0.7998046875,0.8056640625,0.8017578125,0.78076171875,0.7900390625,0.77880859375,0.79052734375,0.78173828125,0.77783203125,0.77099609375,0.779296875,0.78759765625,0.79345703125,0.80322265625,0.8017578125,0.7900390625,0.8017578125,0.78173828125,0.79833984375,0.78466796875,0.783203125,0.7919921875,0.783203125,0.783203125,0.798828125,0.7939453125,0.7861328125,0.787109375,0.779296875,0.80029296875,0.78759765625,0.7890625,0.7861328125,0.79736328125,0.791015625,0.78271484375,0.7841796875,0.7900390625,0.78662109375,0.779296875,0.77978515625,0.7841796875,0.7802734375,0.78271484375,0.78369140625,0.77734375,0.78466796875,0.79443359375,0.7822265625,0.79638671875,0.78466796875,0.775390625,0.76806640625,0.77490234375,0.7861328125,0.7841796875,0.77880859375,0.7685546875,0.7822265625,0.77685546875,0.77197265625,0.783203125,0.775390625,0.779296875,0.78173828125,0.77978515625,0.76171875,0.76220703125,0.763671875,0.77880859375,0.7646484375,0.78173828125,0.763671875,0.77001953125,0.77392578125,0.76953125,0.771484375,0.7607421875,0.78173828125,0.7685546875,0.767578125,0.76318359375,0.763671875,0.7802734375,0.75341796875,0.76708984375,0.759765625,0.7685546875,0.76904296875,0.763671875,0.7587890625,0.76953125,0.775390625,0.7646484375,0.7607421875,0.76611328125,0.765625,0.751953125,0.7529296875,0.76416015625,0.75634765625,0.767578125,0.765625,0.76171875,0.765625,0.7451171875,0.75634765625,0.75244140625,0.76123046875,0.75439453125,0.75390625,0.7451171875,0.76318359375,0.7685546875,0.751953125,0.75390625,0.76953125,0.75244140625,0.74462890625,0.748046875,0.7431640625,0.7587890625,0.7451171875,0.7529296875,0.759765625,0.75048828125,0.73779296875,0.7607421875,0.74951171875,0.75341796875,0.73681640625,0.7373046875,0.7412109375,0.74658203125,0.75244140625,0.74365234375,0.75537109375,0.74658203125,0.7568359375,0.7607421875,0.75439453125,0.74755859375,0.755859375,0.763671875,0.7451171875,0.736328125,0.74072265625,0.74853515625,0.74462890625,0.74560546875,0.74951171875,0.74609375,0.74755859375,0.74609375,0.7509765625,0.75341796875,0.751953125,0.7548828125,0.76318359375,0.75341796875,0.740234375,0.759765625,0.7568359375,0.75048828125,0.7578125,0.75732421875,0.755859375,0.74609375,0.75537109375,0.76171875,0.7529296875,0.74951171875,0.7529296875,0.7431640625,0.76416015625,0.76318359375,0.74365234375,0.740234375,0.7548828125,0.75244140625,0.7451171875,0.732421875,0.75,0.75341796875,0.7451171875,0.7470703125,0.7490234375,0.75341796875,0.7451171875,0.75,0.75537109375,0.75537109375,0.75341796875,0.75830078125,0.7509765625,0.75341796875,0.74267578125,0.736328125,0.73779296875,0.73974609375,0.74658203125,0.76318359375,0.76123046875,0.7412109375,0.74951171875,0.7509765625,0.75732421875,0.748046875,0.751953125,0.75537109375,0.7646484375,0.740234375,0.74267578125,0.75390625,0.7470703125,0.7568359375,0.74560546875,0.744140625,0.7451171875,0.7685546875,0.75927734375,0.76708984375,0.75390625,0.76318359375,0.76513671875,0.76953125,0.7666015625,0.74853515625,0.77783203125,0.74365234375,0.76123046875,0.7763671875,0.7568359375,0.76025390625,0.751953125,0.7783203125,0.767578125,0.7490234375,0.76904296875,0.759765625,0.75244140625,0.75927734375,0.76611328125,0.76611328125,0.76220703125,0.7607421875,0.74951171875,0.7587890625,0.76953125,0.767578125,0.77197265625,0.76708984375,0.767578125,0.7568359375,0.779296875,0.7607421875,0.77197265625,0.77490234375,0.771484375,0.77001953125,0.76904296875,0.73876953125,0.76513671875,0.75732421875,0.76513671875,0.7705078125,0.74609375,0.77734375,0.74853515625,0.76904296875,0.7548828125,0.74755859375,0.7470703125,0.76171875,0.74755859375,0.73681640625,0.75732421875,0.75537109375,0.767578125,0.7646484375,0.759765625,0.75537109375,0.759765625,0.75927734375,0.759765625,0.76123046875,0.75830078125,0.7529296875,0.755859375,0.75,0.76708984375,0.7529296875,0.755859375,0.74951171875,0.76953125,0.76123046875,0.77392578125,0.76513671875,0.74365234375,0.7509765625,0.76904296875,0.75439453125,0.767578125,0.75048828125,0.75830078125,0.75439453125,0.75048828125,0.7509765625,0.75,0.755859375,0.759765625,0.765625,0.7685546875,0.76220703125,0.76611328125,0.77197265625,0.763671875,0.765625,0.75146484375,0.7607421875,0.75634765625,0.75830078125,0.759765625,0.7724609375,0.77490234375,0.76220703125,0.75244140625,0.75927734375,0.75634765625,0.7626953125,0.7626953125,0.75439453125,0.765625,0.74951171875,0.7509765625,0.7578125,0.751953125,0.751953125,0.7509765625,0.7412109375,0.73974609375,0.75634765625,0.75244140625,0.7529296875,0.748046875,0.75244140625,0.74755859375,0.7431640625,0.748046875,0.75537109375,0.74755859375,0.75244140625,0.7685546875,0.748046875,0.74560546875,0.75,0.7548828125,0.75634765625,0.7451171875,0.755859375,0.7607421875,0.759765625,0.75146484375,0.7470703125,0.755859375,0.74609375,0.7607421875,0.7509765625,0.75634765625,0.76220703125,0.751953125,0.75390625,0.75341796875,0.7490234375,0.75439453125,0.744140625,0.74755859375,0.7578125,0.755859375,0.74853515625,0.76708984375,0.755859375,0.7587890625,0.7421875,0.7431640625,0.75732421875,0.7568359375,0.73583984375,0.75927734375,0.740234375,0.75341796875,0.75146484375,0.7626953125,0.74609375,0.74853515625,0.74658203125,0.74365234375,0.736328125,0.7392578125,0.7421875,0.7431640625,0.73291015625,0.75146484375,0.75244140625,0.76025390625,0.740234375,0.74462890625,0.7578125,0.75439453125,0.74609375,0.7392578125,0.74267578125,0.7490234375,0.759765625,0.74853515625,0.7578125,0.73974609375,0.74560546875,0.74755859375,0.74169921875,0.74609375,0.7548828125,0.75341796875,0.7353515625,0.73974609375,0.740234375,0.74560546875,0.73486328125,0.74365234375,0.75048828125,0.74365234375,0.7470703125,0.75634765625,0.7509765625,0.73681640625,0.75341796875,0.7470703125,0.75048828125,0.7490234375,0.74853515625,0.75,0.732421875,0.74169921875,0.74072265625,0.74072265625,0.73828125,0.73583984375,0.75634765625,0.73681640625,0.74365234375,0.75732421875,0.73583984375,0.75927734375,0.7490234375,0.75439453125,0.74658203125,0.7392578125,0.76611328125,0.7412109375,0.75341796875,0.7275390625,0.72998046875,0.75390625,0.7333984375,0.7431640625,0.75927734375,0.74072265625,0.74365234375,0.74267578125,0.73876953125,0.7412109375,0.75439453125,0.73291015625,0.74072265625,0.7431640625,0.74072265625,0.74169921875,0.74951171875,0.72607421875],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"mode\":\"lines+markers\",\"name\":\"Learning Rate\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923],\"y\":[0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"mode\":\"lines+markers\",\"name\":\"Dev STS Pearson Cosine\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923],\"y\":[0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863],\"type\":\"scatter\",\"xaxis\":\"x5\",\"yaxis\":\"y5\"},{\"mode\":\"lines+markers\",\"name\":\"Dev STS Spearman Cosine\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923],\"y\":[0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645],\"type\":\"scatter\",\"xaxis\":\"x5\",\"yaxis\":\"y5\"},{\"mode\":\"lines+markers\",\"name\":\"Test STS Pearson Cosine\",\"x\":[0,154,308,462,616,770,924],\"y\":[0.5676088669847356,0.644605688190804,0.5473917103098469,0.5076255584049224,0.466274195074839,0.4665545855427623,0.4266041901636244],\"type\":\"scatter\",\"xaxis\":\"x6\",\"yaxis\":\"y6\"},{\"mode\":\"lines+markers\",\"name\":\"Test STS Spearman Cosine\",\"x\":[0,154,308,462,616,770,924],\"y\":[0.5716446774391377,0.6387236448623214,0.5440794571617531,0.5050998104419027,0.46443531813764677,0.4754335320182503,0.4330498331815567],\"type\":\"scatter\",\"xaxis\":\"x6\",\"yaxis\":\"y6\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"autorange\":true},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.7777777777777778,1.0],\"autorange\":true},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"autorange\":true},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.7777777777777778,1.0],\"autorange\":true},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,0.45],\"autorange\":true},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.3888888888888889,0.6111111111111112],\"autorange\":true},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.55,1.0],\"autorange\":true},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.3888888888888889,0.6111111111111112],\"autorange\":true},\"xaxis5\":{\"anchor\":\"y5\",\"domain\":[0.0,0.45],\"autorange\":true},\"yaxis5\":{\"anchor\":\"x5\",\"domain\":[0.0,0.22222222222222224],\"autorange\":true},\"xaxis6\":{\"anchor\":\"y6\",\"domain\":[0.55,1.0],\"autorange\":true},\"yaxis6\":{\"anchor\":\"x6\",\"domain\":[0.0,0.22222222222222224],\"autorange\":true},\"annotations\":[{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=distilbert-base-uncased, batch_size=128, projection_depth=2, projection_size=8192, epochs=1, warmup_proportion=0.0, max_seq_length=75,\\u003cbr\\u003eaug_p=0.20943901101203366, learning_rate=0.0006737552047782743, model_save_path=\\u002fcontent\\u002fdrive\\u002fMyDrive\\u002fviolet_checkpoints_2\\u002ftrain_stsb_bt-\\u003cbr\\u003edistilbert-2025-03-30_18-23-26_trial19, num_workers=10, weight_decay=0.1333928565968183, lambda_bt=0.0017107385254106905, lambda_mixup=0.6059879433460729,\\u003cbr\\u003euse_amp=True, patience=770, Augmenters: Synonym_Aug[substitute:0.20943901101203366], RandomWord_Aug[swap:0.20943901101203366],\\u003cbr\\u003eRandomWord_Aug[delete:0.20943901101203366]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=distilbert-base-uncased, batch_size=128, projection_depth=2, projection_size=8192, epochs=1, warmup_proportion=0.0, max_seq_length=75,\\u003cbr\\u003eaug_p=0.20943901101203366, learning_rate=0.0006737552047782743, model_save_path=\\u002fcontent\\u002fdrive\\u002fMyDrive\\u002fviolet_checkpoints_2\\u002ftrain_stsb_bt-\\u003cbr\\u003edistilbert-2025-03-30_18-23-26_trial19, num_workers=10, weight_decay=0.1333928565968183, lambda_bt=0.0017107385254106905, lambda_mixup=0.6059879433460729,\\u003cbr\\u003euse_amp=True, patience=770, Augmenters: Synonym_Aug[substitute:0.20943901101203366], RandomWord_Aug[swap:0.20943901101203366],\\u003cbr\\u003eRandomWord_Aug[delete:0.20943901101203366]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=distilbert-base-uncased, batch_size=128, projection_depth=2, projection_size=8192, epochs=1, warmup_proportion=0.0, max_seq_length=75,\\u003cbr\\u003eaug_p=0.20943901101203366, learning_rate=0.0006737552047782743, model_save_path=\\u002fcontent\\u002fdrive\\u002fMyDrive\\u002fviolet_checkpoints_2\\u002ftrain_stsb_bt-\\u003cbr\\u003edistilbert-2025-03-30_18-23-26_trial19, num_workers=10, weight_decay=0.1333928565968183, lambda_bt=0.0017107385254106905, lambda_mixup=0.6059879433460729,\\u003cbr\\u003euse_amp=True, patience=770, Augmenters: Synonym_Aug[substitute:0.20943901101203366], RandomWord_Aug[swap:0.20943901101203366],\\u003cbr\\u003eRandomWord_Aug[delete:0.20943901101203366]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=distilbert-base-uncased, batch_size=128, projection_depth=2, projection_size=8192, epochs=1, warmup_proportion=0.0, max_seq_length=75,\\u003cbr\\u003eaug_p=0.20943901101203366, learning_rate=0.0006737552047782743, model_save_path=\\u002fcontent\\u002fdrive\\u002fMyDrive\\u002fviolet_checkpoints_2\\u002ftrain_stsb_bt-\\u003cbr\\u003edistilbert-2025-03-30_18-23-26_trial19, num_workers=10, weight_decay=0.1333928565968183, lambda_bt=0.0017107385254106905, lambda_mixup=0.6059879433460729,\\u003cbr\\u003euse_amp=True, patience=770, Augmenters: Synonym_Aug[substitute:0.20943901101203366], RandomWord_Aug[swap:0.20943901101203366],\\u003cbr\\u003eRandomWord_Aug[delete:0.20943901101203366]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=distilbert-base-uncased, batch_size=128, projection_depth=2, projection_size=8192, epochs=1, warmup_proportion=0.0, max_seq_length=75,\\u003cbr\\u003eaug_p=0.20943901101203366, learning_rate=0.0006737552047782743, model_save_path=\\u002fcontent\\u002fdrive\\u002fMyDrive\\u002fviolet_checkpoints_2\\u002ftrain_stsb_bt-\\u003cbr\\u003edistilbert-2025-03-30_18-23-26_trial19, num_workers=10, weight_decay=0.1333928565968183, lambda_bt=0.0017107385254106905, lambda_mixup=0.6059879433460729,\\u003cbr\\u003euse_amp=True, patience=770, Augmenters: Synonym_Aug[substitute:0.20943901101203366], RandomWord_Aug[swap:0.20943901101203366],\\u003cbr\\u003eRandomWord_Aug[delete:0.20943901101203366]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=distilbert-base-uncased, batch_size=128, projection_depth=2, projection_size=8192, epochs=1, warmup_proportion=0.0, max_seq_length=75,\\u003cbr\\u003eaug_p=0.20943901101203366, learning_rate=0.0006737552047782743, model_save_path=\\u002fcontent\\u002fdrive\\u002fMyDrive\\u002fviolet_checkpoints_2\\u002ftrain_stsb_bt-\\u003cbr\\u003edistilbert-2025-03-30_18-23-26_trial19, num_workers=10, weight_decay=0.1333928565968183, lambda_bt=0.0017107385254106905, lambda_mixup=0.6059879433460729,\\u003cbr\\u003euse_amp=True, patience=770, Augmenters: Synonym_Aug[substitute:0.20943901101203366], RandomWord_Aug[swap:0.20943901101203366],\\u003cbr\\u003eRandomWord_Aug[delete:0.20943901101203366]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"right\",\"font\":{\"color\":\"blue\",\"size\":12},\"showarrow\":false,\"text\":\"Best Spearman (Test): 0.6387\\u003cbr\\u003eBest Pearson (Test): 0.6446\\u003cbr\\u003eBest Spearman (Val): 0.7214\\u003cbr\\u003eBest Pearson (Val): 0.7112\",\"x\":1.0,\"xanchor\":\"right\",\"xref\":\"paper\",\"y\":0.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Training Metrics\"},\"margin\":{\"l\":50,\"r\":50,\"t\":100,\"b\":150},\"width\":1200,\"height\":800,\"showlegend\":true,\"sliders\":[{\"active\":0,\"currentvalue\":{\"prefix\":\"Epoch: \"},\"pad\":{\"t\":50},\"steps\":[{\"args\":[[\"0\"],{\"frame\":{\"duration\":0,\"redraw\":true},\"mode\":\"immediate\",\"transition\":{\"duration\":0}}],\"label\":\"0\",\"method\":\"animate\"}]}]},                        {\"toImageButtonOptions\": {\"filename\": \"/content/drive/MyDrive/violet_checkpoints_2/train_stsb_bt-distilbert-2025-03-30_18-23-26_trial19\", \"format\": \"png\", \"width\": 1200, \"height\": 800, \"scale\": 1}, \"responsive\": true}                    ).then(function(){\n",
              "                            Plotly.addFrames('45d58fbd-bf86-4d1a-8262-402bc1021648', [{\"data\":[{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923],\"y\":[2184.04931640625,1983.4031982421875,2353.45263671875,2254.07666015625,2084.581787109375,2133.153564453125,2503.723388671875,2078.739013671875,2005.5802001953125,2266.14697265625,2104.712646484375,3401.214111328125,3609.413330078125,2888.32763671875,3574.197021484375,3122.470703125,2397.123046875,4627.8798828125,3877.140625,8705.0146484375,5476.33056640625,6641.5224609375,5605.20751953125,4989.2197265625,6068.1494140625,5906.90625,7050.85107421875,5772.2763671875,5937.7392578125,5983.46875,4966.58642578125,5047.068359375,5064.64111328125,4171.52783203125,4845.81640625,4704.21533203125,4271.83642578125,4129.44775390625,4225.4052734375,3870.098388671875,4036.8369140625,4191.96337890625,3985.055419921875,3870.5546875,3672.560791015625,3871.08447265625,3605.823486328125,3602.1943359375,3504.866455078125,3269.451171875,3242.383056640625,3500.841552734375,3260.43017578125,3194.70166015625,3089.480224609375,3261.708251953125,2973.025146484375,3248.0244140625,3132.086669921875,2972.9873046875,3015.1328125,3062.911376953125,2897.294921875,2905.134521484375,3083.473388671875,2883.169921875,3007.33447265625,2855.205322265625,2749.44482421875,2665.544189453125,2795.4677734375,2898.389892578125,2539.48095703125,2657.467041015625,2672.58447265625,2807.129638671875,2626.128662109375,2635.3115234375,2603.81884765625,2538.813720703125,2629.49755859375,2585.46875,2729.25390625,2889.599365234375,2571.7509765625,2546.88232421875,2639.643310546875,2677.061279296875,2613.91748046875,2542.6005859375,2387.650146484375,2551.9814453125,2625.17431640625,2288.15234375,2543.691650390625,2299.156494140625,2452.7109375,2413.29150390625,2422.348876953125,2410.810302734375,2441.215087890625,2360.94921875,2312.332763671875,2253.322265625,2313.64013671875,2221.976318359375,2369.6708984375,2344.638671875,2372.512451171875,2344.41748046875,2391.457275390625,2298.00732421875,2366.61962890625,2425.797607421875,2383.840087890625,2473.2294921875,2344.06982421875,2277.3046875,2162.002685546875,2240.65771484375,2090.579345703125,2160.350830078125,2187.04638671875,2222.5771484375,2342.02783203125,2140.847900390625,2374.008544921875,2212.7919921875,2404.049560546875,2151.045654296875,2162.59716796875,2167.560546875,2292.27197265625,2179.55859375,2102.42041015625,2118.402587890625,2123.3642578125,2138.322021484375,2250.0400390625,2078.17822265625,2177.81396484375,2006.2635498046875,2052.32666015625,2146.7099609375,2148.5205078125,2104.013916015625,2160.99267578125,1984.7083740234375,2044.4686279296875,2115.667724609375,2120.58544921875,1979.99560546875,2006.0946044921875,1957.8602294921875,2156.751708984375,2191.641845703125,1942.1488037109375,2018.19677734375,1958.3203125,2024.4345703125,2068.90380859375,2025.610595703125,2019.18603515625,1931.1683349609375,1973.284423828125,1996.4876708984375,1923.44384765625,2019.7890625,1999.924072265625,1989.446533203125,2035.8707275390625,1963.87255859375,2075.565185546875,1843.8394775390625,2055.10205078125,2073.451171875,2063.59228515625,1982.7244873046875,2066.51806640625,2153.405517578125,1933.9688720703125,1999.5731201171875,2122.982177734375,2003.0164794921875,2039.780517578125,1887.970458984375,1996.6365966796875,1970.2550048828125,2181.506591796875,1941.0601806640625,2212.856201171875,1863.734619140625,1991.4134521484375,1986.8101806640625,2062.920166015625,1849.864501953125,1870.833740234375,1964.9537353515625,2091.39453125,2100.997802734375,1884.25830078125,2044.0526123046875,1936.533935546875,1932.615478515625,1844.2413330078125,1928.952880859375,2046.32666015625,1942.5894775390625,1924.6131591796875,1875.9683837890625,1823.730712890625,1824.0518798828125,1844.1058349609375,1881.4888916015625,2040.17333984375,1945.9327392578125,1858.8936767578125,1933.2437744140625,1872.61962890625,2024.79443359375,2040.5474853515625,1942.0767822265625,1888.067138671875,1758.4969482421875,1922.0318603515625,1880.570556640625,1851.70654296875,1946.4593505859375,2136.203369140625,2063.121337890625,1772.0777587890625,1884.97314453125,2060.81005859375,1788.6715087890625,1864.437744140625,1810.082763671875,2000.97802734375,1800.813232421875,1968.85546875,1891.5966796875,1899.931884765625,1815.623779296875,1957.54736328125,1907.1517333984375,1915.7181396484375,1935.09912109375,1857.7066650390625,1961.375,1759.22998046875,2015.0150146484375,1999.161865234375,1779.4990234375,1885.5833740234375,1958.8857421875,1964.07373046875,1864.060546875,1960.498291015625,1860.5762939453125,1770.842529296875,1780.5047607421875,1808.705078125,1843.1641845703125,1863.455322265625,1819.564697265625,2007.3016357421875,1856.6392822265625,2040.84521484375,1856.37841796875,1962.5206298828125,1872.885498046875,1787.365966796875,1705.6119384765625,1715.3443603515625,1812.4407958984375,1827.584228515625,1754.346923828125,1798.8155517578125,1844.5072021484375,1817.0223388671875,1987.3961181640625,1721.669189453125,1792.4080810546875,1689.3326416015625,2030.359375,1935.2386474609375,1718.1163330078125,1818.704833984375,1988.37060546875,1947.5478515625,1657.03125,1698.67724609375,1947.4478759765625,1856.3809814453125,1764.092529296875,1818.61865234375,1690.2720947265625,1782.00244140625,1706.0611572265625,1780.5560302734375,1787.09716796875,1943.7528076171875,2044.2430419921875,1710.953125,1844.69873046875,1792.9075927734375,1783.7379150390625,1916.963623046875,1847.9573974609375,1701.2054443359375,1629.361572265625,1671.542236328125,1736.3023681640625,1866.7186279296875,1626.2861328125,1724.7769775390625,1680.0926513671875,1691.5908203125,2005.597900390625,1915.6448974609375,1659.0665283203125,1632.9630126953125,1805.438232421875,1977.181640625,1755.7713623046875,1780.0418701171875,1768.3831787109375,1815.577392578125,1796.5587158203125,1630.0545654296875,1870.797119140625,1827.4119873046875,1790.0096435546875,1751.2828369140625,1714.6748046875,1939.12255859375,1737.22216796875,1778.3409423828125,1631.6680908203125,1767.0589599609375,1753.0928955078125,1697.05908203125,1797.8033447265625,1793.31494140625,1762.67919921875,1850.1617431640625,1807.800048828125,1888.177490234375,1908.947509765625,1724.9886474609375,1712.3734130859375,1819.3165283203125,1921.387451171875,1809.8427734375,1802.9951171875,1801.5897216796875,1701.462646484375,1751.3837890625,1786.652587890625,1707.57080078125,1846.1514892578125,1809.2276611328125,1645.0750732421875,1790.6729736328125,1670.666259765625,1610.7281494140625,1675.9735107421875,1812.88720703125,1619.8223876953125,1720.11376953125,1959.7518310546875,1730.88427734375,1625.269287109375,1682.1165771484375,1682.94873046875,1771.28125,1632.952392578125,1668.057373046875,1779.508544921875,1863.0364990234375,1858.46240234375,1646.5313720703125,1895.7626953125,1690.8944091796875,1880.478271484375,1729.5030517578125,1851.7313232421875,1938.54052734375,1734.018310546875,1688.9522705078125,1660.7005615234375,1889.736083984375,1698.5377197265625,1765.724365234375,1713.4193115234375,1774.1422119140625,1766.2008056640625,1649.6806640625,1650.837890625,1837.87548828125,1725.1451416015625,1802.7637939453125,1685.5811767578125,1610.7413330078125,1658.5269775390625,1761.90673828125,1648.1513671875,1650.2696533203125,1941.12158203125,1853.60107421875,1648.056396484375,1648.9647216796875,1691.39599609375,1725.3314208984375,1664.8206787109375,1579.3760986328125,1768.5262451171875,1694.365478515625,1575.0360107421875,1710.62841796875,1749.15673828125,1672.88671875,1641.230224609375,1638.2042236328125,1788.3424072265625,1710.8719482421875,1700.314697265625,1720.8782958984375,1844.096923828125,1932.069580078125,1791.4766845703125,1756.2938232421875,1811.583984375,1622.189453125,1608.297607421875,1633.774658203125,1658.5855712890625,1729.297119140625,1679.865234375,1640.606689453125,1734.8829345703125,1694.4578857421875,1662.9088134765625,1767.036865234375,1682.9365234375,1881.9139404296875,1704.71484375,1604.936767578125,1687.24169921875,1599.4986572265625,1874.391357421875,1749.858642578125,1666.9678955078125,1742.6834716796875,1714.3948974609375,1724.53466796875,1730.8233642578125,1742.1802978515625,1675.146240234375,1687.826904296875,1667.973388671875,1741.0731201171875,1568.669921875,1855.1732177734375,1581.217529296875,1703.8634033203125,1651.5819091796875,1643.476318359375,1795.859130859375,1726.215576171875,1637.117919921875,1665.7601318359375,1595.8302001953125,1526.0782470703125,1759.78515625,1673.4251708984375,1730.4197998046875,1740.5770263671875,1649.74169921875,1673.830810546875,1651.449462890625,1799.7156982421875,1609.931396484375,1677.3193359375,1544.12841796875,1608.07177734375,1652.18896484375,1537.2838134765625,1668.48193359375,1589.5181884765625,1659.6143798828125,1864.2921142578125,1576.16162109375,1643.4678955078125,1851.994384765625,1723.4219970703125,1695.15771484375,1667.596923828125,1791.7008056640625,1739.633544921875,1660.8448486328125,1714.7882080078125,1595.81396484375,1680.565185546875,1735.9949951171875,1658.9197998046875,1781.302001953125,1772.92919921875,1647.8841552734375,1563.7022705078125,1610.2608642578125,1644.480224609375,1573.8887939453125,1818.3568115234375,1825.7406005859375,1544.078857421875,1820.8056640625,1722.438232421875,1574.5010986328125,1748.9564208984375,1600.0245361328125,1611.709716796875,1550.394775390625,1767.21826171875,1681.5467529296875,1644.09033203125,1712.6585693359375,1551.542724609375,1545.43701171875,1712.346923828125,1795.7347412109375,1816.9864501953125,1662.9892578125,1584.65673828125,1612.86865234375,1660.620849609375,1684.9063720703125,1616.299072265625,1633.3564453125,1510.9185791015625,1771.213623046875,1777.3662109375,1530.2935791015625,1749.89794921875,1694.1256103515625,1841.9532470703125,1632.1435546875,1626.1676025390625,1748.9197998046875,1730.97314453125,1665.6622314453125,1708.891357421875,1622.9208984375,1755.0118408203125,1678.03076171875,1799.8338623046875,1648.1695556640625,1588.328857421875,1676.1573486328125,1583.5804443359375,1866.173828125,1540.6219482421875,1513.43359375,1552.37451171875,1611.457275390625,1524.7220458984375,1680.0928955078125,1687.6571044921875,1693.9561767578125,1742.46337890625,1633.2938232421875,1794.4957275390625,1691.961181640625,1612.406494140625,1618.7484130859375,1627.1107177734375,1653.280029296875,1733.948486328125,1811.252197265625,1774.274169921875,1639.3599853515625,1721.7550048828125,1643.200927734375,1780.7841796875,1725.0399169921875,1572.7352294921875,1529.957763671875,1671.322509765625,1528.382080078125,1636.955810546875,1626.020263671875,1804.8883056640625,1700.280029296875,1765.4384765625,1648.707763671875,1664.78125,1770.2423095703125,1632.2452392578125,1627.826416015625,1637.997314453125,1802.9091796875,1712.6427001953125,1612.778564453125,1635.864501953125,1657.6328125,1650.10205078125,1858.840576171875,1805.4468994140625,1597.86328125,1613.146240234375,1629.639892578125,1708.8358154296875,1626.736572265625,1612.7337646484375,1606.506591796875,1576.52294921875,1646.4681396484375,1548.4552001953125,1690.074951171875,1861.4100341796875,1702.587158203125,1605.5899658203125,1795.1888427734375,1680.474365234375,1710.9532470703125,1641.439697265625,1601.8741455078125,1686.92431640625,1567.973388671875,1700.46240234375,1594.6082763671875,1653.652587890625,1686.740966796875,1654.4967041015625,1561.6160888671875,1641.1441650390625,1713.9815673828125,1606.5009765625,1560.056884765625,1798.01171875,1650.976318359375,1738.92919921875,1629.9254150390625,1761.7381591796875,1788.0682373046875,1696.8671875,1650.7532958984375,1597.110595703125,1671.492431640625,1752.4537353515625,1545.0106201171875,1555.508544921875,1723.8768310546875,1634.0345458984375,1588.721923828125,1633.80712890625,1744.7935791015625,1660.8818359375,1630.013916015625,1592.0196533203125,1710.1221923828125,1762.029541015625,1696.931884765625,1709.3885498046875,1709.27197265625,1721.071044921875,1534.6181640625,1530.628173828125,1818.1724853515625,1529.1168212890625,1542.11572265625,1697.0540771484375,1642.0443115234375,1718.1134033203125,1734.6785888671875,1955.7269287109375,1612.0548095703125,1773.982421875,1778.07958984375,1585.538818359375,1599.7490234375,1726.688232421875,1602.88671875,1841.69482421875,1618.055419921875,1711.3094482421875,1793.0,1535.4974365234375,1542.757568359375,1576.6749267578125,1773.0750732421875,1676.835693359375,1643.352294921875,1683.1326904296875,1595.1279296875,1575.72216796875,1624.6778564453125,1517.22265625,1600.0406494140625,1534.5665283203125,1628.044189453125,1764.3499755859375,1651.2735595703125,1512.8416748046875,1721.7869873046875,1596.7655029296875,1615.77880859375,1573.3056640625,1677.6065673828125,1615.9671630859375,1610.1622314453125,1690.5177001953125,1549.9678955078125,1627.4439697265625,1595.456298828125,1659.22802734375,1579.873779296875,1540.76806640625,1646.6473388671875,1659.3668212890625,1695.214111328125,1755.4193115234375,1865.73583984375,1608.6171875,1806.2353515625,1708.7181396484375,1633.813232421875,1529.949462890625,1736.136474609375,1717.490966796875,1537.7904052734375,1655.935546875,1613.2894287109375,1557.215576171875,1549.6387939453125,1669.3841552734375,1652.816162109375,1883.6767578125,1475.8017578125,1776.657470703125,1535.2059326171875,1667.9176025390625,1697.70068359375,1682.02587890625,1573.8223876953125,1747.4732666015625,1588.5858154296875,1683.8055419921875,1684.2923583984375,1593.50634765625,1493.4482421875,1722.9737548828125,1628.6561279296875,1833.414306640625,1651.8326416015625,1620.477783203125,1702.2911376953125,1567.3885498046875,1684.05615234375,1783.7095947265625,1566.0726318359375,1640.2757568359375,1541.9832763671875,1492.8621826171875,1559.2955322265625,1608.587158203125,1796.85009765625,1608.3258056640625,1627.50048828125,1584.294921875,1605.550048828125,1592.84716796875,1572.8023681640625,1503.39697265625,1520.3907470703125,1607.875,1460.3983154296875,1533.839111328125,1545.7391357421875,1557.441650390625,1637.578369140625,1523.206787109375,1542.3277587890625,1657.6065673828125,1580.796630859375,1768.15869140625,1672.0279541015625,1599.3695068359375,1497.514404296875,1693.6761474609375,1615.431884765625,1663.1387939453125,1810.5506591796875,1661.30126953125,1698.3651123046875,1553.73095703125,1677.7958984375,1692.6414794921875,1711.646240234375,1580.0029296875,1688.468505859375,1687.14013671875,1555.5155029296875,1659.143798828125,1693.280029296875,1548.0535888671875,1683.82763671875,1718.7196044921875,1557.8599853515625,1692.7822265625,1647.867431640625,1591.0401611328125,1678.7830810546875,1638.5780029296875,1517.833740234375,1646.35546875,1749.5714111328125,1751.5517578125,1587.82421875,1505.72802734375,1666.2259521484375,1701.48828125,1739.3485107421875,1682.9757080078125,1641.47412109375,1725.18994140625,1625.9686279296875,1683.45703125,1571.336669921875,1541.7601318359375,1609.5576171875,1531.2562255859375,1670.834228515625,1728.03759765625,1607.4444580078125,1687.792236328125,1585.863037109375,1700.5504150390625,1625.1669921875,1675.2762451171875,1664.1427001953125,1573.153076171875,1759.489501953125,1674.961669921875,1689.684814453125,1508.6636962890625,1537.14208984375,1676.8367919921875,1587.60107421875,1575.19970703125,1582.2796630859375,1531.721435546875,1646.64013671875,1558.443359375,1586.7362060546875,1733.926025390625,1552.8822021484375,1663.404541015625,1584.673583984375,1753.7603759765625,1718.53125,1725.0479736328125,1753.980224609375,1646.6717529296875,1624.05224609375,1501.5184326171875,1554.085693359375,1746.2984619140625,1760.218994140625,1623.32177734375,1612.3231201171875,1548.0450439453125,1572.1817626953125,1533.09228515625,1527.5013427734375,1655.37060546875,1602.680908203125,1573.357421875,1651.0838623046875,1643.25439453125,1549.3087158203125,1656.6068115234375,1535.1982421875,1664.527099609375,1539.7725830078125,1655.2696533203125,1655.29443359375,1757.6768798828125,1636.640869140625,1736.9434814453125,1641.236083984375,1634.2689208984375,1654.700927734375,1647.0391845703125,1566.8438720703125,1705.439208984375,1662.5692138671875,1565.0712890625,1657.4320068359375,1591.241943359375,1668.448974609375,1727.3717041015625,1698.8590087890625,1657.8038330078125,1622.94287109375,1637.342529296875,1631.055419921875,1552.53369140625,1573.811767578125,1554.42724609375,1560.052490234375,1528.4698486328125,1736.9091796875,1653.656005859375,1648.004638671875,1758.7308349609375,1688.8389892578125,1618.995361328125,1643.1900634765625,1628.1287841796875,1563.1634521484375,1646.097412109375,1594.6884765625,1568.364501953125,1521.877685546875],\"type\":\"scatter\"},{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923],\"y\":[null,null,null,null,null,null,null,null,null,null,59.35167831746244,null,null,null,null,306.5546045709554,72.61591212651632,489.3573525786012,263.6552233903265,960.9733253900438,371.4001178161901,349.0667201027965,248.92315954116174,372.5783160467275,708.2256565313642,289.33608116331317,841.3395286712109,524.7248677425353,566.6775362666447,502.91771030184685,146.917826149838,126.52501221094883,143.58779166490064,83.86337658662343,140.48900015416578,103.66774653247911,92.88343862080819,110.1174509803741,90.94526483220314,63.15189715055965,69.89080149308793,78.71314986062082,87.28684072537038,72.8254882707523,70.73483012969983,77.70276596833638,55.897425901333406,57.914243547439504,63.1630449607388,51.118454082138825,54.122100748506995,68.4505376090699,50.87320102453999,48.46543306668106,46.49335581348463,45.680150623943966,44.066624521551226,55.06500161567298,62.13202090828731,43.17003397958166,36.40101980710659,42.63483891853614,42.25891426652258,35.81359104591229,42.022015246048824,34.35230011160471,43.83310923772364,41.76800949657681,35.855944540346115,30.896380880414434,31.25549702732283,35.38330757208764,24.819259130022118,30.07271841163393,25.87718452581091,31.179479924012448,32.10428105760644,30.300288143479023,28.911834906051045,25.572476278043297,28.00564926482603,32.97649456381382,27.316423372946975,39.10005248389424,26.99570390878185,23.65735977878964,26.16770763506326,32.44284903515792,25.275287708104702,35.36355160459561,24.030872802454887,26.61063036332636,28.676134584421806,23.936065120915256,27.007760212699438,21.94705075717484,20.827193486846244,22.77617980059793,24.065880778314504,24.00520221866148,22.304742109115956,20.35226195929253,22.421690044064608,23.469635990457697,19.883542671162925,19.174236868430427,24.147884885813557,23.513872712263485,23.127027814569246,25.002410774688315,22.020046339717545,20.019821696611192,22.321824692557716,23.19840866260517,25.1125348483023,22.54649183153815,24.361715892051066,19.48016345774146,18.761138284344046,19.876765991934505,19.487428281445137,19.574450896783926,19.87048564143823,21.204156235047794,22.611338943490324,18.953734916583215,27.79261319475205,21.69333266837987,25.70610454580254,27.00553152851499,19.64044205285247,19.321783541704256,20.495627709076594,18.09206882457383,17.26363174670825,20.829564786656462,18.629393632228673,20.658954746090988,24.04818120601673,16.116160823782828,20.527043385240933,15.780795402681292,18.078103868882188,18.75774330085104,21.4851621193212,17.583824707027198,16.578097428786275,19.564667245467966,16.54373953236175,16.547041088353833,15.8813218966271,17.57534175564776,16.04253746656879,14.965285180897729,24.20751149267438,17.1191152521693,15.082735385832425,17.768574392584004,16.596856315005496,18.85545735111945,23.77196587619382,18.60653070989076,14.9450128180208,16.885048517940454,19.094838786141736,16.193214891599666,17.58020127421265,17.794120134137103,15.315698071392534,16.558055135897977,18.000650080878962,16.36491174280205,19.697236581282567,15.362788399193887,15.27049383011941,18.27363344543651,21.819562447504936,17.47579632500409,20.54505447178388,19.626770474923703,17.754186913108843,16.255399995409444,19.943940443684237,16.582470815647984,17.108474799949356,14.711217286289068,16.7605406416572,16.554187942721775,21.34768396715195,19.154606905090844,19.98349705865133,13.748692052138402,16.10634188483536,17.430118726414168,17.36981702932686,15.660138587098016,13.89342110735946,17.60655633940754,17.46976497600602,18.640534842643728,15.426293512694425,20.029831840634746,14.949202888595126,19.392764496257907,15.12345112757001,17.849152444532397,16.035736590649076,16.53009361700838,17.02941622950352,13.108884936064905,14.682066825349589,14.296728504857935,14.300782675446747,14.37010159806794,16.067616597057025,13.828744421791987,14.10305805033886,15.781987294386186,14.912670399423984,15.443716780188769,17.34265077428923,15.358702664915313,15.072223032793367,16.203686779560268,17.18398849394286,18.159055060554618,15.13525222257343,15.410815864355126,19.384127755255182,19.329871941737874,14.937076370463087,15.699508041244778,18.444550915487074,15.322488182466227,13.019406022565919,14.3333264924755,16.425669238536493,13.718624976650066,18.130816162945887,17.135276490186005,14.461488998862043,14.456279061180172,16.982887934835087,16.76285589438988,15.834617682303328,15.381747926578546,15.85929528058026,13.494879212300148,15.259907943793703,16.480606675174833,19.00057713378309,17.158889374789254,14.094688036389634,17.118756436681135,17.414836320467185,16.537032941446626,16.174189761692592,14.122131635840754,14.446401193353626,19.257438887239605,13.720008872903607,12.20455070129343,14.886760399366423,16.4090322643977,15.933423023682682,14.068018103754032,16.604893144648944,15.474632968350999,15.2730167808193,19.051777591321024,13.86759284409592,12.872435655449543,13.376508985218864,14.953289892278997,14.32463987933782,13.459106160652299,11.705859540205646,16.261443341075925,13.955746537116726,16.23584098878221,14.114566055667556,13.388893314860299,11.286333503182256,16.312554536161723,17.28991285455913,13.16764921957219,13.679088256467894,17.277606742428418,17.17414405192574,11.962449670689757,14.871222399426088,16.471579334755525,16.115659594624415,12.417630735015036,14.933560997853453,14.621313652854122,16.278455375719656,12.147900895301497,12.694329264727529,12.861870448507974,15.893836865567218,18.696017987630523,12.819056561363224,12.844753775593022,14.906080716554586,12.316166205791117,17.11316043836268,15.044024920190619,13.351255571311722,12.593079133443949,11.467427343919525,14.63292632770299,16.80879950880307,11.324897313132302,11.077463280842476,13.81668915266181,13.34396143260978,16.147155659776452,17.51421423111112,12.472168276179977,12.519589686904512,15.59309535285485,18.553001636081767,16.673310169959137,13.75376179031301,12.105140400024132,15.701887909607082,14.442250343350727,12.307553352678944,15.365757118484428,15.855817480742703,15.046211599252839,13.880416895612141,12.810346499393798,15.574883483708991,14.470202545212443,14.80593575200785,11.50939332742055,17.04179800236674,14.598176329532912,13.409198383576713,12.85249223537345,13.700024400722302,14.759504678230106,14.622628905154071,12.665505063477408,15.769726014457838,15.886274006067366,12.220625520784768,10.789369419945245,13.321627735855383,15.509309067291722,13.116741292237027,13.433928732154358,13.569991501901264,12.285846430719545,14.63071091410939,13.184132982517005,11.85706602055917,13.855902234755698,12.569406802860634,13.063405393246407,12.518397843821226,13.032570578947023,13.16701145853364,11.037656174171659,17.029010013174602,11.760412074440277,15.821450966724056,18.639601828328768,13.133110089687465,10.27881469379214,11.880710567592285,12.02287194093318,13.220069023494975,11.31754746203441,16.521256698466253,14.189707511153232,15.310333567585248,14.788777094955298,11.270261031679707,16.35197123490947,11.449790217345143,14.031833371022936,11.87218735088465,14.122356619230823,16.16234920709738,12.869615880265505,12.18136600511357,11.97442305284719,14.413993137938599,11.72444415504553,13.710097233987096,12.272130740338946,14.433060298469428,12.879609994464,10.742414682933802,13.162215070352893,13.344911275835726,13.253253718909079,16.974524522138157,11.374704691022437,13.521892306054514,11.5433317971455,14.52503701950329,12.368618187606632,12.007794409492467,17.87648228479681,14.487208387863387,12.210844019441124,12.461943002624318,11.5540353460123,11.172473031011098,15.354490474054948,10.496494263148733,14.96777461406754,11.247501523384603,10.496857068913302,13.080062705815894,15.476509053983838,11.195565963686931,11.516265267952031,10.63463849577094,14.710007142388813,11.66620144604864,11.929644060574237,13.762648344572373,12.924126282266654,15.745511617050507,15.89045747150036,11.791105171384352,14.767329767233885,16.825196226910634,10.736094144229218,10.928005675340145,12.885949133932627,11.23090236968173,11.931165944082625,12.179379670498157,12.64671086199947,14.767474341036955,11.543409697892713,14.31963608832238,11.280266313918512,16.585405943632036,15.44411616321985,10.019821424020911,11.665672715990087,9.882157439431998,14.870700019578447,10.91243090330858,12.07938158484801,12.486880086998726,15.567968261334697,16.268982531493524,12.482972885423898,14.227691533573005,12.99243639963452,13.685404625874064,13.3478963544975,12.022537167352356,12.963420497100099,16.100935907020354,11.351873628417582,12.847351963063986,18.33128418223075,12.367848573668807,16.19630901098356,11.80961864227213,12.951581670419213,12.819857664431396,11.51019846342992,11.618246750965081,12.566081884950291,10.137228421670978,13.896507701692169,15.27121589869902,10.604237630532735,12.31247924594042,11.370960691197068,14.877027361821456,11.470835643358214,12.059149183351616,11.698852337247866,9.869982908059422,13.269969962061074,15.422281151292337,12.509611440957462,10.791366041363855,11.512435485701294,16.4306076314058,10.177561157521101,12.240180967454357,15.551756723573993,12.384507089954885,12.494501871520576,11.012276740656361,12.867717489921576,12.78656494727358,11.11165899974939,12.080451522074718,11.182143002916298,13.53931069903831,13.250644440581658,18.30358987701666,13.321373009000279,12.996375445222984,11.800059966300784,10.804805689700878,12.730179029882132,10.350095852240063,10.768754251048719,14.292539675263264,14.845594034507286,10.195412895227415,14.199599683370135,11.964801949427814,9.667579070469372,14.413216735437034,10.236346499678367,10.562735941236923,9.20382198111472,12.846279059505795,12.32003825521722,10.92341050170777,10.779139500393514,10.721006737951901,12.092211106216812,13.875691830338132,14.159905316745492,15.892125683651038,12.926425156294211,13.012205475225066,10.515633428131322,12.821662432645024,12.6290349082857,10.099530181438386,11.072847026590455,10.784477188733007,15.574610075949973,11.736514978278768,11.116858880561104,13.317436288789875,12.94443618847568,15.14701762856848,11.864000781351672,10.306080197160988,12.989615405230694,13.231668892027773,12.110598079081825,12.909791050245595,10.816730607152701,17.32126050282405,11.954291555659317,12.806628009653727,10.643324402269986,10.091346146100589,10.953344240460885,9.895602401385636,14.176384846477232,11.092263811249941,9.757562932316516,11.028465905766282,12.960543411580316,12.023141263168588,10.728385284456694,11.33043099178672,12.05440995470622,11.748037944145207,10.618355627565538,13.379258703990251,13.46726468508406,12.058466434054695,11.538316386929397,10.788898109890242,11.806949158022285,12.39766976730945,15.763095197621054,14.993350010950431,14.143545127504714,13.517919552576766,11.853458258578755,12.818124838334297,12.34321822588146,10.23705681106729,9.423955042889673,11.374954407636526,11.278325983958128,11.211903364850016,11.04136659445136,16.290116789885943,13.962216918049927,13.501710679414032,12.348770906711199,11.985740984536228,12.836052437036226,10.767314756018383,10.143545489297171,10.371995163895317,13.263546107171768,13.040948240159882,12.712013721082725,11.15155214981606,12.309605008504443,10.21895076416583,13.61592258768324,14.799160634854395,10.329202917812827,11.241650789869356,13.20691272484238,11.653292089152623,10.623294747179639,11.551388703901912,13.793824967293897,11.930807218526535,12.100801952637712,9.901282831556996,13.127428248668615,16.098521861066324,10.636890436427773,12.208867209472912,13.648782638846805,17.617164362086278,11.689027813219655,10.360856746151343,12.216713922408358,12.4149915204378,11.591305576870775,10.318705512774741,11.456761158603646,9.603217599371893,13.312126304568633,10.647670662848338,11.728312967129746,12.898764682436557,11.864705625083452,11.598446265916104,10.998270397296565,13.031869669564594,9.851510582110608,12.161944315643384,9.867584432486124,13.438925278126359,13.052895139646555,11.25219500005792,10.876188455791501,11.146353246215519,12.268240343695748,12.605223303134258,11.617556714698484,9.348086031714265,13.525399186281023,9.658506744667521,9.362132413832285,10.13396139337125,14.821928389615016,12.138919725204117,11.58764345920539,13.325276546589308,12.019997227086728,13.162874593490264,11.63772150382274,12.8557503349505,13.52872440490896,12.655463731454711,9.729088125783806,10.255957153221313,13.844337801484409,9.467842304965439,10.071938136151017,10.996860384495694,12.57341214881289,14.412519533222827,11.846843960745463,18.67993449577431,12.373774248627136,12.640551387120379,11.990454873484573,10.771050864106229,10.285132925419944,12.553423986031461,10.023383877742841,12.86131758529609,10.631271352031499,11.590694247766416,11.471627068618407,11.490768528978153,12.82080357577394,10.582944482476995,13.887466300159941,12.712993153914073,11.351899783179986,9.921073920118188,9.821374778308268,10.443474285649346,10.086427735136878,8.546639296890541,10.193310834306045,10.465620098719942,9.63646143258475,12.770510868839036,10.732740255388139,9.945626217006291,10.777521244550005,9.25455590004991,10.14202265620456,10.581402957015639,10.645383299082633,9.89435296520108,9.598104957502528,11.589951744268628,9.997523303504648,9.967864101634726,9.672510507141826,12.19007390747825,9.516268968963354,10.516795181498496,11.246693858677016,11.30694463860327,12.887090816053812,15.614986535027217,15.870742936335121,10.415827175418475,15.37274302441243,10.04065889414053,10.670327136662959,10.191270863737126,12.50566462208292,11.793225162746465,10.056774101327939,12.047489278315108,10.479435907272979,9.593703123108886,10.26918855359712,11.722312793452582,11.482818929285376,16.776531824379955,8.96627369340615,11.602723699410571,11.061640118103261,14.859108337317293,11.769555277423052,11.372799712200433,10.169951559484153,11.753775079073051,8.215259580160179,10.973615839214835,12.560191296760044,9.827292651017167,10.539027106950144,11.525854660179263,10.812420050600693,15.622180159285199,10.922722018704771,10.963034200171569,11.057143773446992,11.104811025409441,12.41217979873561,12.914106350018983,9.51705417549785,9.47500666479296,9.852213238634917,9.720503795547641,9.081102881209885,11.07556522466358,13.77457151368353,9.834240754748016,11.086395161227848,9.261121330623402,10.470136394626039,10.606487871671689,10.044629461918568,8.471906385414924,9.112559052428452,12.22859735621129,7.9450563333167254,9.537410020607773,10.66329196035001,9.599568024730111,9.716415388877284,9.596692223895465,9.274949536179285,9.793415713160963,9.425108670753456,14.597207246757666,10.029869262995343,10.866915504108754,10.3950505876543,12.492908854748016,9.344903597161084,11.875719875651583,13.9026251131023,12.072368779836902,11.18544704105556,10.212527632203532,11.018686450897862,10.931305634324527,23.247536889102136,10.040310164826163,12.125668122275659,14.280900699780766,9.354011386209796,10.47424590130088,12.681468121355548,11.063506080395033,12.037163184127643,11.353429158771128,10.811624437548593,10.75911550587176,10.949367584973501,9.721661634086562,12.216438462366702,11.459443828906736,9.178929372573313,10.455895271157994,12.880822856093863,11.64742931523044,11.110396278313196,8.742419447725679,9.89322920944598,13.709518682503925,13.35763350564543,11.92280370804145,11.974040805339605,10.679930327121248,10.471555709243026,11.963765598529843,9.750002642726848,9.604106315693311,9.28081165014074,9.865138207523012,10.205985712980484,12.540324048330447,10.362575014662896,12.906225355192126,11.86853763387871,10.218028842567287,10.17691969328726,13.246148572416624,9.82787284115349,8.836646013977097,13.074544834234725,11.027550131798868,12.643742978935395,9.173886555567131,11.60735812178298,11.911507521329707,9.831243677359002,10.348120667901162,8.561524854902828,9.983590174586102,10.989226887435903,10.16721849947884,10.902769225872953,10.80956071548065,9.33754078989781,13.298944689008529,9.152926969264254,11.285670804112769,11.395057756441224,10.891388035970484,11.906744302528706,11.07070770676767,11.011830355753172,9.633323721109011,12.531244614555577,10.371853028075284,14.52627659463303,11.516588221983199,11.026736603215848,11.488956357407448,9.991065442594067,8.325157152589005,8.54707886744255,10.529331972772434,9.365876170032132,9.46691569680368,11.385962421650357,9.170384693586623,9.46875982822292,9.204130136270306,9.438769737423096,11.648523041599189,8.636908260649916,9.12994785303068,9.487988966784775,12.967599068091523,10.744829446405031,12.76086725862514,10.665013304581866,10.828546735687114,10.018354870532756,10.565653617117125,8.853437296192148,12.53811346604392,10.885662677814125,8.890710534163125,8.737297032648494,7.919108103078744,11.777094970918846,13.202844433390576,10.549505468861286,12.341616502057905,10.677138972217687,11.617562056382726,11.683871644514742,10.87791037516505,8.849832779652534,8.986279503072614,9.277847749390304,11.455695000251112,11.616137677967282,10.010497286233004,9.68676411432611,11.350080244524273,11.669307151870406,11.997807258611601,12.06829106314947,10.978465151950944,9.287174648029398,10.604643642370188,12.132075633442478,10.295826359007224,8.373975686629548],\"type\":\"scatter\"},{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923],\"y\":[0.1153564453125,0.1156005859375,0.11541748046875,0.1158447265625,0.11529541015625,0.115234375,0.11553955078125,0.11602783203125,0.11553955078125,0.115478515625,0.11566162109375,0.134765625,0.1339111328125,0.1358642578125,0.1343994140625,0.1357421875,0.159423828125,0.2054443359375,0.2239990234375,0.314697265625,0.29296875,0.48388671875,0.38720703125,0.432373046875,0.5029296875,0.5302734375,0.58984375,0.8515625,0.77978515625,0.8857421875,0.90185546875,0.98486328125,1.015625,1.0576171875,1.146484375,1.216796875,1.33203125,1.3447265625,1.4375,1.439453125,1.482421875,1.5263671875,1.5576171875,1.6044921875,1.541015625,1.5732421875,1.609375,1.5947265625,1.65234375,1.6181640625,1.62890625,1.609375,1.689453125,1.6572265625,1.6884765625,1.673828125,1.666015625,1.6357421875,1.6533203125,1.6640625,1.6162109375,1.62890625,1.630859375,1.6064453125,1.6201171875,1.583984375,1.6103515625,1.5859375,1.583984375,1.5791015625,1.58203125,1.58984375,1.623046875,1.6259765625,1.5986328125,1.5986328125,1.611328125,1.609375,1.599609375,1.5439453125,1.5576171875,1.5595703125,1.52734375,1.546875,1.54296875,1.517578125,1.5458984375,1.4912109375,1.513671875,1.494140625,1.466796875,1.4833984375,1.4951171875,1.4794921875,1.4697265625,1.4697265625,1.4736328125,1.46484375,1.4560546875,1.4599609375,1.435546875,1.4326171875,1.435546875,1.451171875,1.42578125,1.408203125,1.396484375,1.3935546875,1.4052734375,1.3837890625,1.3916015625,1.3955078125,1.3828125,1.3798828125,1.375,1.3623046875,1.3662109375,1.3642578125,1.3349609375,1.33203125,1.3115234375,1.3330078125,1.3359375,1.333984375,1.32421875,1.3193359375,1.3203125,1.2880859375,1.2919921875,1.2841796875,1.296875,1.3173828125,1.306640625,1.28125,1.2958984375,1.29296875,1.2880859375,1.2841796875,1.28125,1.283203125,1.2607421875,1.279296875,1.2822265625,1.26953125,1.2734375,1.2607421875,1.26171875,1.25390625,1.2373046875,1.244140625,1.2578125,1.2470703125,1.2255859375,1.2255859375,1.2412109375,1.2314453125,1.21875,1.2138671875,1.2060546875,1.1953125,1.20703125,1.19140625,1.208984375,1.1982421875,1.1748046875,1.1982421875,1.1748046875,1.173828125,1.1650390625,1.1689453125,1.1640625,1.16796875,1.1689453125,1.1533203125,1.1640625,1.1474609375,1.1494140625,1.150390625,1.1416015625,1.1474609375,1.1298828125,1.138671875,1.1396484375,1.1435546875,1.1357421875,1.1298828125,1.1181640625,1.1181640625,1.1328125,1.1103515625,1.1181640625,1.142578125,1.1171875,1.1201171875,1.1181640625,1.125,1.1083984375,1.111328125,1.09765625,1.1171875,1.08984375,1.1015625,1.099609375,1.0986328125,1.0810546875,1.091796875,1.0625,1.06640625,1.076171875,1.0732421875,1.05078125,1.0537109375,1.05859375,1.0517578125,1.046875,1.0673828125,1.04296875,1.0478515625,1.0615234375,1.025390625,1.029296875,1.0244140625,1.03515625,1.0390625,1.0341796875,1.052734375,1.037109375,1.041015625,1.0244140625,1.0380859375,1.0078125,1.021484375,1.021484375,1.0185546875,1.0166015625,1.0244140625,1.0126953125,1.005859375,1.0146484375,1.021484375,1.005859375,1.013671875,1.0048828125,0.99755859375,1.0,0.9853515625,0.9921875,0.9990234375,0.99658203125,0.9931640625,0.9853515625,0.97607421875,0.97900390625,0.99072265625,1.0048828125,0.9775390625,0.98046875,0.9833984375,0.98291015625,0.97998046875,0.9765625,0.9833984375,0.98095703125,0.9716796875,0.97119140625,0.970703125,0.974609375,0.97509765625,0.970703125,0.97265625,0.95654296875,0.9619140625,0.955078125,0.94189453125,0.939453125,0.93603515625,0.91455078125,0.93701171875,0.92578125,0.9384765625,0.93505859375,0.9404296875,0.9345703125,0.947265625,0.94384765625,0.9208984375,0.923828125,0.935546875,0.91552734375,0.91748046875,0.91796875,0.916015625,0.92626953125,0.923828125,0.93359375,0.912109375,0.92724609375,0.92333984375,0.90771484375,0.91064453125,0.896484375,0.90478515625,0.9033203125,0.90380859375,0.90087890625,0.8974609375,0.89599609375,0.90087890625,0.89892578125,0.89404296875,0.89208984375,0.8916015625,0.892578125,0.89404296875,0.8896484375,0.89208984375,0.8876953125,0.8857421875,0.87890625,0.87255859375,0.86328125,0.8779296875,0.8828125,0.8642578125,0.8720703125,0.875,0.87646484375,0.86962890625,0.8935546875,0.86669921875,0.89111328125,0.87451171875,0.869140625,0.87451171875,0.87646484375,0.86767578125,0.87451171875,0.86962890625,0.857421875,0.8583984375,0.87109375,0.859375,0.861328125,0.865234375,0.857421875,0.8798828125,0.8525390625,0.85107421875,0.8662109375,0.8515625,0.8623046875,0.8603515625,0.853515625,0.86474609375,0.8701171875,0.8564453125,0.85546875,0.86376953125,0.8505859375,0.85205078125,0.859375,0.85693359375,0.8466796875,0.8486328125,0.84765625,0.85107421875,0.833984375,0.83544921875,0.84033203125,0.81494140625,0.82666015625,0.8291015625,0.8193359375,0.841796875,0.8193359375,0.8369140625,0.84375,0.83837890625,0.8359375,0.8212890625,0.82763671875,0.83447265625,0.828125,0.8359375,0.8330078125,0.81982421875,0.828125,0.81982421875,0.81103515625,0.82177734375,0.814453125,0.83740234375,0.82958984375,0.826171875,0.8173828125,0.83544921875,0.830078125,0.8076171875,0.81298828125,0.80419921875,0.8173828125,0.80810546875,0.81494140625,0.80712890625,0.806640625,0.80517578125,0.82080078125,0.81103515625,0.8095703125,0.81591796875,0.81689453125,0.8125,0.818359375,0.8203125,0.8115234375,0.822265625,0.8017578125,0.8154296875,0.80908203125,0.81591796875,0.80078125,0.8046875,0.80517578125,0.81591796875,0.8134765625,0.8076171875,0.82177734375,0.81689453125,0.79638671875,0.80126953125,0.8115234375,0.79296875,0.79248046875,0.81103515625,0.80517578125,0.80322265625,0.79931640625,0.80712890625,0.80322265625,0.79052734375,0.80908203125,0.79541015625,0.798828125,0.8037109375,0.7890625,0.7998046875,0.8056640625,0.8017578125,0.78076171875,0.7900390625,0.77880859375,0.79052734375,0.78173828125,0.77783203125,0.77099609375,0.779296875,0.78759765625,0.79345703125,0.80322265625,0.8017578125,0.7900390625,0.8017578125,0.78173828125,0.79833984375,0.78466796875,0.783203125,0.7919921875,0.783203125,0.783203125,0.798828125,0.7939453125,0.7861328125,0.787109375,0.779296875,0.80029296875,0.78759765625,0.7890625,0.7861328125,0.79736328125,0.791015625,0.78271484375,0.7841796875,0.7900390625,0.78662109375,0.779296875,0.77978515625,0.7841796875,0.7802734375,0.78271484375,0.78369140625,0.77734375,0.78466796875,0.79443359375,0.7822265625,0.79638671875,0.78466796875,0.775390625,0.76806640625,0.77490234375,0.7861328125,0.7841796875,0.77880859375,0.7685546875,0.7822265625,0.77685546875,0.77197265625,0.783203125,0.775390625,0.779296875,0.78173828125,0.77978515625,0.76171875,0.76220703125,0.763671875,0.77880859375,0.7646484375,0.78173828125,0.763671875,0.77001953125,0.77392578125,0.76953125,0.771484375,0.7607421875,0.78173828125,0.7685546875,0.767578125,0.76318359375,0.763671875,0.7802734375,0.75341796875,0.76708984375,0.759765625,0.7685546875,0.76904296875,0.763671875,0.7587890625,0.76953125,0.775390625,0.7646484375,0.7607421875,0.76611328125,0.765625,0.751953125,0.7529296875,0.76416015625,0.75634765625,0.767578125,0.765625,0.76171875,0.765625,0.7451171875,0.75634765625,0.75244140625,0.76123046875,0.75439453125,0.75390625,0.7451171875,0.76318359375,0.7685546875,0.751953125,0.75390625,0.76953125,0.75244140625,0.74462890625,0.748046875,0.7431640625,0.7587890625,0.7451171875,0.7529296875,0.759765625,0.75048828125,0.73779296875,0.7607421875,0.74951171875,0.75341796875,0.73681640625,0.7373046875,0.7412109375,0.74658203125,0.75244140625,0.74365234375,0.75537109375,0.74658203125,0.7568359375,0.7607421875,0.75439453125,0.74755859375,0.755859375,0.763671875,0.7451171875,0.736328125,0.74072265625,0.74853515625,0.74462890625,0.74560546875,0.74951171875,0.74609375,0.74755859375,0.74609375,0.7509765625,0.75341796875,0.751953125,0.7548828125,0.76318359375,0.75341796875,0.740234375,0.759765625,0.7568359375,0.75048828125,0.7578125,0.75732421875,0.755859375,0.74609375,0.75537109375,0.76171875,0.7529296875,0.74951171875,0.7529296875,0.7431640625,0.76416015625,0.76318359375,0.74365234375,0.740234375,0.7548828125,0.75244140625,0.7451171875,0.732421875,0.75,0.75341796875,0.7451171875,0.7470703125,0.7490234375,0.75341796875,0.7451171875,0.75,0.75537109375,0.75537109375,0.75341796875,0.75830078125,0.7509765625,0.75341796875,0.74267578125,0.736328125,0.73779296875,0.73974609375,0.74658203125,0.76318359375,0.76123046875,0.7412109375,0.74951171875,0.7509765625,0.75732421875,0.748046875,0.751953125,0.75537109375,0.7646484375,0.740234375,0.74267578125,0.75390625,0.7470703125,0.7568359375,0.74560546875,0.744140625,0.7451171875,0.7685546875,0.75927734375,0.76708984375,0.75390625,0.76318359375,0.76513671875,0.76953125,0.7666015625,0.74853515625,0.77783203125,0.74365234375,0.76123046875,0.7763671875,0.7568359375,0.76025390625,0.751953125,0.7783203125,0.767578125,0.7490234375,0.76904296875,0.759765625,0.75244140625,0.75927734375,0.76611328125,0.76611328125,0.76220703125,0.7607421875,0.74951171875,0.7587890625,0.76953125,0.767578125,0.77197265625,0.76708984375,0.767578125,0.7568359375,0.779296875,0.7607421875,0.77197265625,0.77490234375,0.771484375,0.77001953125,0.76904296875,0.73876953125,0.76513671875,0.75732421875,0.76513671875,0.7705078125,0.74609375,0.77734375,0.74853515625,0.76904296875,0.7548828125,0.74755859375,0.7470703125,0.76171875,0.74755859375,0.73681640625,0.75732421875,0.75537109375,0.767578125,0.7646484375,0.759765625,0.75537109375,0.759765625,0.75927734375,0.759765625,0.76123046875,0.75830078125,0.7529296875,0.755859375,0.75,0.76708984375,0.7529296875,0.755859375,0.74951171875,0.76953125,0.76123046875,0.77392578125,0.76513671875,0.74365234375,0.7509765625,0.76904296875,0.75439453125,0.767578125,0.75048828125,0.75830078125,0.75439453125,0.75048828125,0.7509765625,0.75,0.755859375,0.759765625,0.765625,0.7685546875,0.76220703125,0.76611328125,0.77197265625,0.763671875,0.765625,0.75146484375,0.7607421875,0.75634765625,0.75830078125,0.759765625,0.7724609375,0.77490234375,0.76220703125,0.75244140625,0.75927734375,0.75634765625,0.7626953125,0.7626953125,0.75439453125,0.765625,0.74951171875,0.7509765625,0.7578125,0.751953125,0.751953125,0.7509765625,0.7412109375,0.73974609375,0.75634765625,0.75244140625,0.7529296875,0.748046875,0.75244140625,0.74755859375,0.7431640625,0.748046875,0.75537109375,0.74755859375,0.75244140625,0.7685546875,0.748046875,0.74560546875,0.75,0.7548828125,0.75634765625,0.7451171875,0.755859375,0.7607421875,0.759765625,0.75146484375,0.7470703125,0.755859375,0.74609375,0.7607421875,0.7509765625,0.75634765625,0.76220703125,0.751953125,0.75390625,0.75341796875,0.7490234375,0.75439453125,0.744140625,0.74755859375,0.7578125,0.755859375,0.74853515625,0.76708984375,0.755859375,0.7587890625,0.7421875,0.7431640625,0.75732421875,0.7568359375,0.73583984375,0.75927734375,0.740234375,0.75341796875,0.75146484375,0.7626953125,0.74609375,0.74853515625,0.74658203125,0.74365234375,0.736328125,0.7392578125,0.7421875,0.7431640625,0.73291015625,0.75146484375,0.75244140625,0.76025390625,0.740234375,0.74462890625,0.7578125,0.75439453125,0.74609375,0.7392578125,0.74267578125,0.7490234375,0.759765625,0.74853515625,0.7578125,0.73974609375,0.74560546875,0.74755859375,0.74169921875,0.74609375,0.7548828125,0.75341796875,0.7353515625,0.73974609375,0.740234375,0.74560546875,0.73486328125,0.74365234375,0.75048828125,0.74365234375,0.7470703125,0.75634765625,0.7509765625,0.73681640625,0.75341796875,0.7470703125,0.75048828125,0.7490234375,0.74853515625,0.75,0.732421875,0.74169921875,0.74072265625,0.74072265625,0.73828125,0.73583984375,0.75634765625,0.73681640625,0.74365234375,0.75732421875,0.73583984375,0.75927734375,0.7490234375,0.75439453125,0.74658203125,0.7392578125,0.76611328125,0.7412109375,0.75341796875,0.7275390625,0.72998046875,0.75390625,0.7333984375,0.7431640625,0.75927734375,0.74072265625,0.74365234375,0.74267578125,0.73876953125,0.7412109375,0.75439453125,0.73291015625,0.74072265625,0.7431640625,0.74072265625,0.74169921875,0.74951171875,0.72607421875],\"type\":\"scatter\"},{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923],\"y\":[0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743,0.0006737552047782743],\"type\":\"scatter\"},{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923],\"y\":[0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.6598772881104383,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.7111729038546595,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6675335956085501,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6375208840264361,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.6018185835049124,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863,0.5952639636862863],\"type\":\"scatter\"},{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923],\"y\":[0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.6764375860780018,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.7213868430993974,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6809567408152821,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6553262415981528,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6254224954641553,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645,0.6151228832676645],\"type\":\"scatter\"}],\"name\":\"0\"}]);\n",
              "                        }).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('45d58fbd-bf86-4d1a-8262-402bc1021648');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/1::  12%|█▏        | 924/7701 [11:07<1:21:33,  1.38it/s, loss=1.5e+3, pearson_cosine=0.564, spearman_cosine=0.586, mean_grad_norm=8.28, learning_rate=0.000674]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping triggered at epoch 1, iteration 924.\n",
            "Best Spearman Correlation: 0.7213868430993974\n",
            "Best Pearson Correlation: 0.7111729038546595\n",
            "Checkpoint and graph saved to /content/drive/MyDrive/violet_checkpoints_2/checkpoint_trial_19.pt and /content/drive/MyDrive/violet_checkpoints_2/graphs/checkpoint_trial_19.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-30 18:35:09,265] Trial 19 finished with value: 0.5863060006247319 and parameters: {'projection_size': 8192, 'aug_p': 0.20943901101203366, 'learning_rate': 0.0006737552047782743, 'weight_decay': 0.1333928565968183, 'lambda_bt': 0.0017107385254106905, 'lambda_mixup': 0.6059879433460729}. Best is trial 10 with value: 0.7860914915940419.\n",
            "<ipython-input-2-e15be7047639>:486: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "<ipython-input-2-e15be7047639>:487: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-2-e15be7047639>:490: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "<ipython-input-2-e15be7047639>:491: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "<ipython-input-2-e15be7047639>:492: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[W 2025-03-30 18:35:27,736] Trial 20 failed with parameters: {'projection_size': 8192, 'aug_p': 0.24587926870359472, 'learning_rate': 3.0272569296527987e-05, 'weight_decay': 0.12177667861235464, 'lambda_bt': 0.11598513741561328, 'lambda_mixup': 1.2203494781701199} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/datasets/sentence-transformers/stsb/resolve/ab7a5ac0e35aa22088bdcf23e7fd99b220e53308/stsb.py\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/load.py\", line 1580, in dataset_module_factory\n",
            "    dataset_script_path = api.hf_hub_download(\n",
            "                          ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\", line 5248, in hf_hub_download\n",
            "    return hf_hub_download(\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 862, in hf_hub_download\n",
            "    return _hf_hub_download_to_cache_dir(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 925, in _hf_hub_download_to_cache_dir\n",
            "    (url_to_download, etag, commit_hash, expected_size, head_call_error) = _get_metadata_or_catch_error(\n",
            "                                                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 1376, in _get_metadata_or_catch_error\n",
            "    metadata = get_hf_file_metadata(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 1296, in get_hf_file_metadata\n",
            "    r = _request_wrapper(\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 280, in _request_wrapper\n",
            "    response = _request_wrapper(\n",
            "               ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 304, in _request_wrapper\n",
            "    hf_raise_for_status(response)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\", line 420, in hf_raise_for_status\n",
            "    raise _format(EntryNotFoundError, message, response) from e\n",
            "huggingface_hub.errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-67e98ede-5b5d800442cc160557e4008b;437d4f0e-9f16-45d3-94e3-33f4a4e5e9c7)\n",
            "\n",
            "Entry Not Found for url: https://huggingface.co/datasets/sentence-transformers/stsb/resolve/ab7a5ac0e35aa22088bdcf23e7fd99b220e53308/stsb.py.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-2-e15be7047639>\", line 497, in objective\n",
            "    trainer = BarlowTwinsNCSE(config)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-2-e15be7047639>\", line 31, in __init__\n",
            "    self._prepare_datasets()\n",
            "  File \"<ipython-input-2-e15be7047639>\", line 234, in _prepare_datasets\n",
            "    self.train_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"train\")\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/load.py\", line 2062, in load_dataset\n",
            "    builder_instance = load_dataset_builder(\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/load.py\", line 1782, in load_dataset_builder\n",
            "    dataset_module = dataset_module_factory(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/load.py\", line 1629, in dataset_module_factory\n",
            "    ).get_module()\n",
            "      ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/load.py\", line 1019, in get_module\n",
            "    data_files = DataFilesDict.from_patterns(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\", line 690, in from_patterns\n",
            "    else DataFilesList.from_patterns(\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\", line 583, in from_patterns\n",
            "    resolve_pattern(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\", line 361, in resolve_pattern\n",
            "    for filepath, info in fs.glob(pattern, detail=True, **glob_kwargs).items()\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_file_system.py\", line 521, in glob\n",
            "    return super().glob(path, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fsspec/spec.py\", line 609, in glob\n",
            "    allpaths = self.find(root, maxdepth=depth, withdirs=True, detail=True, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_file_system.py\", line 556, in find\n",
            "    return super().find(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fsspec/spec.py\", line 500, in find\n",
            "    out[path] = self.info(path)\n",
            "                ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_file_system.py\", line 719, in info\n",
            "    paths_info = self._api.get_paths_info(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\", line 3281, in get_paths_info\n",
            "    response = get_session().post(\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 637, in post\n",
            "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\", line 96, in send\n",
            "    return super().send(request, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 667, in send\n",
            "    resp = conn.urlopen(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
            "    response = self._make_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 534, in _make_request\n",
            "    response = conn.getresponse()\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\", line 516, in getresponse\n",
            "    httplib_response = super().getresponse()\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 1395, in getresponse\n",
            "    response.begin()\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 325, in begin\n",
            "    version, status, reason = self._read_status()\n",
            "                              ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 286, in _read_status\n",
            "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/socket.py\", line 718, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ssl.py\", line 1314, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ssl.py\", line 1166, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-03-30 18:35:27,763] Trial 20 failed with value None.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/datasets/sentence-transformers/stsb/resolve/ab7a5ac0e35aa22088bdcf23e7fd99b220e53308/stsb.py",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1580\u001b[0;31m                 dataset_script_path = api.hf_hub_download(\n\u001b[0m\u001b[1;32m   1581\u001b[0m                     \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(self, repo_id, filename, subfolder, repo_type, revision, cache_dir, local_dir, force_download, proxies, etag_timeout, token, local_files_only, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   5247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5248\u001b[0;31m         return hf_hub_download(\n\u001b[0m\u001b[1;32m   5249\u001b[0m             \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    863\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;31m# If we can't, a HEAD request error is returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m     (url_to_download, etag, commit_hash, expected_size, head_call_error) = _get_metadata_or_catch_error(\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1377\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1297\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    281\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{response.status_code} Client Error.\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"Entry Not Found for url: {response.url}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEntryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-67e98ede-5b5d800442cc160557e4008b;437d4f0e-9f16-45d3-94e3-33f4a4e5e9c7)\n\nEntry Not Found for url: https://huggingface.co/datasets/sentence-transformers/stsb/resolve/ab7a5ac0e35aa22088bdcf23e7fd99b220e53308/stsb.py.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e15be7047639>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0mload_if_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m )\n\u001b[0;32m--> 523\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best trial:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-e15be7047639>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    495\u001b[0m     }\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBarlowTwinsNCSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m     \u001b[0;31m# Resume from checkpoint if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"checkpoint_trial_{trial.number}.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-e15be7047639>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# Enable cuDNN benchmarking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_optimizer_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-e15be7047639>\u001b[0m in \u001b[0;36m_prepare_datasets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentence-transformers/stsb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentence-transformers/stsb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentence-transformers/stsb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2062\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2063\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1783\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m                     \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m                     \u001b[0muse_exported_dataset_infos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_exported_dataset_infos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m                 ).get_module()\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mGatedRepoError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m                 \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Dataset '{path}' is a gated dataset on the Hub.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mget_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0mpatterns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m         data_files = DataFilesDict.from_patterns(\n\u001b[0m\u001b[1;32m   1020\u001b[0m             \u001b[0mpatterns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mfrom_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    688\u001b[0m                 \u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFilesList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m                 else DataFilesList.from_patterns(\n\u001b[0m\u001b[1;32m    691\u001b[0m                     \u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                     \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mfrom_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                 data_files.extend(\n\u001b[0;32m--> 583\u001b[0;31m                     resolve_pattern(\n\u001b[0m\u001b[1;32m    584\u001b[0m                         \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                         \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    359\u001b[0m     matched_paths = [\n\u001b[1;32m    360\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol_prefix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mprotocol_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mglob_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"islink\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles_to_ignore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_file_system.py\u001b[0m in \u001b[0;36mglob\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"expand_info\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"detail\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"revision\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     def find(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mglob\u001b[0;34m(self, path, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m                 \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0mallpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwithdirs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mends_with_sep\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_file_system.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(self, path, maxdepth, withdirs, detail, refresh, revision, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \"\"\"\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxdepth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             return super().find(\n\u001b[0m\u001b[1;32m    557\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwithdirs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwithdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(self, path, maxdepth, withdirs, detail, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# This is needed for posix glob compliance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwithdirs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_file_system.py\u001b[0m in \u001b[0;36minfo\u001b[0;34m(self, path, refresh, revision, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrefresh\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexpand_info\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"last_commit\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 paths_info = self._api.get_paths_info(\n\u001b[0m\u001b[1;32m    720\u001b[0m                     \u001b[0mresolved_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m                     \u001b[0mresolved_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_in_repo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mget_paths_info\u001b[0;34m(self, repo_id, paths, expand, revision, repo_type, token)\u001b[0m\n\u001b[1;32m   3279\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_hf_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3281\u001b[0;31m         response = get_session().post(\n\u001b[0m\u001b[1;32m   3282\u001b[0m             \u001b[0;34mf\"{self.endpoint}/api/{repo_type}s/{repo_id}/paths-info/{revision}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3283\u001b[0m             data={\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \"\"\"\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Send: {_curlify(request)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_AMZN_TRACE_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "import os\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from sentence_transformers import SentenceTransformer, models, util\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "import copy\n",
        "import gc\n",
        "import math\n",
        "import random\n",
        "import nlpaug.augmenter.word as naw\n",
        "from IPython.display import display, clear_output\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "from torch import GradScaler\n",
        "from torch.amp import autocast\n",
        "import optuna\n",
        "\n",
        "# ----- Your BarlowTwinsNCSE and SentenceDataset classes as provided -----\n",
        "# (The code below is the same as your original, including the plotting functions.)\n",
        "\n",
        "class BarlowTwinsNCSE:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        torch.backends.cudnn.benchmark = True  # Enable cuDNN benchmarking\n",
        "        self._prepare_datasets()\n",
        "        self._initialize_models()\n",
        "        self._initialize_optimizer_scheduler()\n",
        "        self.scaler = GradScaler(\"cuda\", enabled=self.config.get(\"use_amp\", True))  # GradScaler for AMP\n",
        "        self.best_spearman = -float(\"inf\")\n",
        "        self.best_pearson = -float(\"inf\")\n",
        "        self.patience_counter = 0\n",
        "        self.augmenters = [\n",
        "            naw.SynonymAug(aug_src='wordnet', aug_p=self.config[\"aug_p\"]),\n",
        "            naw.RandomWordAug(action=\"swap\", aug_p=self.config[\"aug_p\"]),\n",
        "            naw.RandomWordAug(aug_p=self.config[\"aug_p\"]),\n",
        "        ]\n",
        "        self.test_sts_pearson_cosine_values = []\n",
        "        self.test_sts_spearman_cosine_values = []\n",
        "        self.test_iterations = []\n",
        "        self._create_plot()\n",
        "\n",
        "    def _create_plot(self):\n",
        "        self.loss_values = []\n",
        "        self.sts_pearson_cosine_values = []\n",
        "        self.sts_spearman_cosine_values = []\n",
        "        self.mean_grad_norm_values = []\n",
        "        self.variance_values = []\n",
        "        self.learning_rate_values = []\n",
        "        self.iterations = []\n",
        "        self.epochs = []\n",
        "\n",
        "        # 3 rows x 2 columns grid with 6 subplots\n",
        "        self.fig = make_subplots(\n",
        "            rows=3, cols=2,\n",
        "            subplot_titles=(\n",
        "                \"Loss vs Iterations\",\n",
        "                \"Mean Gradient Norm vs Iterations\",\n",
        "                \"Variance vs Iterations\",\n",
        "                \"Learning Rate vs Iterations\",\n",
        "                \"Dev STS Cosine (Pearson & Spearman) vs Iterations\",\n",
        "                \"Test STS Cosine (Pearson & Spearman) vs Iterations\"\n",
        "            )\n",
        "        )\n",
        "        # Dev metrics\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Loss'), row=1, col=1)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Mean Gradient Norm'), row=1, col=2)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Variance'), row=2, col=1)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Learning Rate'), row=2, col=2)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Dev STS Pearson Cosine'), row=3, col=1)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Dev STS Spearman Cosine'), row=3, col=1)\n",
        "        # Test metrics\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Test STS Pearson Cosine'), row=3, col=2)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Test STS Spearman Cosine'), row=3, col=2)\n",
        "\n",
        "        # Prepare footer text\n",
        "        footer_text = \", \".join([f\"{key}={value}\" for key, value in self.config.items()])\n",
        "        augmenters_text = \", Augmenters: \" + \", \".join([f\"{aug.name}[{aug.action}:{aug.aug_p}]\" for aug in self.augmenters])\n",
        "        footer_text += augmenters_text\n",
        "        wrapped_footer = \"<br>\".join(textwrap.wrap(footer_text, width=160))\n",
        "\n",
        "        # Configure the download button\n",
        "        self.plot_config = {\n",
        "            'toImageButtonOptions': {\n",
        "                'filename': self.config[\"model_save_path\"],\n",
        "                'format': 'png',\n",
        "                'width': 1200,\n",
        "                'height': 800,\n",
        "                'scale': 1\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.fig.update_layout(\n",
        "            width=1200,\n",
        "            height=800,\n",
        "            title_text='Training Metrics',\n",
        "            showlegend=True,\n",
        "            margin=dict(l=50, r=50, t=100, b=150),\n",
        "            annotations=[\n",
        "                dict(\n",
        "                    text=wrapped_footer,\n",
        "                    showarrow=False,\n",
        "                    xref=\"paper\",\n",
        "                    yref=\"paper\",\n",
        "                    x=0.5,\n",
        "                    y=-0.15,\n",
        "                    xanchor='center',\n",
        "                    yanchor='top',\n",
        "                    align=\"center\",\n",
        "                    font=dict(size=10, color=\"gray\")\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Annotation for best metrics\n",
        "        self.best_metrics_annotation_index = len(self.fig.layout.annotations)\n",
        "        self.fig.add_annotation(\n",
        "            text=\"Best Spearman (Test): N/A<br>Best Pearson (Test): N/A<br>Best Spearman (Val): N/A<br>Best Pearson (Val): N/A\",\n",
        "            showarrow=False,\n",
        "            xref=\"paper\",\n",
        "            yref=\"paper\",\n",
        "            x=1.0,\n",
        "            y=0.0,\n",
        "            xanchor='right',\n",
        "            yanchor='bottom',\n",
        "            align=\"right\",\n",
        "            font=dict(size=12, color=\"blue\")\n",
        "        )\n",
        "\n",
        "        self.fig.show(config=self.plot_config)\n",
        "\n",
        "    def _update_traces(self):\n",
        "        with self.fig.batch_update():\n",
        "            self.fig.data[0].x = self.iterations\n",
        "            self.fig.data[0].y = self.loss_values\n",
        "            self.fig.data[1].x = self.iterations\n",
        "            self.fig.data[1].y = self.mean_grad_norm_values\n",
        "            self.fig.data[2].x = self.iterations\n",
        "            self.fig.data[2].y = self.variance_values\n",
        "            self.fig.data[3].x = self.iterations\n",
        "            self.fig.data[3].y = self.learning_rate_values\n",
        "            self.fig.data[4].x = self.iterations\n",
        "            self.fig.data[4].y = self.sts_pearson_cosine_values\n",
        "            self.fig.data[5].x = self.iterations\n",
        "            self.fig.data[5].y = self.sts_spearman_cosine_values\n",
        "            self.fig.data[6].x = self.test_iterations\n",
        "            self.fig.data[6].y = self.test_sts_pearson_cosine_values\n",
        "            self.fig.data[7].x = self.test_iterations\n",
        "            self.fig.data[7].y = self.test_sts_spearman_cosine_values\n",
        "\n",
        "            for i in range(1, 4):\n",
        "                for j in range(1, 3):\n",
        "                    self.fig.update_yaxes(autorange=True, row=i, col=j)\n",
        "                    self.fig.update_xaxes(autorange=True, row=i, col=j)\n",
        "\n",
        "    def _update_plot(self):\n",
        "        self._update_traces()\n",
        "        unique_epochs = sorted(set(self.epochs))\n",
        "        frames = []\n",
        "        for ep in unique_epochs:\n",
        "            indices = [i for i, e in enumerate(self.epochs) if e == ep]\n",
        "            frame = go.Frame(\n",
        "                data=[\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.loss_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.mean_grad_norm_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.variance_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.learning_rate_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.sts_pearson_cosine_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.sts_spearman_cosine_values[i] for i in indices])\n",
        "                ],\n",
        "                name=str(ep)\n",
        "            )\n",
        "            frames.append(frame)\n",
        "        self.fig.frames = frames\n",
        "\n",
        "        slider_steps = [\n",
        "            {\"args\": [[str(ep)], {\"frame\": {\"duration\": 0, \"redraw\": True},\n",
        "                                   \"mode\": \"immediate\", \"transition\": {\"duration\": 0}}],\n",
        "             \"label\": str(ep), \"method\": \"animate\"} for ep in unique_epochs\n",
        "        ]\n",
        "\n",
        "        self.fig.update_layout(\n",
        "            sliders=[{\n",
        "                \"active\": len(unique_epochs) - 1 if unique_epochs else 0,\n",
        "                \"currentvalue\": {\"prefix\": \"Epoch: \"},\n",
        "                \"pad\": {\"t\": 50},\n",
        "                \"steps\": slider_steps\n",
        "            }]\n",
        "        )\n",
        "\n",
        "        # Compute best test metrics if available\n",
        "        if self.test_sts_spearman_cosine_values:\n",
        "            best_test_spearman = max(self.test_sts_spearman_cosine_values)\n",
        "        else:\n",
        "            best_test_spearman = float('nan')\n",
        "        if self.test_sts_pearson_cosine_values:\n",
        "            best_test_pearson = max(self.test_sts_pearson_cosine_values)\n",
        "        else:\n",
        "            best_test_pearson = float('nan')\n",
        "\n",
        "        self.fig.layout.annotations[self.best_metrics_annotation_index].text = (\n",
        "            f\"Best Spearman (Test): {best_test_spearman:.4f}<br>\"\n",
        "            f\"Best Pearson (Test): {best_test_pearson:.4f}<br>\"\n",
        "            f\"Best Spearman (Val): {self.best_spearman:.4f}<br>\"\n",
        "            f\"Best Pearson (Val): {self.best_pearson:.4f}\"\n",
        "        )\n",
        "\n",
        "        for i in range(1, 4):\n",
        "            for j in range(1, 3):\n",
        "                self.fig.update_yaxes(autorange=True, row=i, col=j)\n",
        "                self.fig.update_xaxes(autorange=True, row=i, col=j)\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        self.fig.show(config=self.plot_config)\n",
        "\n",
        "    def _prepare_datasets(self):\n",
        "        wikipedia_url = \"https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse/resolve/main/wiki1m_for_simcse.txt\"\n",
        "        wikipedia_dataset_path = \"data/wiki1m_for_simcse.txt\"\n",
        "        if not os.path.exists(wikipedia_dataset_path):\n",
        "            util.http_get(wikipedia_url, wikipedia_dataset_path)\n",
        "        train_sentences = []\n",
        "        with open(wikipedia_dataset_path, encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if len(line) >= 10:\n",
        "                    train_sentences.append(line)\n",
        "        self.train_sentences = train_sentences\n",
        "\n",
        "        self.train_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"train\")\n",
        "        self.eval_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n",
        "        self.test_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"test\")\n",
        "\n",
        "        self.train_data_loader = DataLoader(\n",
        "            SentenceDataset(self.train_sentences),\n",
        "            batch_size=self.config[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            num_workers=self.config[\"num_workers\"],\n",
        "            pin_memory=True\n",
        "        )\n",
        "        self.test_evaluator = EmbeddingSimilarityEvaluator(\n",
        "            sentences1=self.test_dataset[\"sentence1\"],\n",
        "            sentences2=self.test_dataset[\"sentence2\"],\n",
        "            scores=self.test_dataset[\"score\"]\n",
        "        )\n",
        "\n",
        "        self.evaluate_steps = max(len(self.train_data_loader) // 50, 1)\n",
        "\n",
        "        # Ensure patience is at least twice as large as evaluate_steps\n",
        "        if self.config.get(\"patience\", 0) < self.evaluate_steps * 5:\n",
        "            print(f\"Warning: Patience ({self.config['patience']}) is less than evaluation steps x5 ({self.evaluate_steps * 5}). Adjusting patience to {self.evaluate_steps * 5}.\")\n",
        "            self.config[\"patience\"] = self.evaluate_steps * 5\n",
        "\n",
        "\n",
        "    def _get_random_augmentation(self):\n",
        "        return random.choice(self.augmenters)\n",
        "\n",
        "    def _apply_augmentation(self, sentences, aug):\n",
        "        return aug.augment(sentences)\n",
        "\n",
        "    def _initialize_models(self):\n",
        "        word_embedding_model = models.Transformer(\n",
        "            self.config[\"model_name\"],\n",
        "            max_seq_length=self.config[\"max_seq_length\"],\n",
        "            config_args={\"attention_dropout\": self.config[\"aug_p\"], \"dropout\": self.config[\"aug_p\"]}\n",
        "        )\n",
        "        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "        projection_layers = [torch.nn.Linear(768, self.config[\"projection_size\"])]\n",
        "        for _ in range(self.config[\"projection_depth\"] - 1):\n",
        "            projection_layers.append(torch.nn.BatchNorm1d(self.config[\"projection_size\"]))\n",
        "            projection_layers.append(torch.nn.ReLU())\n",
        "            projection_layers.append(torch.nn.Linear(self.config[\"projection_size\"], self.config[\"projection_size\"]))\n",
        "        projection_head = torch.nn.Sequential(*projection_layers)\n",
        "        self.online_model = SentenceTransformer(modules=[word_embedding_model, pooling_model, projection_head]).to(self.device)\n",
        "        encoder_modules = [copy.deepcopy(self.online_model[i]) for i in range(2)]\n",
        "        self.encoder = SentenceTransformer(modules=encoder_modules).to(self.device)\n",
        "\n",
        "    def _update_encoder(self):\n",
        "        for i in range(len(self.encoder)):\n",
        "            self.encoder[i].load_state_dict(self.online_model[i].state_dict())\n",
        "\n",
        "    def _initialize_optimizer_scheduler(self):\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.online_model.parameters(),\n",
        "            lr=self.config[\"learning_rate\"],\n",
        "            betas=(0.9, 0.999),\n",
        "            eps=1e-8,\n",
        "            weight_decay=self.config[\"weight_decay\"]\n",
        "        )\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=200, verbose=True)\n",
        "\n",
        "    def _forward_pass(self, model, sentences, train):\n",
        "        if train:\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "        features = model.tokenize(sentences)\n",
        "        features = {k: v.to(self.device, non_blocking=True) for k, v in features.items()}\n",
        "        with autocast(\"cuda\", enabled=self.config.get(\"use_amp\", True)):\n",
        "            embeddings = model[0](features)[\"token_embeddings\"]\n",
        "            pooled = model[1]({\"token_embeddings\": embeddings})[\"sentence_embedding\"]\n",
        "            if len(model) > 2:\n",
        "                return model[2](pooled)\n",
        "            else:\n",
        "                return pooled\n",
        "\n",
        "    def _mixed_barlow_twins_loss(self, z_a, z_b):\n",
        "        N, D = z_a.size()\n",
        "        z_a_norm = (z_a - z_a.mean(dim=0)) / (z_a.std(dim=0) + 1e-6)\n",
        "        z_b_norm = (z_b - z_b.mean(dim=0)) / (z_b.std(dim=0) + 1e-6)\n",
        "        c = torch.matmul(z_a_norm.T, z_b_norm) / N\n",
        "        I = torch.eye(D, device=z_a.device)\n",
        "        c_diff = (c - I).pow(2)\n",
        "        off_diag_mask = ~torch.eye(D, dtype=torch.bool, device=z_a.device)\n",
        "        c_diff[off_diag_mask] *= self.config[\"lambda_bt\"]\n",
        "        loss_bt = c_diff.sum()\n",
        "        # MixUp Regularization\n",
        "        idx = torch.randperm(N)\n",
        "        alpha = torch.tensor(np.random.beta(1.0, 1.0), device=z_a.device, dtype=z_a.dtype)\n",
        "        z_m = alpha * z_a + (1 - alpha) * z_b[idx, :]\n",
        "        z_m_norm = (z_m - z_m.mean(dim=0)) / (z_m.std(dim=0) + 1e-6)\n",
        "        cc_m_a = torch.matmul(z_m_norm.T, z_a_norm) / N\n",
        "        cc_m_b = torch.matmul(z_m_norm.T, z_b_norm) / N\n",
        "        cc_m_a_gt = alpha * torch.matmul(z_a_norm.T, z_a_norm) / N + (1 - alpha) * torch.matmul(z_b_norm[idx, :].T, z_a_norm) / N\n",
        "        cc_m_b_gt = alpha * torch.matmul(z_a_norm.T, z_b_norm) / N + (1 - alpha) * torch.matmul(z_b_norm[idx, :].T, z_b_norm) / N\n",
        "        loss_mix = self.config[\"lambda_mixup\"] * self.config[\"lambda_bt\"] * (\n",
        "            (cc_m_a - cc_m_a_gt).pow(2).sum() + (cc_m_b - cc_m_b_gt).pow(2).sum()\n",
        "        )\n",
        "        return loss_bt + loss_mix\n",
        "\n",
        "    def _evaluate_without_heads(self):\n",
        "        self._update_encoder()\n",
        "        self.encoder.eval()\n",
        "        indices = list(range(len(self.eval_dataset[\"sentence1\"])))\n",
        "        random.shuffle(indices)\n",
        "        sentences1 = [self.eval_dataset[\"sentence1\"][i] for i in indices]\n",
        "        sentences2 = [self.eval_dataset[\"sentence2\"][i] for i in indices]\n",
        "        scores = [self.eval_dataset[\"score\"][i] for i in indices]\n",
        "        evaluator = EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)\n",
        "        return evaluator(self.encoder)\n",
        "\n",
        "    def fit(self):\n",
        "        latest_eval_metrics = {}\n",
        "        for epoch in range(self.config[\"epochs\"]):\n",
        "            early_stop = False\n",
        "            epoch_loss = 0\n",
        "            pbar = tqdm(self.train_data_loader, desc=f\"Epoch {epoch+1}/{self.config['epochs']}:\")\n",
        "            for idx, sentences in enumerate(pbar):\n",
        "                s1 = self._apply_augmentation(sentences, self._get_random_augmentation())\n",
        "                s2 = self._apply_augmentation(sentences, self._get_random_augmentation())\n",
        "                with autocast(\"cuda\", enabled=self.config.get(\"use_amp\", True)):\n",
        "                    z_a = self._forward_pass(self.online_model, s1, train=True)\n",
        "                    z_b = self._forward_pass(self.online_model, s2, train=True)\n",
        "                    loss = self._mixed_barlow_twins_loss(z_a, z_b)\n",
        "                epoch_loss += loss.item()\n",
        "                self.optimizer.zero_grad()\n",
        "                self.scaler.scale(loss).backward()\n",
        "                scale = self.scaler.get_scale()\n",
        "                scale = scale if scale != 0 else 1e-8\n",
        "                total_norm_scaled = 0.0\n",
        "                for param in self.online_model.parameters():\n",
        "                    if param.grad is not None:\n",
        "                        param_norm = param.grad.data.norm(2).item()\n",
        "                        total_norm_scaled += param_norm ** 2\n",
        "                total_norm_scaled = math.sqrt(total_norm_scaled)\n",
        "                total_norm = total_norm_scaled / scale\n",
        "                mean_grad_norm = total_norm / len(list(self.online_model.parameters()))\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "                self.scheduler.step(loss)\n",
        "                if idx % self.evaluate_steps == 0 or idx in [0, len(self.train_data_loader)-1]:\n",
        "                    latest_eval_metrics = self._evaluate_without_heads()\n",
        "                    last_spearman = latest_eval_metrics.get('spearman_cosine', -float('inf'))\n",
        "                    last_pearson = latest_eval_metrics.get('pearson_cosine', -float('inf'))\n",
        "                    self._update_encoder()\n",
        "                    self.encoder.eval()\n",
        "                    test_metrics = self.test_evaluator(self.encoder)\n",
        "                    self.test_sts_pearson_cosine_values.append(test_metrics.get('pearson_cosine', np.nan))\n",
        "                    self.test_sts_spearman_cosine_values.append(test_metrics.get('spearman_cosine', np.nan))\n",
        "                    self.test_iterations.append(idx)\n",
        "                    self._update_plot()\n",
        "\n",
        "                    if last_spearman > self.best_spearman:\n",
        "                        self.best_spearman = last_spearman\n",
        "                        self.best_pearson = last_pearson\n",
        "                        self.patience_counter = 0\n",
        "                    else:\n",
        "                        self.patience_counter += self.evaluate_steps\n",
        "                pbar.set_postfix({\n",
        "                    \"loss\": loss.item(),\n",
        "                    **latest_eval_metrics,\n",
        "                    \"mean_grad_norm\": mean_grad_norm,\n",
        "                    \"learning_rate\": self.optimizer.param_groups[0]['lr']\n",
        "                })\n",
        "                self.loss_values.append(loss.item())\n",
        "                self.sts_pearson_cosine_values.append(latest_eval_metrics.get('pearson_cosine', np.nan))\n",
        "                self.sts_spearman_cosine_values.append(latest_eval_metrics.get('spearman_cosine', np.nan))\n",
        "                self.mean_grad_norm_values.append(mean_grad_norm)\n",
        "                self.variance_values.append(torch.var(z_a, dim=0).mean().item())\n",
        "                self.learning_rate_values.append(self.optimizer.param_groups[0]['lr'])\n",
        "                self.iterations.append(idx)\n",
        "                self.epochs.append(epoch)\n",
        "                if idx % self.evaluate_steps == 0 or idx in [0, len(self.train_data_loader)-1]:\n",
        "                    if self.patience_counter >= self.config[\"patience\"]:\n",
        "                        early_stop = True\n",
        "                        print(f\"Early stopping triggered at epoch {epoch+1}, iteration {idx}.\")\n",
        "                        print(f\"Best Spearman Correlation: {self.best_spearman}\")\n",
        "                        print(f\"Best Pearson Correlation: {self.best_pearson}\")\n",
        "                        break\n",
        "            if early_stop:\n",
        "                break\n",
        "            avg_loss = epoch_loss / len(self.train_data_loader)\n",
        "            pbar.set_description(f\"Epoch {epoch+1} Loss: {avg_loss}\")\n",
        "\n",
        "    def cleanup(self):\n",
        "        del self.online_model, self.optimizer, self.scheduler, self.scaler\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "class SentenceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sentences):\n",
        "        self.sentences = sentences\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentences[idx]\n",
        "\n",
        "# ----- End of original code -----\n",
        "\n",
        "# Mount Google Drive for checkpoint and study persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/violet_checkpoints_2\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "STUDY_DB_PATH = os.path.join(CHECKPOINT_DIR, \"llm_finetuning_study_2.db\")\n",
        "\n",
        "def save_checkpoint(trial_number, trainer):\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_trial_{trial_number}.pt\")\n",
        "    torch.save({\n",
        "        'online_model_state_dict': trainer.online_model.state_dict(),\n",
        "        'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
        "        'scheduler_state_dict': trainer.scheduler.state_dict(),\n",
        "        'best_spearman': trainer.best_spearman,\n",
        "        'best_pearson': trainer.best_pearson,\n",
        "        'epochs': trainer.epochs,\n",
        "        'iterations': trainer.iterations,\n",
        "    }, checkpoint_path)\n",
        "\n",
        "    # Also save the graph to a subfolder called \"graphs\"\n",
        "    graphs_dir = os.path.join(CHECKPOINT_DIR, \"graphs\")\n",
        "    os.makedirs(graphs_dir, exist_ok=True)\n",
        "    graph_path = os.path.join(graphs_dir, f\"checkpoint_trial_{trial_number}.png\")\n",
        "    trainer.fig.write_image(graph_path)\n",
        "    print(f\"Checkpoint and graph saved to {checkpoint_path} and {graph_path}\")\n",
        "\n",
        "    return checkpoint_path\n",
        "\n",
        "def load_checkpoint(trainer, checkpoint_path):\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        trainer.online_model.load_state_dict(checkpoint['online_model_state_dict'])\n",
        "        trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        trainer.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        trainer.best_spearman = checkpoint.get('best_spearman', -float(\"inf\"))\n",
        "        trainer.best_pearson = checkpoint.get('best_pearson', -float(\"inf\"))\n",
        "        trainer.epochs = checkpoint.get('epochs', [])\n",
        "        trainer.iterations = checkpoint.get('iterations', [])\n",
        "        print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
        "\n",
        "def objective(trial):\n",
        "    # Experiment with various hyperparameters\n",
        "    config = {\n",
        "        \"model_name\": \"distilbert-base-uncased\",\n",
        "        \"batch_size\": 128, #trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512]),\n",
        "        \"projection_depth\": 2, # trial.suggest_int(\"projection_depth\", 2, 6),\n",
        "        \"projection_size\": trial.suggest_categorical(\"projection_size\", [8192, 12288]),# [2048, 4096, 6144, 8192]),\n",
        "        \"epochs\": 1,  # fixed at 1\n",
        "        \"warmup_proportion\": 0.0,\n",
        "        \"max_seq_length\": 75, #trial.suggest_categorical(\"max_seq_length\", [32, 64, 75]),\n",
        "        \"aug_p\": trial.suggest_uniform(\"aug_p\", 0.2, 0.4),\n",
        "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 3e-5, 1e-3),\n",
        "        \"model_save_path\": os.path.join(CHECKPOINT_DIR, f\"train_stsb_bt-distilbert-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_trial{trial.number}\"),\n",
        "        \"num_workers\": 10,# 2\n",
        "        \"weight_decay\": trial.suggest_uniform(\"weight_decay\", 0.1, 0.2),\n",
        "        \"lambda_bt\": trial.suggest_uniform(\"lambda_bt\", 0.001, 0.2),\n",
        "        \"lambda_mixup\": trial.suggest_uniform(\"lambda_mixup\", 0.6, 1.5),\n",
        "        \"use_amp\": True,\n",
        "        \"patience\": 500\n",
        "    }\n",
        "\n",
        "    trainer = BarlowTwinsNCSE(config)\n",
        "    # Resume from checkpoint if available\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_trial_{trial.number}.pt\")\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        load_checkpoint(trainer, checkpoint_path)\n",
        "\n",
        "    try:\n",
        "        trainer.fit()\n",
        "    except KeyboardInterrupt:\n",
        "        save_checkpoint(trial.number, trainer)\n",
        "        raise optuna.TrialPruned(\"Trial interrupted and checkpoint saved.\")\n",
        "\n",
        "    # Save checkpoint at end of trial\n",
        "    save_checkpoint(trial.number, trainer)\n",
        "\n",
        "    # Use the STS-B validation metric (spearman cosine) as objective.\n",
        "    val_metrics = trainer._evaluate_without_heads()\n",
        "    return val_metrics.get(\"spearman_cosine\", -float(\"inf\"))\n",
        "\n",
        "# Create or resume an Optuna study persisted on Google Drive\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=\"llm_finetuning_study_2\",\n",
        "    storage=f\"sqlite:///{STUDY_DB_PATH}\",\n",
        "    load_if_exists=True\n",
        ")\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSdRrcJ1Hrz0",
        "outputId": "1ed49a27-fb2a-408e-bf5e-dfe4ed10ad9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-30 12:56:37,239] Using an existing study with name 'llm_finetuning_study' instead of creating a new one.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best trial:\n",
            "FrozenTrial(number=25, state=1, values=[0.8027423473243455], datetime_start=datetime.datetime(2025, 3, 28, 14, 9, 18, 733650), datetime_complete=datetime.datetime(2025, 3, 28, 14, 40, 13, 582679), params={'batch_size': 128, 'projection_depth': 2, 'projection_size': 6144, 'max_seq_length': 75, 'aug_p': 0.11568972876642647, 'learning_rate': 5.7605107036319744e-05, 'weight_decay': 0.17805243663726578, 'lambda_bt': 0.1117488685923089, 'lambda_mixup': 0.9545383539595996}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'batch_size': CategoricalDistribution(choices=(64, 128, 256, 512)), 'projection_depth': IntDistribution(high=6, log=False, low=2, step=1), 'projection_size': CategoricalDistribution(choices=(2048, 4096, 6144, 8192)), 'max_seq_length': CategoricalDistribution(choices=(32, 64, 75)), 'aug_p': FloatDistribution(high=0.5, log=False, low=0.1, step=None), 'learning_rate': FloatDistribution(high=0.001, log=True, low=1e-05, step=None), 'weight_decay': FloatDistribution(high=0.5, log=False, low=0.001, step=None), 'lambda_bt': FloatDistribution(high=0.5, log=False, low=0.001, step=None), 'lambda_mixup': FloatDistribution(high=1.5, log=False, low=0.1, step=None)}, trial_id=26, value=None)\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive for checkpoint and study persistence\n",
        "import os\n",
        "import optuna\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/violet_checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "STUDY_DB_PATH = os.path.join(CHECKPOINT_DIR, \"llm_finetuning_study.db\")\n",
        "\n",
        "# Create or resume an Optuna study persisted on Google Drive\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=\"llm_finetuning_study\",\n",
        "    storage=f\"sqlite:///{STUDY_DB_PATH}\",\n",
        "    load_if_exists=True\n",
        ")\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8ThE5RmVwLCU",
        "outputId": "7cbf59c6-9ff5-48de-98fe-e6fabccbf821"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"53bdc0a0-0738-45c3-b910-185d07ff17a3\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"53bdc0a0-0738-45c3-b910-185d07ff17a3\")) {                    Plotly.newPlot(                        \"53bdc0a0-0738-45c3-b910-185d07ff17a3\",                        [{\"mode\":\"markers\",\"name\":\"Objective Value\",\"x\":[0,1,2,3,4,7,8,9,11,12,13,14,15,16,17,18,20,21,22,23,24,25,30,31,32,33,34,35,37,38,39,40,41,44,48,49,50,53,54,55,57,59,60,61,64,65,66,69,70],\"y\":[0.7455494904284854,0.0236697649623803,0.75931680276735,0.056557673814597906,0.6799753076659553,0.6908040501681469,0.1092536050568012,0.7039900574311984,0.192393734760853,0.7351698074818683,0.4923110472438195,0.16267849350806157,0.7447648530874725,0.6962242500636692,0.7668625786647199,0.7461548170643358,0.7643821775305021,0.6605551690745088,0.10945840355992627,0.7814326363604599,0.784922054435193,0.8027423473243455,0.7491690762259118,0.07667964727779672,0.16844158065908355,0.6569051645043923,0.7534230525135056,0.7828388317944088,0.7694057974841851,0.7089990829526277,0.5079512447683701,0.7774132225994416,0.36158379433919263,0.7916892464057343,0.6560227671942386,0.7488056656100469,0.6581336697896849,0.7987004938675029,0.8010981861318826,0.7286361946913053,0.7931638534632942,0.7196553299056778,0.7558355573611538,0.7772445360873618,0.736446316505921,0.5536664123274877,0.7638190570026132,0.6630577408398245,0.790755231211498],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Best Value\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73],\"y\":[0.7455494904284854,0.7455494904284854,0.75931680276735,0.75931680276735,0.75931680276735,0.75931680276735,0.75931680276735,0.75931680276735,0.75931680276735,0.75931680276735,0.75931680276735,0.75931680276735,0.75931680276735,0.75931680276735,0.75931680276735,0.75931680276735,0.75931680276735,0.7668625786647199,0.7668625786647199,0.7668625786647199,0.7668625786647199,0.7668625786647199,0.7668625786647199,0.7814326363604599,0.784922054435193,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455,0.8027423473243455],\"type\":\"scatter\"},{\"marker\":{\"color\":\"#cccccc\"},\"mode\":\"markers\",\"name\":\"Infeasible Trial\",\"showlegend\":false,\"x\":[],\"y\":[],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Optimization History Plot\"},\"xaxis\":{\"title\":{\"text\":\"Trial\"}},\"yaxis\":{\"title\":{\"text\":\"Objective Value\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('53bdc0a0-0738-45c3-b910-185d07ff17a3');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"51bf65fc-784f-42d8-86d9-9f8046e67862\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"51bf65fc-784f-42d8-86d9-9f8046e67862\")) {                    Plotly.newPlot(                        \"51bf65fc-784f-42d8-86d9-9f8046e67862\",                        [{\"cliponaxis\":false,\"hovertemplate\":[\"max_seq_length (CategoricalDistribution): 0.009749943335929281\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"lambda_mixup (FloatDistribution): 0.03855233770234009\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"aug_p (FloatDistribution): 0.045230655593260866\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"weight_decay (FloatDistribution): 0.04742641235325317\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"projection_depth (IntDistribution): 0.07819291865694424\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"projection_size (CategoricalDistribution): 0.10660287773024968\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"lambda_bt (FloatDistribution): 0.1363195110661695\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"batch_size (CategoricalDistribution): 0.2008463301142025\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"learning_rate (FloatDistribution): 0.33707901344765057\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"],\"name\":\"Objective Value\",\"orientation\":\"h\",\"text\":[\"\\u003c0.01\",\"0.04\",\"0.05\",\"0.05\",\"0.08\",\"0.11\",\"0.14\",\"0.20\",\"0.34\"],\"textposition\":\"outside\",\"x\":[0.009749943335929281,0.03855233770234009,0.045230655593260866,0.04742641235325317,0.07819291865694424,0.10660287773024968,0.1363195110661695,0.2008463301142025,0.33707901344765057],\"y\":[\"max_seq_length\",\"lambda_mixup\",\"aug_p\",\"weight_decay\",\"projection_depth\",\"projection_size\",\"lambda_bt\",\"batch_size\",\"learning_rate\"],\"type\":\"bar\"}],                        {\"title\":{\"text\":\"Hyperparameter Importances\"},\"xaxis\":{\"title\":{\"text\":\"Hyperparameter Importance\"}},\"yaxis\":{\"title\":{\"text\":\"Hyperparameter\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('51bf65fc-784f-42d8-86d9-9f8046e67862');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"d01f33e2-aaed-4830-a3cc-a201781b4e44\" class=\"plotly-graph-div\" style=\"height:525px; width:2700px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d01f33e2-aaed-4830-a3cc-a201781b4e44\")) {                    Plotly.newPlot(                        \"d01f33e2-aaed-4830-a3cc-a201781b4e44\",                        [{\"marker\":{\"color\":[0,1,2,3,4,7,8,9,11,12,13,14,15,16,17,18,20,21,22,23,24,25,30,31,32,33,34,35,37,38,39,40,41,44,48,49,50,53,54,55,57,59,60,61,64,65,66,69,70],\"colorbar\":{\"title\":{\"text\":\"Trial\"},\"x\":1.0,\"xpad\":40},\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"line\":{\"color\":\"Grey\",\"width\":0.5},\"showscale\":true},\"mode\":\"markers\",\"name\":\"Feasible Trial\",\"showlegend\":false,\"x\":[0.32836617557863806,0.18934002266094807,0.2793421819216587,0.4314269825492332,0.19743400473673264,0.3964436059088924,0.2518856268039458,0.2996095654629599,0.1624758701816983,0.21092655550522177,0.10763918256089883,0.3555113612307246,0.4934718079582789,0.31201132752174465,0.28269887668552773,0.24897270623102452,0.27040377972415286,0.36719105058055235,0.2553065970874263,0.12305749170585525,0.10520871410871563,0.11568972876642647,0.14382020481980798,0.1311740305545105,0.17457817603275017,0.23026684023489613,0.11962885484013475,0.15670444058634436,0.16138140662971814,0.10056270619613124,0.14681554396333135,0.1815011812058979,0.21526079853110447,0.12277581036342065,0.19287363972903582,0.12844110495931757,0.1516088155293977,0.12048254774526967,0.12213086228207404,0.11818798849821871,0.10096620207217462,0.4365098618940274,0.1340592592010611,0.203268970263565,0.12306983187309883,0.10131874647153981,0.17501567961396192,0.1388673516107248,0.11795716226114125],\"y\":[0.7455494904284854,0.0236697649623803,0.75931680276735,0.056557673814597906,0.6799753076659553,0.6908040501681469,0.1092536050568012,0.7039900574311984,0.192393734760853,0.7351698074818683,0.4923110472438195,0.16267849350806157,0.7447648530874725,0.6962242500636692,0.7668625786647199,0.7461548170643358,0.7643821775305021,0.6605551690745088,0.10945840355992627,0.7814326363604599,0.784922054435193,0.8027423473243455,0.7491690762259118,0.07667964727779672,0.16844158065908355,0.6569051645043923,0.7534230525135056,0.7828388317944088,0.7694057974841851,0.7089990829526277,0.5079512447683701,0.7774132225994416,0.36158379433919263,0.7916892464057343,0.6560227671942386,0.7488056656100469,0.6581336697896849,0.7987004938675029,0.8010981861318826,0.7286361946913053,0.7931638534632942,0.7196553299056778,0.7558355573611538,0.7772445360873618,0.736446316505921,0.5536664123274877,0.7638190570026132,0.6630577408398245,0.790755231211498],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"marker\":{\"color\":[0,1,2,3,4,7,8,9,11,12,13,14,15,16,17,18,20,21,22,23,24,25,30,31,32,33,34,35,37,38,39,40,41,44,48,49,50,53,54,55,57,59,60,61,64,65,66,69,70],\"colorbar\":{\"title\":{\"text\":\"Trial\"},\"x\":1.0,\"xpad\":40},\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"line\":{\"color\":\"Grey\",\"width\":0.5},\"showscale\":false},\"mode\":\"markers\",\"name\":\"Feasible Trial\",\"showlegend\":false,\"x\":[128,64,256,128,64,512,64,128,256,64,256,128,256,512,128,256,128,128,128,128,128,128,512,128,128,128,128,128,512,128,128,128,128,64,64,64,64,128,128,64,128,512,256,128,64,256,128,128,128],\"y\":[0.7455494904284854,0.0236697649623803,0.75931680276735,0.056557673814597906,0.6799753076659553,0.6908040501681469,0.1092536050568012,0.7039900574311984,0.192393734760853,0.7351698074818683,0.4923110472438195,0.16267849350806157,0.7447648530874725,0.6962242500636692,0.7668625786647199,0.7461548170643358,0.7643821775305021,0.6605551690745088,0.10945840355992627,0.7814326363604599,0.784922054435193,0.8027423473243455,0.7491690762259118,0.07667964727779672,0.16844158065908355,0.6569051645043923,0.7534230525135056,0.7828388317944088,0.7694057974841851,0.7089990829526277,0.5079512447683701,0.7774132225994416,0.36158379433919263,0.7916892464057343,0.6560227671942386,0.7488056656100469,0.6581336697896849,0.7987004938675029,0.8010981861318826,0.7286361946913053,0.7931638534632942,0.7196553299056778,0.7558355573611538,0.7772445360873618,0.736446316505921,0.5536664123274877,0.7638190570026132,0.6630577408398245,0.790755231211498],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"marker\":{\"color\":[0,1,2,3,4,7,8,9,11,12,13,14,15,16,17,18,20,21,22,23,24,25,30,31,32,33,34,35,37,38,39,40,41,44,48,49,50,53,54,55,57,59,60,61,64,65,66,69,70],\"colorbar\":{\"title\":{\"text\":\"Trial\"},\"x\":1.0,\"xpad\":40},\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"line\":{\"color\":\"Grey\",\"width\":0.5},\"showscale\":false},\"mode\":\"markers\",\"name\":\"Feasible Trial\",\"showlegend\":false,\"x\":[0.17469119235167874,0.4766952828340136,0.3247568842647391,0.24357307552174878,0.023384253784940447,0.03144193253409849,0.0784035854406689,0.059424270659944906,0.027765193758599097,0.21035352977040597,0.39105977294935423,0.33878400988398816,0.15889723451074197,0.31942863672743266,0.15361544071274083,0.13007430523779517,0.29821085524343,0.24863741158912114,0.3990391091117084,0.11032469736138698,0.11447932554932835,0.1117488685923089,0.19957411250198914,0.06983875599860229,0.1354771243321331,0.20203173442685823,0.10535476045890865,0.17673735503811983,0.16845367753711332,0.003139987954997203,0.13648563766986505,0.18620584738337137,0.4964290798874085,0.05576323846510517,0.23059343620570388,0.045403708820159344,0.07960305475568735,0.1127695760193322,0.09041263880624094,0.09034491352583367,0.03871979140700997,0.038403609862258554,0.0200488530531632,0.052575808978445826,0.08219587911885834,0.28175348971093606,0.025178503539676295,0.14144214061516924,0.06324899336612168],\"y\":[0.7455494904284854,0.0236697649623803,0.75931680276735,0.056557673814597906,0.6799753076659553,0.6908040501681469,0.1092536050568012,0.7039900574311984,0.192393734760853,0.7351698074818683,0.4923110472438195,0.16267849350806157,0.7447648530874725,0.6962242500636692,0.7668625786647199,0.7461548170643358,0.7643821775305021,0.6605551690745088,0.10945840355992627,0.7814326363604599,0.784922054435193,0.8027423473243455,0.7491690762259118,0.07667964727779672,0.16844158065908355,0.6569051645043923,0.7534230525135056,0.7828388317944088,0.7694057974841851,0.7089990829526277,0.5079512447683701,0.7774132225994416,0.36158379433919263,0.7916892464057343,0.6560227671942386,0.7488056656100469,0.6581336697896849,0.7987004938675029,0.8010981861318826,0.7286361946913053,0.7931638534632942,0.7196553299056778,0.7558355573611538,0.7772445360873618,0.736446316505921,0.5536664123274877,0.7638190570026132,0.6630577408398245,0.790755231211498],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"marker\":{\"color\":[0,1,2,3,4,7,8,9,11,12,13,14,15,16,17,18,20,21,22,23,24,25,30,31,32,33,34,35,37,38,39,40,41,44,48,49,50,53,54,55,57,59,60,61,64,65,66,69,70],\"colorbar\":{\"title\":{\"text\":\"Trial\"},\"x\":1.0,\"xpad\":40},\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"line\":{\"color\":\"Grey\",\"width\":0.5},\"showscale\":false},\"mode\":\"markers\",\"name\":\"Feasible Trial\",\"showlegend\":false,\"x\":[0.6719274052857119,1.4458171065926169,0.49455848783627376,0.5151843594789187,1.2825865790117879,0.7650377580366385,0.16797170561806687,1.0134700521813407,1.1809338217911947,1.3396053152940643,0.2901711245607147,0.66206483869652,0.4493608783258739,0.9475401309701692,0.5612372224451203,0.42994863968189,0.1186646811087288,0.146284072182446,0.30933659630764665,0.10263987447561014,0.9214649018336508,0.9545383539595996,0.8291002050587908,0.8481946724783502,1.1390561838951796,0.8856918254529056,0.6442109748264517,0.7375005330552058,0.7523768024928212,1.0618285322801952,0.7357966029758329,1.474639788540796,1.2248576047850164,0.909416683446068,0.9545235329507995,0.6971026671188281,1.3725832323404186,1.0943654352223153,1.1049341678838982,1.1331664469745517,1.0349156222570985,1.0420528346994558,1.245025133612623,1.003451844332601,1.1069527924727738,1.305669612443545,1.1893522426728333,1.0031603082098324,0.8013153459674038],\"y\":[0.7455494904284854,0.0236697649623803,0.75931680276735,0.056557673814597906,0.6799753076659553,0.6908040501681469,0.1092536050568012,0.7039900574311984,0.192393734760853,0.7351698074818683,0.4923110472438195,0.16267849350806157,0.7447648530874725,0.6962242500636692,0.7668625786647199,0.7461548170643358,0.7643821775305021,0.6605551690745088,0.10945840355992627,0.7814326363604599,0.784922054435193,0.8027423473243455,0.7491690762259118,0.07667964727779672,0.16844158065908355,0.6569051645043923,0.7534230525135056,0.7828388317944088,0.7694057974841851,0.7089990829526277,0.5079512447683701,0.7774132225994416,0.36158379433919263,0.7916892464057343,0.6560227671942386,0.7488056656100469,0.6581336697896849,0.7987004938675029,0.8010981861318826,0.7286361946913053,0.7931638534632942,0.7196553299056778,0.7558355573611538,0.7772445360873618,0.736446316505921,0.5536664123274877,0.7638190570026132,0.6630577408398245,0.790755231211498],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"marker\":{\"color\":[0,1,2,3,4,7,8,9,11,12,13,14,15,16,17,18,20,21,22,23,24,25,30,31,32,33,34,35,37,38,39,40,41,44,48,49,50,53,54,55,57,59,60,61,64,65,66,69,70],\"colorbar\":{\"title\":{\"text\":\"Trial\"},\"x\":1.0,\"xpad\":40},\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"line\":{\"color\":\"Grey\",\"width\":0.5},\"showscale\":false},\"mode\":\"markers\",\"name\":\"Feasible Trial\",\"showlegend\":false,\"x\":[0.000118241345051841,0.0006938147510973437,0.00002830912757130487,0.0007137696733253514,0.000021477425311834218,0.000014759844175109111,0.00011365963437353935,0.00007499611387311669,0.00023128661754804076,0.00001712874532455072,0.00004023360400756036,0.00019621154502011724,0.00004481252263857261,0.000283438167342585,0.0000450655598418093,0.000036331676674233134,0.000025562991368277428,0.000010293434708319737,0.00008595937287586316,0.00005735669344903684,0.00004627369350353354,0.000057605107036319744,0.00016995491661900194,0.0004426540274723203,0.00014057771448159194,0.00005585420413769078,0.00008696778864435523,0.00003270690088119771,0.000030853551204548494,0.000054251973450421175,0.00010504875984876097,0.000023559338943215423,0.000032669404067442275,0.00001951901765118055,0.000010170294560518593,0.000020698810107396578,0.000012424387629958407,0.000029191826368841383,0.000028526972536607186,0.000019286916739200656,0.000024809745771416558,0.00002663060955721946,0.00001432598337425839,0.000022289669572799393,0.00003931299419969663,0.000026549237263751275,0.000014176896264968205,0.00001721669535526003,0.00003778615771373759],\"y\":[0.7455494904284854,0.0236697649623803,0.75931680276735,0.056557673814597906,0.6799753076659553,0.6908040501681469,0.1092536050568012,0.7039900574311984,0.192393734760853,0.7351698074818683,0.4923110472438195,0.16267849350806157,0.7447648530874725,0.6962242500636692,0.7668625786647199,0.7461548170643358,0.7643821775305021,0.6605551690745088,0.10945840355992627,0.7814326363604599,0.784922054435193,0.8027423473243455,0.7491690762259118,0.07667964727779672,0.16844158065908355,0.6569051645043923,0.7534230525135056,0.7828388317944088,0.7694057974841851,0.7089990829526277,0.5079512447683701,0.7774132225994416,0.36158379433919263,0.7916892464057343,0.6560227671942386,0.7488056656100469,0.6581336697896849,0.7987004938675029,0.8010981861318826,0.7286361946913053,0.7931638534632942,0.7196553299056778,0.7558355573611538,0.7772445360873618,0.736446316505921,0.5536664123274877,0.7638190570026132,0.6630577408398245,0.790755231211498],\"type\":\"scatter\",\"xaxis\":\"x5\",\"yaxis\":\"y5\"},{\"marker\":{\"color\":[0,1,2,3,4,7,8,9,11,12,13,14,15,16,17,18,20,21,22,23,24,25,30,31,32,33,34,35,37,38,39,40,41,44,48,49,50,53,54,55,57,59,60,61,64,65,66,69,70],\"colorbar\":{\"title\":{\"text\":\"Trial\"},\"x\":1.0,\"xpad\":40},\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"line\":{\"color\":\"Grey\",\"width\":0.5},\"showscale\":false},\"mode\":\"markers\",\"name\":\"Feasible Trial\",\"showlegend\":false,\"x\":[64,75,32,64,64,64,64,32,64,64,32,32,75,32,32,32,32,32,32,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,75,64,75,75,64,75,75,75],\"y\":[0.7455494904284854,0.0236697649623803,0.75931680276735,0.056557673814597906,0.6799753076659553,0.6908040501681469,0.1092536050568012,0.7039900574311984,0.192393734760853,0.7351698074818683,0.4923110472438195,0.16267849350806157,0.7447648530874725,0.6962242500636692,0.7668625786647199,0.7461548170643358,0.7643821775305021,0.6605551690745088,0.10945840355992627,0.7814326363604599,0.784922054435193,0.8027423473243455,0.7491690762259118,0.07667964727779672,0.16844158065908355,0.6569051645043923,0.7534230525135056,0.7828388317944088,0.7694057974841851,0.7089990829526277,0.5079512447683701,0.7774132225994416,0.36158379433919263,0.7916892464057343,0.6560227671942386,0.7488056656100469,0.6581336697896849,0.7987004938675029,0.8010981861318826,0.7286361946913053,0.7931638534632942,0.7196553299056778,0.7558355573611538,0.7772445360873618,0.736446316505921,0.5536664123274877,0.7638190570026132,0.6630577408398245,0.790755231211498],\"type\":\"scatter\",\"xaxis\":\"x6\",\"yaxis\":\"y6\"},{\"marker\":{\"color\":[0,1,2,3,4,7,8,9,11,12,13,14,15,16,17,18,20,21,22,23,24,25,30,31,32,33,34,35,37,38,39,40,41,44,48,49,50,53,54,55,57,59,60,61,64,65,66,69,70],\"colorbar\":{\"title\":{\"text\":\"Trial\"},\"x\":1.0,\"xpad\":40},\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"line\":{\"color\":\"Grey\",\"width\":0.5},\"showscale\":false},\"mode\":\"markers\",\"name\":\"Feasible Trial\",\"showlegend\":false,\"x\":[3,4,4,5,6,6,6,3,6,5,2,3,3,4,2,2,4,2,5,2,2,2,3,2,3,2,3,3,3,2,2,3,2,2,3,4,2,2,2,2,2,2,2,2,5,2,2,2,2],\"y\":[0.7455494904284854,0.0236697649623803,0.75931680276735,0.056557673814597906,0.6799753076659553,0.6908040501681469,0.1092536050568012,0.7039900574311984,0.192393734760853,0.7351698074818683,0.4923110472438195,0.16267849350806157,0.7447648530874725,0.6962242500636692,0.7668625786647199,0.7461548170643358,0.7643821775305021,0.6605551690745088,0.10945840355992627,0.7814326363604599,0.784922054435193,0.8027423473243455,0.7491690762259118,0.07667964727779672,0.16844158065908355,0.6569051645043923,0.7534230525135056,0.7828388317944088,0.7694057974841851,0.7089990829526277,0.5079512447683701,0.7774132225994416,0.36158379433919263,0.7916892464057343,0.6560227671942386,0.7488056656100469,0.6581336697896849,0.7987004938675029,0.8010981861318826,0.7286361946913053,0.7931638534632942,0.7196553299056778,0.7558355573611538,0.7772445360873618,0.736446316505921,0.5536664123274877,0.7638190570026132,0.6630577408398245,0.790755231211498],\"type\":\"scatter\",\"xaxis\":\"x7\",\"yaxis\":\"y7\"},{\"marker\":{\"color\":[0,1,2,3,4,7,8,9,11,12,13,14,15,16,17,18,20,21,22,23,24,25,30,31,32,33,34,35,37,38,39,40,41,44,48,49,50,53,54,55,57,59,60,61,64,65,66,69,70],\"colorbar\":{\"title\":{\"text\":\"Trial\"},\"x\":1.0,\"xpad\":40},\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"line\":{\"color\":\"Grey\",\"width\":0.5},\"showscale\":false},\"mode\":\"markers\",\"name\":\"Feasible Trial\",\"showlegend\":false,\"x\":[2048,8192,4096,8192,2048,4096,6144,8192,6144,2048,4096,2048,4096,2048,4096,4096,4096,4096,4096,4096,6144,6144,6144,6144,6144,6144,6144,6144,6144,6144,6144,8192,6144,8192,8192,8192,8192,8192,8192,8192,8192,8192,8192,8192,8192,8192,8192,8192,8192],\"y\":[0.7455494904284854,0.0236697649623803,0.75931680276735,0.056557673814597906,0.6799753076659553,0.6908040501681469,0.1092536050568012,0.7039900574311984,0.192393734760853,0.7351698074818683,0.4923110472438195,0.16267849350806157,0.7447648530874725,0.6962242500636692,0.7668625786647199,0.7461548170643358,0.7643821775305021,0.6605551690745088,0.10945840355992627,0.7814326363604599,0.784922054435193,0.8027423473243455,0.7491690762259118,0.07667964727779672,0.16844158065908355,0.6569051645043923,0.7534230525135056,0.7828388317944088,0.7694057974841851,0.7089990829526277,0.5079512447683701,0.7774132225994416,0.36158379433919263,0.7916892464057343,0.6560227671942386,0.7488056656100469,0.6581336697896849,0.7987004938675029,0.8010981861318826,0.7286361946913053,0.7931638534632942,0.7196553299056778,0.7558355573611538,0.7772445360873618,0.736446316505921,0.5536664123274877,0.7638190570026132,0.6630577408398245,0.790755231211498],\"type\":\"scatter\",\"xaxis\":\"x8\",\"yaxis\":\"y8\"},{\"marker\":{\"color\":[0,1,2,3,4,7,8,9,11,12,13,14,15,16,17,18,20,21,22,23,24,25,30,31,32,33,34,35,37,38,39,40,41,44,48,49,50,53,54,55,57,59,60,61,64,65,66,69,70],\"colorbar\":{\"title\":{\"text\":\"Trial\"},\"x\":1.0,\"xpad\":40},\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"line\":{\"color\":\"Grey\",\"width\":0.5},\"showscale\":false},\"mode\":\"markers\",\"name\":\"Feasible Trial\",\"showlegend\":false,\"x\":[0.04673345358786073,0.3228749477937434,0.40615817018750844,0.07864135004523762,0.12048029910093874,0.43326810810620187,0.2696917352786458,0.30495548264096306,0.1517167237888657,0.2857313874326568,0.47691958700506265,0.0046191652794669025,0.3793734316474784,0.18616226012729092,0.20533553705686064,0.22120736838522648,0.37669508081841613,0.355702219139719,0.232163909599783,0.48556899562406813,0.19208179728106003,0.17805243663726578,0.49667671304184025,0.10486892766323486,0.24548440227511636,0.33007779640931123,0.18329676626253405,0.06577631965756808,0.04628105380238097,0.057280998636037306,0.09775762452330372,0.03180702523731799,0.1257400444139697,0.08002977534112553,0.0011299776842993714,0.1574696453955616,0.09926651768919303,0.13369507548458562,0.13064501485955834,0.12172242303923879,0.14523496298318028,0.1468978207010723,0.16434064556854105,0.20657965552778051,0.26273647775961906,0.1349197497240108,0.033461363812990294,0.10836954272794108,0.22326498299364642],\"y\":[0.7455494904284854,0.0236697649623803,0.75931680276735,0.056557673814597906,0.6799753076659553,0.6908040501681469,0.1092536050568012,0.7039900574311984,0.192393734760853,0.7351698074818683,0.4923110472438195,0.16267849350806157,0.7447648530874725,0.6962242500636692,0.7668625786647199,0.7461548170643358,0.7643821775305021,0.6605551690745088,0.10945840355992627,0.7814326363604599,0.784922054435193,0.8027423473243455,0.7491690762259118,0.07667964727779672,0.16844158065908355,0.6569051645043923,0.7534230525135056,0.7828388317944088,0.7694057974841851,0.7089990829526277,0.5079512447683701,0.7774132225994416,0.36158379433919263,0.7916892464057343,0.6560227671942386,0.7488056656100469,0.6581336697896849,0.7987004938675029,0.8010981861318826,0.7286361946913053,0.7931638534632942,0.7196553299056778,0.7558355573611538,0.7772445360873618,0.736446316505921,0.5536664123274877,0.7638190570026132,0.6630577408398245,0.790755231211498],\"type\":\"scatter\",\"xaxis\":\"x9\",\"yaxis\":\"y9\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.09135802469135802],\"title\":{\"text\":\"aug_p\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Objective Value\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.11358024691358025,0.20493827160493827],\"title\":{\"text\":\"batch_size\"},\"type\":\"category\",\"categoryorder\":\"array\",\"categoryarray\":[64,128,256,512]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"matches\":\"y\",\"showticklabels\":false},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.2271604938271605,0.31851851851851853],\"title\":{\"text\":\"lambda_bt\"}},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,1.0],\"matches\":\"y\",\"showticklabels\":false},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.34074074074074073,0.43209876543209874],\"title\":{\"text\":\"lambda_mixup\"}},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.0,1.0],\"matches\":\"y\",\"showticklabels\":false},\"xaxis5\":{\"anchor\":\"y5\",\"domain\":[0.454320987654321,0.5456790123456791],\"title\":{\"text\":\"learning_rate\"},\"type\":\"log\"},\"yaxis5\":{\"anchor\":\"x5\",\"domain\":[0.0,1.0],\"matches\":\"y\",\"showticklabels\":false},\"xaxis6\":{\"anchor\":\"y6\",\"domain\":[0.5679012345679012,0.6592592592592592],\"title\":{\"text\":\"max_seq_length\"},\"type\":\"category\",\"categoryorder\":\"array\",\"categoryarray\":[32,64,75]},\"yaxis6\":{\"anchor\":\"x6\",\"domain\":[0.0,1.0],\"matches\":\"y\",\"showticklabels\":false},\"xaxis7\":{\"anchor\":\"y7\",\"domain\":[0.6814814814814815,0.7728395061728395],\"title\":{\"text\":\"projection_depth\"}},\"yaxis7\":{\"anchor\":\"x7\",\"domain\":[0.0,1.0],\"matches\":\"y\",\"showticklabels\":false},\"xaxis8\":{\"anchor\":\"y8\",\"domain\":[0.7950617283950617,0.8864197530864197],\"title\":{\"text\":\"projection_size\"},\"type\":\"category\",\"categoryorder\":\"array\",\"categoryarray\":[2048,4096,6144,8192]},\"yaxis8\":{\"anchor\":\"x8\",\"domain\":[0.0,1.0],\"matches\":\"y\",\"showticklabels\":false},\"xaxis9\":{\"anchor\":\"y9\",\"domain\":[0.908641975308642,1.0],\"title\":{\"text\":\"weight_decay\"}},\"yaxis9\":{\"anchor\":\"x9\",\"domain\":[0.0,1.0],\"matches\":\"y\",\"showticklabels\":false},\"title\":{\"text\":\"Slice Plot\"},\"width\":2700},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d01f33e2-aaed-4830-a3cc-a201781b4e44');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# After optimization, use Optuna's visualization tools:\n",
        "import optuna.visualization as vis\n",
        "\n",
        "# Plot the optimization history\n",
        "opt_history_fig = vis.plot_optimization_history(study)\n",
        "opt_history_fig.show()\n",
        "\n",
        "# Plot hyperparameter importances\n",
        "opt_param_fig = vis.plot_param_importances(study)\n",
        "opt_param_fig.show()\n",
        "\n",
        "# Plot slices for selected hyperparameters\n",
        "opt_slice_fig = vis.plot_slice(study)\n",
        "opt_slice_fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83nPcSvGqBNk"
      },
      "source": [
        "## Mixed Barlow Twins with BERT-Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UxjxbGvEqKD9",
        "outputId": "4b354786-09c3-41a8-a6ab-7969bb287e5b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-13 14:33:20,494] Using an existing study with name 'llm_finetuning_study' instead of creating a new one.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-e84e7b3b277f>:478: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "<ipython-input-4-e84e7b3b277f>:479: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-4-e84e7b3b277f>:482: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "<ipython-input-4-e84e7b3b277f>:483: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "<ipython-input-4-e84e7b3b277f>:484: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"0542bbad-83b3-477f-9a5b-267a54495695\" class=\"plotly-graph-div\" style=\"height:800px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"0542bbad-83b3-477f-9a5b-267a54495695\")) {                    Plotly.newPlot(                        \"0542bbad-83b3-477f-9a5b-267a54495695\",                        [{\"mode\":\"lines+markers\",\"name\":\"Loss\",\"x\":[],\"y\":[],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"lines+markers\",\"name\":\"Mean Gradient Norm\",\"x\":[],\"y\":[],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"mode\":\"lines+markers\",\"name\":\"Variance\",\"x\":[],\"y\":[],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"mode\":\"lines+markers\",\"name\":\"Learning Rate\",\"x\":[],\"y\":[],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"mode\":\"lines+markers\",\"name\":\"Dev STS Pearson Cosine\",\"x\":[],\"y\":[],\"type\":\"scatter\",\"xaxis\":\"x5\",\"yaxis\":\"y5\"},{\"mode\":\"lines+markers\",\"name\":\"Dev STS Spearman Cosine\",\"x\":[],\"y\":[],\"type\":\"scatter\",\"xaxis\":\"x5\",\"yaxis\":\"y5\"},{\"mode\":\"lines+markers\",\"name\":\"Test STS Pearson Cosine\",\"x\":[],\"y\":[],\"type\":\"scatter\",\"xaxis\":\"x6\",\"yaxis\":\"y6\"},{\"mode\":\"lines+markers\",\"name\":\"Test STS Spearman Cosine\",\"x\":[],\"y\":[],\"type\":\"scatter\",\"xaxis\":\"x6\",\"yaxis\":\"y6\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.7777777777777778,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.7777777777777778,1.0]},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,0.45]},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.3888888888888889,0.6111111111111112]},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.55,1.0]},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.3888888888888889,0.6111111111111112]},\"xaxis5\":{\"anchor\":\"y5\",\"domain\":[0.0,0.45]},\"yaxis5\":{\"anchor\":\"x5\",\"domain\":[0.0,0.22222222222222224]},\"xaxis6\":{\"anchor\":\"y6\",\"domain\":[0.55,1.0]},\"yaxis6\":{\"anchor\":\"x6\",\"domain\":[0.0,0.22222222222222224]},\"annotations\":[{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=bert-base-uncased, batch_size=128, projection_depth=3, projection_size=8192, epochs=1, warmup_proportion=0.0, max_seq_length=75,\\u003cbr\\u003eaug_p=0.16539755977310927, learning_rate=4.837592944784042e-05, model_save_path=\\u002fcontent\\u002fdrive\\u002fMyDrive\\u002fviolet_bert_base_checkpoints_2\\u002ftrain_stsb_bt-\\u003cbr\\u003edistilbert-2025-04-13_14-33-20_trial38, num_workers=10, weight_decay=0.05398982009405306, lambda_bt=0.06954297619664421, lambda_mixup=1.16868068523566,\\u003cbr\\u003euse_amp=True, patience=1000, Augmenters: Synonym_Aug[substitute:0.16539755977310927], RandomWord_Aug[swap:0.16539755977310927],\\u003cbr\\u003eRandomWord_Aug[delete:0.16539755977310927]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=bert-base-uncased, batch_size=128, projection_depth=3, projection_size=8192, epochs=1, warmup_proportion=0.0, max_seq_length=75,\\u003cbr\\u003eaug_p=0.16539755977310927, learning_rate=4.837592944784042e-05, model_save_path=\\u002fcontent\\u002fdrive\\u002fMyDrive\\u002fviolet_bert_base_checkpoints_2\\u002ftrain_stsb_bt-\\u003cbr\\u003edistilbert-2025-04-13_14-33-20_trial38, num_workers=10, weight_decay=0.05398982009405306, lambda_bt=0.06954297619664421, lambda_mixup=1.16868068523566,\\u003cbr\\u003euse_amp=True, patience=1000, Augmenters: Synonym_Aug[substitute:0.16539755977310927], RandomWord_Aug[swap:0.16539755977310927],\\u003cbr\\u003eRandomWord_Aug[delete:0.16539755977310927]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=bert-base-uncased, batch_size=128, projection_depth=3, projection_size=8192, epochs=1, warmup_proportion=0.0, max_seq_length=75,\\u003cbr\\u003eaug_p=0.16539755977310927, learning_rate=4.837592944784042e-05, model_save_path=\\u002fcontent\\u002fdrive\\u002fMyDrive\\u002fviolet_bert_base_checkpoints_2\\u002ftrain_stsb_bt-\\u003cbr\\u003edistilbert-2025-04-13_14-33-20_trial38, num_workers=10, weight_decay=0.05398982009405306, lambda_bt=0.06954297619664421, lambda_mixup=1.16868068523566,\\u003cbr\\u003euse_amp=True, patience=1000, Augmenters: Synonym_Aug[substitute:0.16539755977310927], RandomWord_Aug[swap:0.16539755977310927],\\u003cbr\\u003eRandomWord_Aug[delete:0.16539755977310927]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=bert-base-uncased, batch_size=128, projection_depth=3, projection_size=8192, epochs=1, warmup_proportion=0.0, max_seq_length=75,\\u003cbr\\u003eaug_p=0.16539755977310927, learning_rate=4.837592944784042e-05, model_save_path=\\u002fcontent\\u002fdrive\\u002fMyDrive\\u002fviolet_bert_base_checkpoints_2\\u002ftrain_stsb_bt-\\u003cbr\\u003edistilbert-2025-04-13_14-33-20_trial38, num_workers=10, weight_decay=0.05398982009405306, lambda_bt=0.06954297619664421, lambda_mixup=1.16868068523566,\\u003cbr\\u003euse_amp=True, patience=1000, Augmenters: Synonym_Aug[substitute:0.16539755977310927], RandomWord_Aug[swap:0.16539755977310927],\\u003cbr\\u003eRandomWord_Aug[delete:0.16539755977310927]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=bert-base-uncased, batch_size=128, projection_depth=3, projection_size=8192, epochs=1, warmup_proportion=0.0, max_seq_length=75,\\u003cbr\\u003eaug_p=0.16539755977310927, learning_rate=4.837592944784042e-05, model_save_path=\\u002fcontent\\u002fdrive\\u002fMyDrive\\u002fviolet_bert_base_checkpoints_2\\u002ftrain_stsb_bt-\\u003cbr\\u003edistilbert-2025-04-13_14-33-20_trial38, num_workers=10, weight_decay=0.05398982009405306, lambda_bt=0.06954297619664421, lambda_mixup=1.16868068523566,\\u003cbr\\u003euse_amp=True, patience=1000, Augmenters: Synonym_Aug[substitute:0.16539755977310927], RandomWord_Aug[swap:0.16539755977310927],\\u003cbr\\u003eRandomWord_Aug[delete:0.16539755977310927]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=bert-base-uncased, batch_size=128, projection_depth=3, projection_size=8192, epochs=1, warmup_proportion=0.0, max_seq_length=75,\\u003cbr\\u003eaug_p=0.16539755977310927, learning_rate=4.837592944784042e-05, model_save_path=\\u002fcontent\\u002fdrive\\u002fMyDrive\\u002fviolet_bert_base_checkpoints_2\\u002ftrain_stsb_bt-\\u003cbr\\u003edistilbert-2025-04-13_14-33-20_trial38, num_workers=10, weight_decay=0.05398982009405306, lambda_bt=0.06954297619664421, lambda_mixup=1.16868068523566,\\u003cbr\\u003euse_amp=True, patience=1000, Augmenters: Synonym_Aug[substitute:0.16539755977310927], RandomWord_Aug[swap:0.16539755977310927],\\u003cbr\\u003eRandomWord_Aug[delete:0.16539755977310927]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"right\",\"font\":{\"color\":\"blue\",\"size\":12},\"showarrow\":false,\"text\":\"Best Spearman (Test): N\\u002fA\\u003cbr\\u003eBest Pearson (Test): N\\u002fA\\u003cbr\\u003eBest Spearman (Val): N\\u002fA\\u003cbr\\u003eBest Pearson (Val): N\\u002fA\",\"x\":1.0,\"xanchor\":\"right\",\"xref\":\"paper\",\"y\":0.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Training Metrics\"},\"margin\":{\"l\":50,\"r\":50,\"t\":100,\"b\":150},\"width\":1200,\"height\":800,\"showlegend\":true},                        {\"toImageButtonOptions\": {\"filename\": \"/content/drive/MyDrive/violet_bert_base_checkpoints_2/train_stsb_bt-distilbert-2025-04-13_14-33-20_trial38\", \"format\": \"png\", \"width\": 1200, \"height\": 800, \"scale\": 1}, \"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('0542bbad-83b3-477f-9a5b-267a54495695');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/1::   0%|          | 0/7701 [00:00<?, ?it/s]\n",
            "[W 2025-04-13 14:33:36,210] Trial 38 failed with parameters: {'batch_size': 128, 'projection_depth': 3, 'projection_size': 8192, 'aug_p': 0.16539755977310927, 'learning_rate': 4.837592944784042e-05, 'weight_decay': 0.05398982009405306, 'lambda_bt': 0.06954297619664421, 'lambda_mixup': 1.16868068523566} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 15.38 MiB is free. Process 10775 has 22.14 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 114.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)').\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-4-e84e7b3b277f>\", line 496, in objective\n",
            "    trainer.fit()\n",
            "  File \"<ipython-input-4-e84e7b3b277f>\", line 351, in fit\n",
            "    z_b = self._forward_pass(self.online_model, s2, train=True)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-4-e84e7b3b277f>\", line 297, in _forward_pass\n",
            "    embeddings = model[0](features)[\"token_embeddings\"]\n",
            "                 ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sentence_transformers/models/Transformer.py\", line 442, in forward\n",
            "    output_states = self.auto_model(**trans_features, **kwargs, return_dict=False)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1142, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "                      ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 695, in forward\n",
            "    layer_outputs = layer_module(\n",
            "                    ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 627, in forward\n",
            "    layer_output = apply_chunking_to_forward(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\", line 254, in apply_chunking_to_forward\n",
            "    return forward_fn(*input_tensors)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 639, in feed_forward_chunk\n",
            "    intermediate_output = self.intermediate(attention_output)\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 540, in forward\n",
            "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/activations.py\", line 78, in forward\n",
            "    return self.act(input)\n",
            "           ^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 15.38 MiB is free. Process 10775 has 22.14 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 114.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "[W 2025-04-13 14:33:36,213] Trial 38 failed with value None.\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 15.38 MiB is free. Process 10775 has 22.14 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 114.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e84e7b3b277f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0mload_if_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m )\n\u001b[0;32m--> 516\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-e84e7b3b277f>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-e84e7b3b277f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_amp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                     \u001b[0mz_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                     \u001b[0mz_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mixed_barlow_twins_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-e84e7b3b277f>\u001b[0m in \u001b[0;36m_forward_pass\u001b[0;34m(self, model, sentences, train)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_amp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_embeddings\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0mpooled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"token_embeddings\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence_embedding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m         }\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0moutput_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrans_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    693\u001b[0m                 )\n\u001b[1;32m    694\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    696\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    628\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/activations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 15.38 MiB is free. Process 10775 has 22.14 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 114.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "import os\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from sentence_transformers import SentenceTransformer, models, util\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "import copy\n",
        "import gc\n",
        "import math\n",
        "import random\n",
        "import nlpaug.augmenter.word as naw\n",
        "from IPython.display import display, clear_output\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "from torch import GradScaler\n",
        "from torch.amp import autocast\n",
        "import optuna\n",
        "\n",
        "class BarlowTwinsNCSE:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        torch.backends.cudnn.benchmark = True  # Enable cuDNN benchmarking\n",
        "        self._prepare_datasets()\n",
        "        self._initialize_models()\n",
        "        self._initialize_optimizer_scheduler()\n",
        "        self.scaler = GradScaler(\"cuda\", enabled=self.config.get(\"use_amp\", True))  # GradScaler for AMP\n",
        "        self.best_spearman = -float(\"inf\")\n",
        "        self.best_pearson = -float(\"inf\")\n",
        "        self.patience_counter = 0\n",
        "        self.augmenters = [\n",
        "            naw.SynonymAug(aug_src='wordnet', aug_p=self.config[\"aug_p\"]),\n",
        "            naw.RandomWordAug(action=\"swap\", aug_p=self.config[\"aug_p\"]),\n",
        "            naw.RandomWordAug(aug_p=self.config[\"aug_p\"]),\n",
        "        ]\n",
        "        self.test_sts_pearson_cosine_values = []\n",
        "        self.test_sts_spearman_cosine_values = []\n",
        "        self.test_iterations = []\n",
        "        self._create_plot()\n",
        "\n",
        "    def _create_plot(self):\n",
        "        self.loss_values = []\n",
        "        self.sts_pearson_cosine_values = []\n",
        "        self.sts_spearman_cosine_values = []\n",
        "        self.mean_grad_norm_values = []\n",
        "        self.variance_values = []\n",
        "        self.learning_rate_values = []\n",
        "        self.iterations = []\n",
        "        self.epochs = []\n",
        "\n",
        "        # 3 rows x 2 columns grid with 6 subplots\n",
        "        self.fig = make_subplots(\n",
        "            rows=3, cols=2,\n",
        "            subplot_titles=(\n",
        "                \"Loss vs Iterations\",\n",
        "                \"Mean Gradient Norm vs Iterations\",\n",
        "                \"Variance vs Iterations\",\n",
        "                \"Learning Rate vs Iterations\",\n",
        "                \"Dev STS Cosine (Pearson & Spearman) vs Iterations\",\n",
        "                \"Test STS Cosine (Pearson & Spearman) vs Iterations\"\n",
        "            )\n",
        "        )\n",
        "        # Dev metrics\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Loss'), row=1, col=1)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Mean Gradient Norm'), row=1, col=2)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Variance'), row=2, col=1)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Learning Rate'), row=2, col=2)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Dev STS Pearson Cosine'), row=3, col=1)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Dev STS Spearman Cosine'), row=3, col=1)\n",
        "        # Test metrics\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Test STS Pearson Cosine'), row=3, col=2)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Test STS Spearman Cosine'), row=3, col=2)\n",
        "\n",
        "        # Prepare footer text\n",
        "        footer_text = \", \".join([f\"{key}={value}\" for key, value in self.config.items()])\n",
        "        augmenters_text = \", Augmenters: \" + \", \".join([f\"{aug.name}[{aug.action}:{aug.aug_p}]\" for aug in self.augmenters])\n",
        "        footer_text += augmenters_text\n",
        "        wrapped_footer = \"<br>\".join(textwrap.wrap(footer_text, width=160))\n",
        "\n",
        "        # Configure the download button\n",
        "        self.plot_config = {\n",
        "            'toImageButtonOptions': {\n",
        "                'filename': self.config[\"model_save_path\"],\n",
        "                'format': 'png',\n",
        "                'width': 1200,\n",
        "                'height': 800,\n",
        "                'scale': 1\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.fig.update_layout(\n",
        "            width=1200,\n",
        "            height=800,\n",
        "            title_text='Training Metrics',\n",
        "            showlegend=True,\n",
        "            margin=dict(l=50, r=50, t=100, b=150),\n",
        "            annotations=[\n",
        "                dict(\n",
        "                    text=wrapped_footer,\n",
        "                    showarrow=False,\n",
        "                    xref=\"paper\",\n",
        "                    yref=\"paper\",\n",
        "                    x=0.5,\n",
        "                    y=-0.15,\n",
        "                    xanchor='center',\n",
        "                    yanchor='top',\n",
        "                    align=\"center\",\n",
        "                    font=dict(size=10, color=\"gray\")\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Annotation for best metrics\n",
        "        self.best_metrics_annotation_index = len(self.fig.layout.annotations)\n",
        "        self.fig.add_annotation(\n",
        "            text=\"Best Spearman (Test): N/A<br>Best Pearson (Test): N/A<br>Best Spearman (Val): N/A<br>Best Pearson (Val): N/A\",\n",
        "            showarrow=False,\n",
        "            xref=\"paper\",\n",
        "            yref=\"paper\",\n",
        "            x=1.0,\n",
        "            y=0.0,\n",
        "            xanchor='right',\n",
        "            yanchor='bottom',\n",
        "            align=\"right\",\n",
        "            font=dict(size=12, color=\"blue\")\n",
        "        )\n",
        "\n",
        "        self.fig.show(config=self.plot_config)\n",
        "\n",
        "    def _update_traces(self):\n",
        "        with self.fig.batch_update():\n",
        "            self.fig.data[0].x = self.iterations\n",
        "            self.fig.data[0].y = self.loss_values\n",
        "            self.fig.data[1].x = self.iterations\n",
        "            self.fig.data[1].y = self.mean_grad_norm_values\n",
        "            self.fig.data[2].x = self.iterations\n",
        "            self.fig.data[2].y = self.variance_values\n",
        "            self.fig.data[3].x = self.iterations\n",
        "            self.fig.data[3].y = self.learning_rate_values\n",
        "            self.fig.data[4].x = self.iterations\n",
        "            self.fig.data[4].y = self.sts_pearson_cosine_values\n",
        "            self.fig.data[5].x = self.iterations\n",
        "            self.fig.data[5].y = self.sts_spearman_cosine_values\n",
        "            self.fig.data[6].x = self.test_iterations\n",
        "            self.fig.data[6].y = self.test_sts_pearson_cosine_values\n",
        "            self.fig.data[7].x = self.test_iterations\n",
        "            self.fig.data[7].y = self.test_sts_spearman_cosine_values\n",
        "\n",
        "            for i in range(1, 4):\n",
        "                for j in range(1, 3):\n",
        "                    self.fig.update_yaxes(autorange=True, row=i, col=j)\n",
        "                    self.fig.update_xaxes(autorange=True, row=i, col=j)\n",
        "\n",
        "    def _update_plot(self):\n",
        "        self._update_traces()\n",
        "        unique_epochs = sorted(set(self.epochs))\n",
        "        frames = []\n",
        "        for ep in unique_epochs:\n",
        "            indices = [i for i, e in enumerate(self.epochs) if e == ep]\n",
        "            frame = go.Frame(\n",
        "                data=[\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.loss_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.mean_grad_norm_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.variance_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.learning_rate_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.sts_pearson_cosine_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.sts_spearman_cosine_values[i] for i in indices])\n",
        "                ],\n",
        "                name=str(ep)\n",
        "            )\n",
        "            frames.append(frame)\n",
        "        self.fig.frames = frames\n",
        "\n",
        "        slider_steps = [\n",
        "            {\"args\": [[str(ep)], {\"frame\": {\"duration\": 0, \"redraw\": True},\n",
        "                                   \"mode\": \"immediate\", \"transition\": {\"duration\": 0}}],\n",
        "             \"label\": str(ep), \"method\": \"animate\"} for ep in unique_epochs\n",
        "        ]\n",
        "\n",
        "        self.fig.update_layout(\n",
        "            sliders=[{\n",
        "                \"active\": len(unique_epochs) - 1 if unique_epochs else 0,\n",
        "                \"currentvalue\": {\"prefix\": \"Epoch: \"},\n",
        "                \"pad\": {\"t\": 50},\n",
        "                \"steps\": slider_steps\n",
        "            }]\n",
        "        )\n",
        "\n",
        "        # Compute best test metrics if available\n",
        "        if self.test_sts_spearman_cosine_values:\n",
        "            best_test_spearman = max(self.test_sts_spearman_cosine_values)\n",
        "        else:\n",
        "            best_test_spearman = float('nan')\n",
        "        if self.test_sts_pearson_cosine_values:\n",
        "            best_test_pearson = max(self.test_sts_pearson_cosine_values)\n",
        "        else:\n",
        "            best_test_pearson = float('nan')\n",
        "\n",
        "        self.fig.layout.annotations[self.best_metrics_annotation_index].text = (\n",
        "            f\"Best Spearman (Test): {best_test_spearman:.4f}<br>\"\n",
        "            f\"Best Pearson (Test): {best_test_pearson:.4f}<br>\"\n",
        "            f\"Best Spearman (Val): {self.best_spearman:.4f}<br>\"\n",
        "            f\"Best Pearson (Val): {self.best_pearson:.4f}\"\n",
        "        )\n",
        "\n",
        "        for i in range(1, 4):\n",
        "            for j in range(1, 3):\n",
        "                self.fig.update_yaxes(autorange=True, row=i, col=j)\n",
        "                self.fig.update_xaxes(autorange=True, row=i, col=j)\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        self.fig.show(config=self.plot_config)\n",
        "\n",
        "    def _prepare_datasets(self):\n",
        "        wikipedia_url = \"https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse/resolve/main/wiki1m_for_simcse.txt\"\n",
        "        wikipedia_dataset_path = \"data/wiki1m_for_simcse.txt\"\n",
        "        if not os.path.exists(wikipedia_dataset_path):\n",
        "            util.http_get(wikipedia_url, wikipedia_dataset_path)\n",
        "        train_sentences = []\n",
        "        with open(wikipedia_dataset_path, encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if len(line) >= 10:\n",
        "                    train_sentences.append(line)\n",
        "        self.train_sentences = train_sentences\n",
        "\n",
        "        self.train_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"train\")\n",
        "        self.eval_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n",
        "        self.test_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"test\")\n",
        "\n",
        "        self.train_data_loader = DataLoader(\n",
        "            SentenceDataset(self.train_sentences),\n",
        "            batch_size=self.config[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            num_workers=self.config[\"num_workers\"],\n",
        "            pin_memory=True\n",
        "        )\n",
        "        self.test_evaluator = EmbeddingSimilarityEvaluator(\n",
        "            sentences1=self.test_dataset[\"sentence1\"],\n",
        "            sentences2=self.test_dataset[\"sentence2\"],\n",
        "            scores=self.test_dataset[\"score\"]\n",
        "        )\n",
        "\n",
        "        self.evaluate_steps = max(len(self.train_data_loader) // 50, 1)\n",
        "\n",
        "        # Ensure patience is at least five times as large as evaluate_steps\n",
        "        if self.config.get(\"patience\", 0) < self.evaluate_steps * 5:\n",
        "            print(f\"Warning: Patience ({self.config['patience']}) is less than evaluation steps x5 ({self.evaluate_steps * 5}). Adjusting patience to {self.evaluate_steps * 5}.\")\n",
        "            self.config[\"patience\"] = self.evaluate_steps * 5\n",
        "\n",
        "    def _apply_augmentation(self, sentences, aug):\n",
        "        return aug.augment(sentences)\n",
        "\n",
        "    def _initialize_models(self):\n",
        "        word_embedding_model = models.Transformer(\n",
        "            self.config[\"model_name\"],\n",
        "            max_seq_length=self.config[\"max_seq_length\"],\n",
        "            config_args={\"attention_dropout\": self.config[\"aug_p\"], \"dropout\": self.config[\"aug_p\"]}\n",
        "        )\n",
        "        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "        projection_layers = [torch.nn.Linear(768, self.config[\"projection_size\"])]\n",
        "        for _ in range(self.config[\"projection_depth\"] - 1):\n",
        "            projection_layers.append(torch.nn.BatchNorm1d(self.config[\"projection_size\"]))\n",
        "            projection_layers.append(torch.nn.ReLU())\n",
        "            projection_layers.append(torch.nn.Linear(self.config[\"projection_size\"], self.config[\"projection_size\"]))\n",
        "        projection_head = torch.nn.Sequential(*projection_layers)\n",
        "        self.online_model = SentenceTransformer(modules=[word_embedding_model, pooling_model, projection_head]).to(self.device)\n",
        "        encoder_modules = [copy.deepcopy(self.online_model[i]) for i in range(2)]\n",
        "        self.encoder = SentenceTransformer(modules=encoder_modules).to(self.device)\n",
        "\n",
        "    def _update_encoder(self):\n",
        "        for i in range(len(self.encoder)):\n",
        "            self.encoder[i].load_state_dict(self.online_model[i].state_dict())\n",
        "\n",
        "    def _initialize_optimizer_scheduler(self):\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.online_model.parameters(),\n",
        "            lr=self.config[\"learning_rate\"],\n",
        "            betas=(0.9, 0.999),\n",
        "            eps=1e-8,\n",
        "            weight_decay=self.config[\"weight_decay\"]\n",
        "        )\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=200, verbose=True, min_lr=1e-6)\n",
        "\n",
        "    def _forward_pass(self, model, sentences, train):\n",
        "        if train:\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "        features = model.tokenize(sentences)\n",
        "        features = {k: v.to(self.device, non_blocking=True) for k, v in features.items()}\n",
        "        with autocast(\"cuda\", enabled=self.config.get(\"use_amp\", True)):\n",
        "            embeddings = model[0](features)[\"token_embeddings\"]\n",
        "            pooled = model[1]({\"token_embeddings\": embeddings})[\"sentence_embedding\"]\n",
        "            if len(model) > 2:\n",
        "                return model[2](pooled)\n",
        "            else:\n",
        "                return pooled\n",
        "\n",
        "    def _mixed_barlow_twins_loss(self, z_a, z_b):\n",
        "        N, D = z_a.size()\n",
        "        z_a_norm = (z_a - z_a.mean(dim=0)) / (z_a.std(dim=0) + 1e-6)\n",
        "        z_b_norm = (z_b - z_b.mean(dim=0)) / (z_b.std(dim=0) + 1e-6)\n",
        "        c = torch.matmul(z_a_norm.T, z_b_norm) / N\n",
        "        I = torch.eye(D, device=z_a.device)\n",
        "        c_diff = (c - I).pow(2)\n",
        "        off_diag_mask = ~torch.eye(D, dtype=torch.bool, device=z_a.device)\n",
        "        c_diff[off_diag_mask] *= self.config[\"lambda_bt\"]\n",
        "        loss_bt = c_diff.sum()\n",
        "        # MixUp Regularization\n",
        "        idx = torch.randperm(N)\n",
        "        alpha = torch.tensor(np.random.beta(1.0, 1.0), device=z_a.device, dtype=z_a.dtype)\n",
        "        z_m = alpha * z_a + (1 - alpha) * z_b[idx, :]\n",
        "        z_m_norm = (z_m - z_m.mean(dim=0)) / (z_m.std(dim=0) + 1e-6)\n",
        "        cc_m_a = torch.matmul(z_m_norm.T, z_a_norm) / N\n",
        "        cc_m_b = torch.matmul(z_m_norm.T, z_b_norm) / N\n",
        "        cc_m_a_gt = alpha * torch.matmul(z_a_norm.T, z_a_norm) / N + (1 - alpha) * torch.matmul(z_b_norm[idx, :].T, z_a_norm) / N\n",
        "        cc_m_b_gt = alpha * torch.matmul(z_a_norm.T, z_b_norm) / N + (1 - alpha) * torch.matmul(z_b_norm[idx, :].T, z_b_norm) / N\n",
        "        loss_mix = self.config[\"lambda_mixup\"] * self.config[\"lambda_bt\"] * (\n",
        "            (cc_m_a - cc_m_a_gt).pow(2).sum() + (cc_m_b - cc_m_b_gt).pow(2).sum()\n",
        "        )\n",
        "        return loss_bt + loss_mix\n",
        "\n",
        "    def _evaluate_without_heads(self):\n",
        "        self._update_encoder()\n",
        "        self.encoder.eval()\n",
        "        indices = list(range(len(self.eval_dataset[\"sentence1\"])))\n",
        "        random.shuffle(indices)\n",
        "        sentences1 = [self.eval_dataset[\"sentence1\"][i] for i in indices]\n",
        "        sentences2 = [self.eval_dataset[\"sentence2\"][i] for i in indices]\n",
        "        scores = [self.eval_dataset[\"score\"][i] for i in indices]\n",
        "        evaluator = EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)\n",
        "        return evaluator(self.encoder)\n",
        "\n",
        "    def fit(self):\n",
        "        latest_eval_metrics = {}\n",
        "        for epoch in range(self.config[\"epochs\"]):\n",
        "            early_stop = False\n",
        "            epoch_loss = 0\n",
        "            pbar = tqdm(self.train_data_loader, desc=f\"Epoch {epoch+1}/{self.config['epochs']}:\")\n",
        "            for idx, sentences in enumerate(pbar):\n",
        "                aug1, aug2 = random.sample(self.augmenters, 2)\n",
        "                s1 = self._apply_augmentation(sentences, aug1)\n",
        "                s2 = self._apply_augmentation(sentences, aug2)\n",
        "                with autocast(\"cuda\", enabled=self.config.get(\"use_amp\", True)):\n",
        "                    z_a = self._forward_pass(self.online_model, s1, train=True)\n",
        "                    z_b = self._forward_pass(self.online_model, s2, train=True)\n",
        "                    loss = self._mixed_barlow_twins_loss(z_a, z_b)\n",
        "                epoch_loss += loss.item()\n",
        "                self.optimizer.zero_grad()\n",
        "                self.scaler.scale(loss).backward()\n",
        "                scale = self.scaler.get_scale()\n",
        "                scale = scale if scale != 0 else 1e-8\n",
        "                total_norm_scaled = 0.0\n",
        "                for param in self.online_model.parameters():\n",
        "                    if param.grad is not None:\n",
        "                        param_norm = param.grad.data.norm(2).item()\n",
        "                        total_norm_scaled += param_norm ** 2\n",
        "                total_norm_scaled = math.sqrt(total_norm_scaled)\n",
        "                total_norm = total_norm_scaled / scale\n",
        "                mean_grad_norm = total_norm / len(list(self.online_model.parameters()))\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "                self.scheduler.step(loss)\n",
        "                if idx % self.evaluate_steps == 0 or idx in [0, len(self.train_data_loader)-1]:\n",
        "                    latest_eval_metrics = self._evaluate_without_heads()\n",
        "                    last_spearman = latest_eval_metrics.get('spearman_cosine', -float('inf'))\n",
        "                    last_pearson = latest_eval_metrics.get('pearson_cosine', -float('inf'))\n",
        "                    self._update_encoder()\n",
        "                    self.encoder.eval()\n",
        "                    test_metrics = self.test_evaluator(self.encoder)\n",
        "                    self.test_sts_pearson_cosine_values.append(test_metrics.get('pearson_cosine', np.nan))\n",
        "                    self.test_sts_spearman_cosine_values.append(test_metrics.get('spearman_cosine', np.nan))\n",
        "                    self.test_iterations.append(idx)\n",
        "                    self._update_plot()\n",
        "\n",
        "                    if last_spearman > self.best_spearman:\n",
        "                        self.best_spearman = last_spearman\n",
        "                        self.best_pearson = last_pearson\n",
        "                        self.patience_counter = 0\n",
        "                    else:\n",
        "                        self.patience_counter += self.evaluate_steps\n",
        "                pbar.set_postfix({\n",
        "                    \"loss\": loss.item(),\n",
        "                    **latest_eval_metrics,\n",
        "                    \"mean_grad_norm\": mean_grad_norm,\n",
        "                    \"learning_rate\": self.optimizer.param_groups[0]['lr']\n",
        "                })\n",
        "                self.loss_values.append(loss.item())\n",
        "                self.sts_pearson_cosine_values.append(latest_eval_metrics.get('pearson_cosine', np.nan))\n",
        "                self.sts_spearman_cosine_values.append(latest_eval_metrics.get('spearman_cosine', np.nan))\n",
        "                self.mean_grad_norm_values.append(mean_grad_norm)\n",
        "                self.variance_values.append(torch.var(z_a, dim=0).mean().item())\n",
        "                self.learning_rate_values.append(self.optimizer.param_groups[0]['lr'])\n",
        "                self.iterations.append(idx)\n",
        "                self.epochs.append(epoch)\n",
        "                if idx % self.evaluate_steps == 0 or idx in [0, len(self.train_data_loader)-1]:\n",
        "                    if self.patience_counter >= self.config[\"patience\"]:\n",
        "                        early_stop = True\n",
        "                        print(f\"Early stopping triggered at epoch {epoch+1}, iteration {idx}.\")\n",
        "                        print(f\"Best Spearman Correlation: {self.best_spearman}\")\n",
        "                        print(f\"Best Pearson Correlation: {self.best_pearson}\")\n",
        "                        break\n",
        "            if early_stop:\n",
        "                break\n",
        "            avg_loss = epoch_loss / len(self.train_data_loader)\n",
        "            pbar.set_description(f\"Epoch {epoch+1} Loss: {avg_loss}\")\n",
        "\n",
        "    def cleanup(self):\n",
        "        del self.online_model, self.optimizer, self.scheduler, self.scaler\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "class SentenceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sentences):\n",
        "        self.sentences = sentences\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentences[idx]\n",
        "\n",
        "# Mount Google Drive for checkpoint and study persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/violet_bert_base_checkpoints_2\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "STUDY_DB_PATH = os.path.join(CHECKPOINT_DIR, \"llm_finetuning_study.db\")\n",
        "\n",
        "def save_checkpoint(trial_number, trainer):\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_trial_{trial_number}.pt\")\n",
        "    torch.save({\n",
        "        'online_model_state_dict': trainer.online_model.state_dict(),\n",
        "        'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
        "        'scheduler_state_dict': trainer.scheduler.state_dict(),\n",
        "        'best_spearman': trainer.best_spearman,\n",
        "        'best_pearson': trainer.best_pearson,\n",
        "        'epochs': trainer.epochs,\n",
        "        'iterations': trainer.iterations,\n",
        "    }, checkpoint_path)\n",
        "\n",
        "    # Also save the graph to a subfolder called \"graphs\"\n",
        "    graphs_dir = os.path.join(CHECKPOINT_DIR, \"graphs\")\n",
        "    os.makedirs(graphs_dir, exist_ok=True)\n",
        "    graph_path = os.path.join(graphs_dir, f\"checkpoint_trial_{trial_number}.png\")\n",
        "    trainer.fig.write_image(graph_path)\n",
        "    print(f\"Checkpoint and graph saved to {checkpoint_path} and {graph_path}\")\n",
        "\n",
        "    return checkpoint_path\n",
        "\n",
        "def load_checkpoint(trainer, checkpoint_path):\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        trainer.online_model.load_state_dict(checkpoint['online_model_state_dict'])\n",
        "        trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        trainer.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        trainer.best_spearman = checkpoint.get('best_spearman', -float(\"inf\"))\n",
        "        trainer.best_pearson = checkpoint.get('best_pearson', -float(\"inf\"))\n",
        "        trainer.epochs = checkpoint.get('epochs', [])\n",
        "        trainer.iterations = checkpoint.get('iterations', [])\n",
        "        print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
        "\n",
        "def objective(trial):\n",
        "    # Experiment with various hyperparameters\n",
        "    config = {\n",
        "        \"model_name\": \"bert-base-uncased\",\n",
        "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128]),\n",
        "        \"projection_depth\": trial.suggest_int(\"projection_depth\", 2, 3),\n",
        "        \"projection_size\": trial.suggest_categorical(\"projection_size\", [2048, 4096, 6144, 8192]),\n",
        "        \"epochs\": 1,  # fixed at 1\n",
        "        \"warmup_proportion\": 0.0,\n",
        "        \"max_seq_length\": 75,#trial.suggest_categorical(\"max_seq_length\", [32, 75]),\n",
        "        \"aug_p\": trial.suggest_uniform(\"aug_p\", 0.1, 0.4),\n",
        "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-4),\n",
        "        \"model_save_path\": os.path.join(CHECKPOINT_DIR, f\"train_stsb_bt-distilbert-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_trial{trial.number}\"),\n",
        "        \"num_workers\": 10,\n",
        "        \"weight_decay\": trial.suggest_uniform(\"weight_decay\", 1e-4, 0.2),\n",
        "        \"lambda_bt\": trial.suggest_uniform(\"lambda_bt\", 0.001, 0.2),\n",
        "        \"lambda_mixup\": trial.suggest_uniform(\"lambda_mixup\", 0.1, 1.5),\n",
        "        \"use_amp\": True,\n",
        "        \"patience\": 1000\n",
        "    }\n",
        "\n",
        "    trainer = BarlowTwinsNCSE(config)\n",
        "    # Resume from checkpoint if available\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_trial_{trial.number}.pt\")\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        load_checkpoint(trainer, checkpoint_path)\n",
        "\n",
        "    try:\n",
        "        trainer.fit()\n",
        "    except KeyboardInterrupt:\n",
        "        save_checkpoint(trial.number, trainer)\n",
        "        raise optuna.TrialPruned(\"Trial interrupted and checkpoint saved.\")\n",
        "\n",
        "    # Save checkpoint at end of trial\n",
        "    save_checkpoint(trial.number, trainer)\n",
        "\n",
        "    # Use the STS-B validation metric (spearman cosine) as objective.\n",
        "    val_metrics = trainer._evaluate_without_heads()\n",
        "    return val_metrics.get(\"spearman_cosine\", -float(\"inf\"))\n",
        "    trainer.cleanup()\n",
        "\n",
        "# Create or resume an Optuna study persisted on Google Drive\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=\"llm_finetuning_study\",\n",
        "    storage=f\"sqlite:///{STUDY_DB_PATH}\",\n",
        "    load_if_exists=True\n",
        ")\n",
        "study.optimize(objective, n_trials=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4wKt9oUXqQR"
      },
      "source": [
        "### adding optuna pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3pCSS3DQXmL9",
        "outputId": "65355b13-254e-48a2-edb6-835b9b91017f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"3671aeaf-1ec2-4f69-b793-70daa25d5fb6\" class=\"plotly-graph-div\" style=\"height:800px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3671aeaf-1ec2-4f69-b793-70daa25d5fb6\")) {                    Plotly.newPlot(                        \"3671aeaf-1ec2-4f69-b793-70daa25d5fb6\",                        [{\"mode\":\"lines+markers\",\"name\":\"Loss\",\"x\":[],\"y\":[],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"lines+markers\",\"name\":\"Mean Gradient Norm\",\"x\":[],\"y\":[],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"mode\":\"lines+markers\",\"name\":\"Variance\",\"x\":[],\"y\":[],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"mode\":\"lines+markers\",\"name\":\"Learning Rate\",\"x\":[],\"y\":[],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"mode\":\"lines+markers\",\"name\":\"Dev STS Pearson Cosine\",\"x\":[],\"y\":[],\"type\":\"scatter\",\"xaxis\":\"x5\",\"yaxis\":\"y5\"},{\"mode\":\"lines+markers\",\"name\":\"Dev STS Spearman Cosine\",\"x\":[],\"y\":[],\"type\":\"scatter\",\"xaxis\":\"x5\",\"yaxis\":\"y5\"},{\"mode\":\"lines+markers\",\"name\":\"Test STS Pearson Cosine\",\"x\":[0],\"y\":[0.47904047789130433],\"type\":\"scatter\",\"xaxis\":\"x6\",\"yaxis\":\"y6\"},{\"mode\":\"lines+markers\",\"name\":\"Test STS Spearman Cosine\",\"x\":[0],\"y\":[0.4729006928189095],\"type\":\"scatter\",\"xaxis\":\"x6\",\"yaxis\":\"y6\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"autorange\":true},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.7777777777777778,1.0],\"autorange\":true},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"autorange\":true},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.7777777777777778,1.0],\"autorange\":true},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,0.45],\"autorange\":true},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.3888888888888889,0.6111111111111112],\"autorange\":true},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.55,1.0],\"autorange\":true},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.3888888888888889,0.6111111111111112],\"autorange\":true},\"xaxis5\":{\"anchor\":\"y5\",\"domain\":[0.0,0.45],\"autorange\":true},\"yaxis5\":{\"anchor\":\"x5\",\"domain\":[0.0,0.22222222222222224],\"autorange\":true},\"xaxis6\":{\"anchor\":\"y6\",\"domain\":[0.55,1.0],\"autorange\":true},\"yaxis6\":{\"anchor\":\"x6\",\"domain\":[0.0,0.22222222222222224],\"autorange\":true},\"annotations\":[{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=bert-base-uncased, batch_size=128, projection_depth=3, projection_size=6144, epochs=1, warmup_proportion=0.0, max_seq_length=75,\\u003cbr\\u003eaug_p=0.11400048437625866, learning_rate=2.064338638869684e-05, model_save_path=\\u002fcontent\\u002fdrive\\u002fMyDrive\\u002fviolet_bert_base_checkpoints_2\\u002ftrain_stsb_bt-\\u003cbr\\u003edistilbert-2025-04-15_19-08-53_trial143, num_workers=10, weight_decay=0.15987527302370444, lambda_bt=0.14134482281890384, lambda_mixup=1.3187214850728965,\\u003cbr\\u003euse_amp=True, patience=1000, Augmenters: Synonym_Aug[substitute:0.11400048437625866], RandomWord_Aug[swap:0.11400048437625866],\\u003cbr\\u003eRandomWord_Aug[delete:0.11400048437625866]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=bert-base-uncased, batch_size=128, projection_depth=3, projection_size=6144, epochs=1, warmup_proportion=0.0, max_seq_length=75,\\u003cbr\\u003eaug_p=0.11400048437625866, learning_rate=2.064338638869684e-05, model_save_path=\\u002fcontent\\u002fdrive\\u002fMyDrive\\u002fviolet_bert_base_checkpoints_2\\u002ftrain_stsb_bt-\\u003cbr\\u003edistilbert-2025-04-15_19-08-53_trial143, num_workers=10, weight_decay=0.15987527302370444, lambda_bt=0.14134482281890384, lambda_mixup=1.3187214850728965,\\u003cbr\\u003euse_amp=True, patience=1000, Augmenters: Synonym_Aug[substitute:0.11400048437625866], RandomWord_Aug[swap:0.11400048437625866],\\u003cbr\\u003eRandomWord_Aug[delete:0.11400048437625866]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=bert-base-uncased, batch_size=128, projection_depth=3, projection_size=6144, epochs=1, warmup_proportion=0.0, max_seq_length=75,\\u003cbr\\u003eaug_p=0.11400048437625866, learning_rate=2.064338638869684e-05, model_save_path=\\u002fcontent\\u002fdrive\\u002fMyDrive\\u002fviolet_bert_base_checkpoints_2\\u002ftrain_stsb_bt-\\u003cbr\\u003edistilbert-2025-04-15_19-08-53_trial143, num_workers=10, weight_decay=0.15987527302370444, lambda_bt=0.14134482281890384, lambda_mixup=1.3187214850728965,\\u003cbr\\u003euse_amp=True, patience=1000, Augmenters: Synonym_Aug[substitute:0.11400048437625866], RandomWord_Aug[swap:0.11400048437625866],\\u003cbr\\u003eRandomWord_Aug[delete:0.11400048437625866]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=bert-base-uncased, batch_size=128, projection_depth=3, projection_size=6144, epochs=1, warmup_proportion=0.0, max_seq_length=75,\\u003cbr\\u003eaug_p=0.11400048437625866, learning_rate=2.064338638869684e-05, model_save_path=\\u002fcontent\\u002fdrive\\u002fMyDrive\\u002fviolet_bert_base_checkpoints_2\\u002ftrain_stsb_bt-\\u003cbr\\u003edistilbert-2025-04-15_19-08-53_trial143, num_workers=10, weight_decay=0.15987527302370444, lambda_bt=0.14134482281890384, lambda_mixup=1.3187214850728965,\\u003cbr\\u003euse_amp=True, patience=1000, Augmenters: Synonym_Aug[substitute:0.11400048437625866], RandomWord_Aug[swap:0.11400048437625866],\\u003cbr\\u003eRandomWord_Aug[delete:0.11400048437625866]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=bert-base-uncased, batch_size=128, projection_depth=3, projection_size=6144, epochs=1, warmup_proportion=0.0, max_seq_length=75,\\u003cbr\\u003eaug_p=0.11400048437625866, learning_rate=2.064338638869684e-05, model_save_path=\\u002fcontent\\u002fdrive\\u002fMyDrive\\u002fviolet_bert_base_checkpoints_2\\u002ftrain_stsb_bt-\\u003cbr\\u003edistilbert-2025-04-15_19-08-53_trial143, num_workers=10, weight_decay=0.15987527302370444, lambda_bt=0.14134482281890384, lambda_mixup=1.3187214850728965,\\u003cbr\\u003euse_amp=True, patience=1000, Augmenters: Synonym_Aug[substitute:0.11400048437625866], RandomWord_Aug[swap:0.11400048437625866],\\u003cbr\\u003eRandomWord_Aug[delete:0.11400048437625866]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"center\",\"font\":{\"color\":\"gray\",\"size\":10},\"showarrow\":false,\"text\":\"model_name=bert-base-uncased, batch_size=128, projection_depth=3, projection_size=6144, epochs=1, warmup_proportion=0.0, max_seq_length=75,\\u003cbr\\u003eaug_p=0.11400048437625866, learning_rate=2.064338638869684e-05, model_save_path=\\u002fcontent\\u002fdrive\\u002fMyDrive\\u002fviolet_bert_base_checkpoints_2\\u002ftrain_stsb_bt-\\u003cbr\\u003edistilbert-2025-04-15_19-08-53_trial143, num_workers=10, weight_decay=0.15987527302370444, lambda_bt=0.14134482281890384, lambda_mixup=1.3187214850728965,\\u003cbr\\u003euse_amp=True, patience=1000, Augmenters: Synonym_Aug[substitute:0.11400048437625866], RandomWord_Aug[swap:0.11400048437625866],\\u003cbr\\u003eRandomWord_Aug[delete:0.11400048437625866]\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":-0.15,\"yanchor\":\"top\",\"yref\":\"paper\"},{\"align\":\"right\",\"font\":{\"color\":\"blue\",\"size\":12},\"showarrow\":false,\"text\":\"Best Spearman (Test): 0.4729\\u003cbr\\u003eBest Pearson (Test): 0.4790\\u003cbr\\u003eBest Spearman (Val): -inf\\u003cbr\\u003eBest Pearson (Val): -inf\",\"x\":1.0,\"xanchor\":\"right\",\"xref\":\"paper\",\"y\":0.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Training Metrics\"},\"margin\":{\"l\":50,\"r\":50,\"t\":100,\"b\":150},\"width\":1200,\"height\":800,\"showlegend\":true,\"sliders\":[{\"active\":0,\"currentvalue\":{\"prefix\":\"Epoch: \"},\"pad\":{\"t\":50}}]},                        {\"toImageButtonOptions\": {\"filename\": \"/content/drive/MyDrive/violet_bert_base_checkpoints_2/train_stsb_bt-distilbert-2025-04-15_19-08-53_trial143\", \"format\": \"png\", \"width\": 1200, \"height\": 800, \"scale\": 1}, \"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('3671aeaf-1ec2-4f69-b793-70daa25d5fb6');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1::   1%|          | 66/7701 [01:00<1:56:35,  1.09it/s, loss=5.3e+4, pearson_cosine=0.592, spearman_cosine=0.593, mean_grad_norm=160, learning_rate=2.06e-5]\n",
            "[I 2025-04-15 19:10:08,348] Trial 143 pruned. Trial interrupted and checkpoint saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph saved to /content/drive/MyDrive/violet_bert_base_checkpoints_2/graphs/checkpoint_trial_143.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-311849fa0b5e>:494: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "<ipython-input-2-311849fa0b5e>:495: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[W 2025-04-15 19:10:10,842] Trial 144 failed with parameters: {'batch_size': 128, 'projection_depth': 3, 'projection_size': 6144, 'aug_p': 0.1268875636524825} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-2-311849fa0b5e>\", line 495, in objective\n",
            "    \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-4),\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/_deprecated.py\", line 114, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/trial/_trial.py\", line 207, in suggest_loguniform\n",
            "    return self.suggest_float(name, low, high, log=True)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/trial/_trial.py\", line 161, in suggest_float\n",
            "    suggested_value = self._suggest(name, distribution)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/trial/_trial.py\", line 637, in _suggest\n",
            "    storage.set_trial_param(trial_id, name, param_value_in_internal_repr, distribution)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/storages/_cached_storage.py\", line 169, in set_trial_param\n",
            "    self._backend.set_trial_param(trial_id, param_name, param_value_internal, distribution)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/storages/_rdb/storage.py\", line 578, in set_trial_param\n",
            "    with _create_scoped_session(self.scoped_session, True) as session:\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/storages/_rdb/storage.py\", line 77, in _create_scoped_session\n",
            "    session.commit()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py\", line 2032, in commit\n",
            "    trans.commit(_to_root=True)\n",
            "  File \"<string>\", line 2, in commit\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/state_changes.py\", line 139, in _go\n",
            "    ret_value = fn(self, *arg, **kw)\n",
            "                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py\", line 1313, in commit\n",
            "    self._prepare_impl()\n",
            "  File \"<string>\", line 2, in _prepare_impl\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/state_changes.py\", line 139, in _go\n",
            "    ret_value = fn(self, *arg, **kw)\n",
            "                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py\", line 1288, in _prepare_impl\n",
            "    self.session.flush()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py\", line 4353, in flush\n",
            "    self._flush(objects)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py\", line 4488, in _flush\n",
            "    with util.safe_reraise():\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/util/langhelpers.py\", line 146, in __exit__\n",
            "    raise exc_value.with_traceback(exc_tb)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py\", line 4449, in _flush\n",
            "    flush_context.execute()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/unitofwork.py\", line 466, in execute\n",
            "    rec.execute(self)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/unitofwork.py\", line 642, in execute\n",
            "    util.preloaded.orm_persistence.save_obj(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/persistence.py\", line 93, in save_obj\n",
            "    _emit_insert_statements(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/persistence.py\", line 1233, in _emit_insert_statements\n",
            "    result = connection.execute(\n",
            "             ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\", line 1416, in execute\n",
            "    return meth(\n",
            "           ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/elements.py\", line 523, in _execute_on_connection\n",
            "    return connection._execute_clauseelement(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\", line 1638, in _execute_clauseelement\n",
            "    ret = self._execute_context(\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\", line 1843, in _execute_context\n",
            "    return self._exec_single_context(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\", line 1983, in _exec_single_context\n",
            "    self._handle_dbapi_exception(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\", line 2355, in _handle_dbapi_exception\n",
            "    raise exc_info[1].with_traceback(exc_info[2])\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\", line 1964, in _exec_single_context\n",
            "    self.dialect.do_execute(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/default.py\", line 945, in do_execute\n",
            "    cursor.execute(statement, parameters)\n",
            "KeyboardInterrupt\n",
            "[W 2025-04-15 19:10:10,855] Trial 144 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-311849fa0b5e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    568\u001b[0m '''\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-311849fa0b5e>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;34m\"max_seq_length\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# or trial.suggest_categorical(\"max_seq_length\", [32, 75]),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;34m\"aug_p\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"aug_p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         \u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest_loguniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m         \u001b[0;34m\"model_save_path\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"train_stsb_bt-distilbert-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_trial{trial.number}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;34m\"num_workers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/_deprecated.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/trial/_trial.py\u001b[0m in \u001b[0;36msuggest_loguniform\u001b[0;34m(self, name, low, high)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \"\"\"\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3.0.0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"6.0.0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_suggest_deprecated_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"(..., step=...)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/trial/_trial.py\u001b[0m in \u001b[0;36msuggest_float\u001b[0;34m(self, name, low, high, step, log)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFloatDistribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0msuggested_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_suggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuggested_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/trial/_trial.py\u001b[0m in \u001b[0;36m_suggest\u001b[0;34m(self, name, distribution)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;31m# `param_value` is validated here (invalid value like `np.nan` raises ValueError).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0mparam_value_in_internal_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_internal_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trial_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_value_in_internal_repr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_frozen_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/storages/_cached_storage.py\u001b[0m in \u001b[0;36mset_trial_param\u001b[0;34m(self, trial_id, param_name, param_value_internal, distribution)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mdistribution\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseDistribution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     ) -> None:\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trial_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_value_internal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_trial_id_from_study_id_trial_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial_number\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/storages/_rdb/storage.py\u001b[0m in \u001b[0;36mset_trial_param\u001b[0;34m(self, trial_id, param_name, param_value_internal, distribution)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0mdistribution\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseDistribution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     ) -> None:\n\u001b[0;32m--> 578\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_create_scoped_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoped_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m             self._set_trial_param_without_commit(\n\u001b[1;32m    580\u001b[0m                 \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_value_internal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/storages/_rdb/storage.py\u001b[0m in \u001b[0;36m_create_scoped_session\u001b[0;34m(scoped_session, ignore_integrity_error)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0msqlalchemy_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegrityError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py\u001b[0m in \u001b[0;36mcommit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2030\u001b[0m             \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autobegin_t\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2032\u001b[0;31m         \u001b[0mtrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2034\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py\u001b[0m in \u001b[0;36mcommit\u001b[0;34m(self, _to_root)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/state_changes.py\u001b[0m in \u001b[0;36m_go\u001b[0;34m(fn, self, *arg, **kw)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_StateChangeStates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHANGE_IN_PROGRESS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mret_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py\u001b[0m in \u001b[0;36mcommit\u001b[0;34m(self, _to_root)\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSessionTransactionState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREPARED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expect_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSessionTransactionState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREPARED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1313\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnested\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py\u001b[0m in \u001b[0;36m_prepare_impl\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/state_changes.py\u001b[0m in \u001b[0;36m_go\u001b[0;34m(fn, self, *arg, **kw)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_StateChangeStates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHANGE_IN_PROGRESS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mret_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py\u001b[0m in \u001b[0;36m_prepare_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1286\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m                 raise exc.FlushError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self, objects)\u001b[0m\n\u001b[1;32m   4351\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4352\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flushing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4354\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flushing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py\u001b[0m in \u001b[0;36m_flush\u001b[0;34m(self, objects)\u001b[0m\n\u001b[1;32m   4486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4487\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4488\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4489\u001b[0m                 \u001b[0mtransaction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_capture_exception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/util/langhelpers.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mexc_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# remove potential circular references\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# remove potential circular references\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py\u001b[0m in \u001b[0;36m_flush\u001b[0;34m(self, objects)\u001b[0m\n\u001b[1;32m   4447\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_on_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4448\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4449\u001b[0;31m                 \u001b[0mflush_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4450\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_on_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/unitofwork.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopological\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdependencies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostsort_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                 \u001b[0mrec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinalize_flush_changes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/unitofwork.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, uow)\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sqlalchemy.orm.persistence\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m         util.preloaded.orm_persistence.save_obj(\n\u001b[0m\u001b[1;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m             \u001b[0muow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates_for_mapper_hierarchy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/persistence.py\u001b[0m in \u001b[0;36msave_obj\u001b[0;34m(base_mapper, states, uowtransaction, single)\u001b[0m\n\u001b[1;32m     91\u001b[0m         )\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         _emit_insert_statements(\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mbase_mapper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0muowtransaction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/persistence.py\u001b[0m in \u001b[0;36m_emit_insert_statements\u001b[0;34m(base_mapper, uowtransaction, mapper, table, insert, bookkeeping, use_orm_insert_stmt, execution_options)\u001b[0m\n\u001b[1;32m   1231\u001b[0m                         )\n\u001b[1;32m   1232\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m                         result = connection.execute(\n\u001b[0m\u001b[1;32m   1234\u001b[0m                             \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m                             \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectNotExecutableError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1416\u001b[0;31m             return meth(\n\u001b[0m\u001b[1;32m   1417\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m                 \u001b[0mdistilled_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/elements.py\u001b[0m in \u001b[0;36m_execute_on_connection\u001b[0;34m(self, connection, distilled_params, execution_options)\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExecutable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             return connection._execute_clauseelement(\n\u001b[0m\u001b[1;32m    524\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistilled_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecution_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_clauseelement\u001b[0;34m(self, elem, distilled_parameters, execution_options)\u001b[0m\n\u001b[1;32m   1636\u001b[0m             \u001b[0mlinting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdialect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler_linting\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWARN_LINTING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m         )\n\u001b[0;32m-> 1638\u001b[0;31m         ret = self._execute_context(\n\u001b[0m\u001b[1;32m   1639\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_ctx_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_compiled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exec_insertmany_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1843\u001b[0;31m             return self._exec_single_context(\n\u001b[0m\u001b[1;32m   1844\u001b[0m                 \u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1845\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1982\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1983\u001b[0;31m             self._handle_dbapi_exception(\n\u001b[0m\u001b[1;32m   1984\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_statement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffective_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reentrant_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1962\u001b[0m                             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt_handled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m                     self.dialect.do_execute(\n\u001b[0m\u001b[1;32m   1965\u001b[0m                         \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_statement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffective_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute_no_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "import os\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from sentence_transformers import SentenceTransformer, models, util\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "import copy\n",
        "import gc\n",
        "import math\n",
        "import random\n",
        "import nlpaug.augmenter.word as naw\n",
        "from IPython.display import display, clear_output\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "from torch import GradScaler\n",
        "from torch.amp import autocast\n",
        "import optuna\n",
        "\n",
        "# Global variable to hold the best validation score so far\n",
        "BEST_SCORE = -float(\"inf\")\n",
        "\n",
        "class BarlowTwinsNCSE:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        torch.backends.cudnn.benchmark = True  # Enable cuDNN benchmarking\n",
        "        self._prepare_datasets()\n",
        "        self._initialize_models()\n",
        "        self._initialize_optimizer_scheduler()\n",
        "        self.scaler = GradScaler(\"cuda\", enabled=self.config.get(\"use_amp\", True))  # GradScaler for AMP\n",
        "        self.best_spearman = -float(\"inf\")\n",
        "        self.best_pearson = -float(\"inf\")\n",
        "        self.patience_counter = 0\n",
        "        self.augmenters = [\n",
        "            naw.SynonymAug(aug_src='wordnet', aug_p=self.config[\"aug_p\"]),\n",
        "            naw.RandomWordAug(action=\"swap\", aug_p=self.config[\"aug_p\"]),\n",
        "            naw.RandomWordAug(aug_p=self.config[\"aug_p\"]),\n",
        "        ]\n",
        "        self.test_sts_pearson_cosine_values = []\n",
        "        self.test_sts_spearman_cosine_values = []\n",
        "        self.test_iterations = []\n",
        "        self._create_plot()\n",
        "\n",
        "    def _create_plot(self):\n",
        "        self.loss_values = []\n",
        "        self.sts_pearson_cosine_values = []\n",
        "        self.sts_spearman_cosine_values = []\n",
        "        self.mean_grad_norm_values = []\n",
        "        self.variance_values = []\n",
        "        self.learning_rate_values = []\n",
        "        self.iterations = []\n",
        "        self.epochs = []\n",
        "\n",
        "        # 3 rows x 2 columns grid with 6 subplots\n",
        "        self.fig = make_subplots(\n",
        "            rows=3, cols=2,\n",
        "            subplot_titles=(\n",
        "                \"Loss vs Iterations\",\n",
        "                \"Mean Gradient Norm vs Iterations\",\n",
        "                \"Variance vs Iterations\",\n",
        "                \"Learning Rate vs Iterations\",\n",
        "                \"Dev STS Cosine (Pearson & Spearman) vs Iterations\",\n",
        "                \"Test STS Cosine (Pearson & Spearman) vs Iterations\"\n",
        "            )\n",
        "        )\n",
        "        # Dev metrics\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Loss'), row=1, col=1)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Mean Gradient Norm'), row=1, col=2)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Variance'), row=2, col=1)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Learning Rate'), row=2, col=2)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Dev STS Pearson Cosine'), row=3, col=1)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Dev STS Spearman Cosine'), row=3, col=1)\n",
        "        # Test metrics\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Test STS Pearson Cosine'), row=3, col=2)\n",
        "        self.fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Test STS Spearman Cosine'), row=3, col=2)\n",
        "\n",
        "        # Prepare footer text\n",
        "        footer_text = \", \".join([f\"{key}={value}\" for key, value in self.config.items()])\n",
        "        augmenters_text = \", Augmenters: \" + \", \".join([f\"{aug.name}[{aug.action}:{aug.aug_p}]\" for aug in self.augmenters])\n",
        "        footer_text += augmenters_text\n",
        "        wrapped_footer = \"<br>\".join(textwrap.wrap(footer_text, width=160))\n",
        "\n",
        "        # Configure the download button\n",
        "        self.plot_config = {\n",
        "            'toImageButtonOptions': {\n",
        "                'filename': self.config[\"model_save_path\"],\n",
        "                'format': 'png',\n",
        "                'width': 1200,\n",
        "                'height': 800,\n",
        "                'scale': 1\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.fig.update_layout(\n",
        "            width=1200,\n",
        "            height=800,\n",
        "            title_text='Training Metrics',\n",
        "            showlegend=True,\n",
        "            margin=dict(l=50, r=50, t=100, b=150),\n",
        "            annotations=[\n",
        "                dict(\n",
        "                    text=wrapped_footer,\n",
        "                    showarrow=False,\n",
        "                    xref=\"paper\",\n",
        "                    yref=\"paper\",\n",
        "                    x=0.5,\n",
        "                    y=-0.15,\n",
        "                    xanchor='center',\n",
        "                    yanchor='top',\n",
        "                    align=\"center\",\n",
        "                    font=dict(size=10, color=\"gray\")\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Annotation for best metrics\n",
        "        self.best_metrics_annotation_index = len(self.fig.layout.annotations)\n",
        "        self.fig.add_annotation(\n",
        "            text=\"Best Spearman (Test): N/A<br>Best Pearson (Test): N/A<br>Best Spearman (Val): N/A<br>Best Pearson (Val): N/A\",\n",
        "            showarrow=False,\n",
        "            xref=\"paper\",\n",
        "            yref=\"paper\",\n",
        "            x=1.0,\n",
        "            y=0.0,\n",
        "            xanchor='right',\n",
        "            yanchor='bottom',\n",
        "            align=\"right\",\n",
        "            font=dict(size=12, color=\"blue\")\n",
        "        )\n",
        "\n",
        "        self.fig.show(config=self.plot_config)\n",
        "\n",
        "    def _update_traces(self):\n",
        "        with self.fig.batch_update():\n",
        "            self.fig.data[0].x = self.iterations\n",
        "            self.fig.data[0].y = self.loss_values\n",
        "            self.fig.data[1].x = self.iterations\n",
        "            self.fig.data[1].y = self.mean_grad_norm_values\n",
        "            self.fig.data[2].x = self.iterations\n",
        "            self.fig.data[2].y = self.variance_values\n",
        "            self.fig.data[3].x = self.iterations\n",
        "            self.fig.data[3].y = self.learning_rate_values\n",
        "            self.fig.data[4].x = self.iterations\n",
        "            self.fig.data[4].y = self.sts_pearson_cosine_values\n",
        "            self.fig.data[5].x = self.iterations\n",
        "            self.fig.data[5].y = self.sts_spearman_cosine_values\n",
        "            self.fig.data[6].x = self.test_iterations\n",
        "            self.fig.data[6].y = self.test_sts_pearson_cosine_values\n",
        "            self.fig.data[7].x = self.test_iterations\n",
        "            self.fig.data[7].y = self.test_sts_spearman_cosine_values\n",
        "\n",
        "            for i in range(1, 4):\n",
        "                for j in range(1, 3):\n",
        "                    self.fig.update_yaxes(autorange=True, row=i, col=j)\n",
        "                    self.fig.update_xaxes(autorange=True, row=i, col=j)\n",
        "\n",
        "    def _update_plot(self):\n",
        "        self._update_traces()\n",
        "        unique_epochs = sorted(set(self.epochs))\n",
        "        frames = []\n",
        "        for ep in unique_epochs:\n",
        "            indices = [i for i, e in enumerate(self.epochs) if e == ep]\n",
        "            frame = go.Frame(\n",
        "                data=[\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.loss_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.mean_grad_norm_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.variance_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.learning_rate_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.sts_pearson_cosine_values[i] for i in indices]),\n",
        "                    go.Scatter(x=[self.iterations[i] for i in indices], y=[self.sts_spearman_cosine_values[i] for i in indices])\n",
        "                ],\n",
        "                name=str(ep)\n",
        "            )\n",
        "            frames.append(frame)\n",
        "        self.fig.frames = frames\n",
        "\n",
        "        slider_steps = [\n",
        "            {\"args\": [[str(ep)], {\"frame\": {\"duration\": 0, \"redraw\": True},\n",
        "                                   \"mode\": \"immediate\", \"transition\": {\"duration\": 0}}],\n",
        "             \"label\": str(ep), \"method\": \"animate\"} for ep in unique_epochs\n",
        "        ]\n",
        "\n",
        "        self.fig.update_layout(\n",
        "            sliders=[{\n",
        "                \"active\": len(unique_epochs) - 1 if unique_epochs else 0,\n",
        "                \"currentvalue\": {\"prefix\": \"Epoch: \"},\n",
        "                \"pad\": {\"t\": 50},\n",
        "                \"steps\": slider_steps\n",
        "            }]\n",
        "        )\n",
        "\n",
        "        # Compute best test metrics if available\n",
        "        if self.test_sts_spearman_cosine_values:\n",
        "            best_test_spearman = max(self.test_sts_spearman_cosine_values)\n",
        "        else:\n",
        "            best_test_spearman = float('nan')\n",
        "        if self.test_sts_pearson_cosine_values:\n",
        "            best_test_pearson = max(self.test_sts_pearson_cosine_values)\n",
        "        else:\n",
        "            best_test_pearson = float('nan')\n",
        "\n",
        "        self.fig.layout.annotations[self.best_metrics_annotation_index].text = (\n",
        "            f\"Best Spearman (Test): {best_test_spearman:.4f}<br>\"\n",
        "            f\"Best Pearson (Test): {best_test_pearson:.4f}<br>\"\n",
        "            f\"Best Spearman (Val): {self.best_spearman:.4f}<br>\"\n",
        "            f\"Best Pearson (Val): {self.best_pearson:.4f}\"\n",
        "        )\n",
        "\n",
        "        for i in range(1, 4):\n",
        "            for j in range(1, 3):\n",
        "                self.fig.update_yaxes(autorange=True, row=i, col=j)\n",
        "                self.fig.update_xaxes(autorange=True, row=i, col=j)\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        self.fig.show(config=self.plot_config)\n",
        "\n",
        "    def _prepare_datasets(self):\n",
        "        wikipedia_url = \"https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse/resolve/main/wiki1m_for_simcse.txt\"\n",
        "        wikipedia_dataset_path = \"data/wiki1m_for_simcse.txt\"\n",
        "        if not os.path.exists(wikipedia_dataset_path):\n",
        "            util.http_get(wikipedia_url, wikipedia_dataset_path)\n",
        "        train_sentences = []\n",
        "        with open(wikipedia_dataset_path, encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if len(line) >= 10:\n",
        "                    train_sentences.append(line)\n",
        "        self.train_sentences = train_sentences\n",
        "\n",
        "        self.train_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"train\")\n",
        "        self.eval_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n",
        "        self.test_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"test\")\n",
        "\n",
        "        self.train_data_loader = DataLoader(\n",
        "            SentenceDataset(self.train_sentences),\n",
        "            batch_size=self.config[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            num_workers=self.config[\"num_workers\"],\n",
        "            pin_memory=True\n",
        "        )\n",
        "        self.test_evaluator = EmbeddingSimilarityEvaluator(\n",
        "            sentences1=self.test_dataset[\"sentence1\"],\n",
        "            sentences2=self.test_dataset[\"sentence2\"],\n",
        "            scores=self.test_dataset[\"score\"]\n",
        "        )\n",
        "\n",
        "        self.evaluate_steps = max(len(self.train_data_loader) // 50, 1)\n",
        "\n",
        "        # Ensure patience is at least five times as large as evaluate_steps\n",
        "        if self.config.get(\"patience\", 0) < self.evaluate_steps * 5:\n",
        "            print(f\"Warning: Patience ({self.config['patience']}) is less than evaluation steps x5 ({self.evaluate_steps * 5}). Adjusting patience to {self.evaluate_steps * 5}.\")\n",
        "            self.config[\"patience\"] = self.evaluate_steps * 5\n",
        "\n",
        "    def _apply_augmentation(self, sentences, aug):\n",
        "        return aug.augment(sentences)\n",
        "\n",
        "    def _initialize_models(self):\n",
        "        word_embedding_model = models.Transformer(\n",
        "            self.config[\"model_name\"],\n",
        "            max_seq_length=self.config[\"max_seq_length\"],\n",
        "            config_args={\"attention_dropout\": self.config[\"aug_p\"], \"dropout\": self.config[\"aug_p\"]}\n",
        "        )\n",
        "        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "        projection_layers = [torch.nn.Linear(768, self.config[\"projection_size\"])]\n",
        "        for _ in range(self.config[\"projection_depth\"] - 1):\n",
        "            projection_layers.append(torch.nn.BatchNorm1d(self.config[\"projection_size\"]))\n",
        "            projection_layers.append(torch.nn.ReLU())\n",
        "            projection_layers.append(torch.nn.Linear(self.config[\"projection_size\"], self.config[\"projection_size\"]))\n",
        "        projection_head = torch.nn.Sequential(*projection_layers)\n",
        "        self.online_model = SentenceTransformer(modules=[word_embedding_model, pooling_model, projection_head]).to(self.device)\n",
        "        encoder_modules = [copy.deepcopy(self.online_model[i]) for i in range(2)]\n",
        "        self.encoder = SentenceTransformer(modules=encoder_modules).to(self.device)\n",
        "\n",
        "    def _update_encoder(self):\n",
        "        for i in range(len(self.encoder)):\n",
        "            self.encoder[i].load_state_dict(self.online_model[i].state_dict())\n",
        "\n",
        "    def _initialize_optimizer_scheduler(self):\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.online_model.parameters(),\n",
        "            lr=self.config[\"learning_rate\"],\n",
        "            betas=(0.9, 0.999),\n",
        "            eps=1e-8,\n",
        "            weight_decay=self.config[\"weight_decay\"]\n",
        "        )\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=200, verbose=True, min_lr=1e-6)\n",
        "\n",
        "    def _forward_pass(self, model, sentences, train):\n",
        "        if train:\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "        features = model.tokenize(sentences)\n",
        "        features = {k: v.to(self.device, non_blocking=True) for k, v in features.items()}\n",
        "        with autocast(\"cuda\", enabled=self.config.get(\"use_amp\", True)):\n",
        "            embeddings = model[0](features)[\"token_embeddings\"]\n",
        "            pooled = model[1]({\"token_embeddings\": embeddings})[\"sentence_embedding\"]\n",
        "            if len(model) > 2:\n",
        "                return model[2](pooled)\n",
        "            else:\n",
        "                return pooled\n",
        "\n",
        "    def _mixed_barlow_twins_loss(self, z_a, z_b):\n",
        "        N, D = z_a.size()\n",
        "        z_a_norm = (z_a - z_a.mean(dim=0)) / (z_a.std(dim=0) + 1e-6)\n",
        "        z_b_norm = (z_b - z_b.mean(dim=0)) / (z_b.std(dim=0) + 1e-6)\n",
        "        c = torch.matmul(z_a_norm.T, z_b_norm) / N\n",
        "        I = torch.eye(D, device=z_a.device)\n",
        "        c_diff = (c - I).pow(2)\n",
        "        off_diag_mask = ~torch.eye(D, dtype=torch.bool, device=z_a.device)\n",
        "        c_diff[off_diag_mask] *= self.config[\"lambda_bt\"]\n",
        "        loss_bt = c_diff.sum()\n",
        "        # MixUp Regularization\n",
        "        idx = torch.randperm(N)\n",
        "        alpha = torch.tensor(np.random.beta(1.0, 1.0), device=z_a.device, dtype=z_a.dtype)\n",
        "        z_m = alpha * z_a + (1 - alpha) * z_b[idx, :]\n",
        "        z_m_norm = (z_m - z_m.mean(dim=0)) / (z_m.std(dim=0) + 1e-6)\n",
        "        cc_m_a = torch.matmul(z_m_norm.T, z_a_norm) / N\n",
        "        cc_m_b = torch.matmul(z_m_norm.T, z_b_norm) / N\n",
        "        cc_m_a_gt = alpha * torch.matmul(z_a_norm.T, z_a_norm) / N + (1 - alpha) * torch.matmul(z_b_norm[idx, :].T, z_a_norm) / N\n",
        "        cc_m_b_gt = alpha * torch.matmul(z_a_norm.T, z_b_norm) / N + (1 - alpha) * torch.matmul(z_b_norm[idx, :].T, z_b_norm) / N\n",
        "        loss_mix = self.config[\"lambda_mixup\"] * self.config[\"lambda_bt\"] * (\n",
        "            (cc_m_a - cc_m_a_gt).pow(2).sum() + (cc_m_b - cc_m_b_gt).pow(2).sum()\n",
        "        )\n",
        "        return loss_bt + loss_mix\n",
        "\n",
        "    def _evaluate_without_heads(self):\n",
        "        self._update_encoder()\n",
        "        self.encoder.eval()\n",
        "        indices = list(range(len(self.eval_dataset[\"sentence1\"])))\n",
        "        random.shuffle(indices)\n",
        "        sentences1 = [self.eval_dataset[\"sentence1\"][i] for i in indices]\n",
        "        sentences2 = [self.eval_dataset[\"sentence2\"][i] for i in indices]\n",
        "        scores = [self.eval_dataset[\"score\"][i] for i in indices]\n",
        "        evaluator = EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)\n",
        "        return evaluator(self.encoder)\n",
        "\n",
        "    def fit(self, trial=None):\n",
        "        latest_eval_metrics = {}\n",
        "        for epoch in range(self.config[\"epochs\"]):\n",
        "            early_stop = False\n",
        "            epoch_loss = 0\n",
        "            pbar = tqdm(self.train_data_loader, desc=f\"Epoch {epoch+1}/{self.config['epochs']}:\")\n",
        "            for idx, sentences in enumerate(pbar):\n",
        "                aug1, aug2 = random.sample(self.augmenters, 2)\n",
        "                s1 = self._apply_augmentation(sentences, aug1)\n",
        "                s2 = self._apply_augmentation(sentences, aug2)\n",
        "                with autocast(\"cuda\", enabled=self.config.get(\"use_amp\", True)):\n",
        "                    z_a = self._forward_pass(self.online_model, s1, train=True)\n",
        "                    z_b = self._forward_pass(self.online_model, s2, train=True)\n",
        "                    loss = self._mixed_barlow_twins_loss(z_a, z_b)\n",
        "                epoch_loss += loss.item()\n",
        "                self.optimizer.zero_grad()\n",
        "                self.scaler.scale(loss).backward()\n",
        "                scale = self.scaler.get_scale()\n",
        "                scale = scale if scale != 0 else 1e-8\n",
        "                total_norm_scaled = 0.0\n",
        "                for param in self.online_model.parameters():\n",
        "                    if param.grad is not None:\n",
        "                        param_norm = param.grad.data.norm(2).item()\n",
        "                        total_norm_scaled += param_norm ** 2\n",
        "                total_norm_scaled = math.sqrt(total_norm_scaled)\n",
        "                total_norm = total_norm_scaled / scale\n",
        "                mean_grad_norm = total_norm / len(list(self.online_model.parameters()))\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "                self.scheduler.step(loss)\n",
        "                if idx % self.evaluate_steps == 0 or idx in [0, len(self.train_data_loader)-1]:\n",
        "                    latest_eval_metrics = self._evaluate_without_heads()\n",
        "                    last_spearman = latest_eval_metrics.get('spearman_cosine', -float('inf'))\n",
        "                    last_pearson = latest_eval_metrics.get('pearson_cosine', -float('inf'))\n",
        "                    self._update_encoder()\n",
        "                    self.encoder.eval()\n",
        "                    test_metrics = self.test_evaluator(self.encoder)\n",
        "                    self.test_sts_pearson_cosine_values.append(test_metrics.get('pearson_cosine', np.nan))\n",
        "                    self.test_sts_spearman_cosine_values.append(test_metrics.get('spearman_cosine', np.nan))\n",
        "                    self.test_iterations.append(idx)\n",
        "                    self._update_plot()\n",
        "\n",
        "                    # Report intermediate value and check for pruning\n",
        "                    if trial is not None:\n",
        "                        current_step = epoch * len(self.train_data_loader) + idx\n",
        "                        trial.report(last_spearman, current_step)\n",
        "                        if trial.should_prune():\n",
        "                            print(f\"Trial {trial.number} pruned at epoch {epoch+1}, iteration {idx}.\")\n",
        "                            save_plot(trial.number, self)\n",
        "                            self.cleanup()\n",
        "                            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "                    if last_spearman > self.best_spearman:\n",
        "                        self.best_spearman = last_spearman\n",
        "                        self.best_pearson = last_pearson\n",
        "                        self.patience_counter = 0\n",
        "                    else:\n",
        "                        self.patience_counter += self.evaluate_steps\n",
        "                pbar.set_postfix({\n",
        "                    \"loss\": loss.item(),\n",
        "                    **latest_eval_metrics,\n",
        "                    \"mean_grad_norm\": mean_grad_norm,\n",
        "                    \"learning_rate\": self.optimizer.param_groups[0]['lr']\n",
        "                })\n",
        "                self.loss_values.append(loss.item())\n",
        "                self.sts_pearson_cosine_values.append(latest_eval_metrics.get('pearson_cosine', np.nan))\n",
        "                self.sts_spearman_cosine_values.append(latest_eval_metrics.get('spearman_cosine', np.nan))\n",
        "                self.mean_grad_norm_values.append(mean_grad_norm)\n",
        "                self.variance_values.append(torch.var(z_a, dim=0).mean().item())\n",
        "                self.learning_rate_values.append(self.optimizer.param_groups[0]['lr'])\n",
        "                self.iterations.append(idx)\n",
        "                self.epochs.append(epoch)\n",
        "                if idx % self.evaluate_steps == 0 or idx in [0, len(self.train_data_loader)-1]:\n",
        "                    if self.patience_counter >= self.config[\"patience\"]:\n",
        "                        early_stop = True\n",
        "                        print(f\"Early stopping triggered at epoch {epoch+1}, iteration {idx}.\")\n",
        "                        print(f\"Best Spearman Correlation: {self.best_spearman}\")\n",
        "                        print(f\"Best Pearson Correlation: {self.best_pearson}\")\n",
        "                        break\n",
        "            if early_stop:\n",
        "                break\n",
        "            avg_loss = epoch_loss / len(self.train_data_loader)\n",
        "            pbar.set_description(f\"Epoch {epoch+1} Loss: {avg_loss}\")\n",
        "\n",
        "    def cleanup(self):\n",
        "        del self.online_model, self.optimizer, self.scheduler, self.scaler\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "class SentenceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sentences):\n",
        "        self.sentences = sentences\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentences[idx]\n",
        "\n",
        "# Mount Google Drive for checkpoint and study persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/violet_bert_base_checkpoints_2\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "STUDY_DB_PATH = os.path.join(CHECKPOINT_DIR, \"llm_finetuning_study.db\")\n",
        "\n",
        "\n",
        "def save_plot(trial_number, trainer):\n",
        "    # Also save the graph to a subfolder called \"graphs\"\n",
        "    graphs_dir = os.path.join(CHECKPOINT_DIR, \"graphs\")\n",
        "    os.makedirs(graphs_dir, exist_ok=True)\n",
        "    graph_path = os.path.join(graphs_dir, f\"checkpoint_trial_{trial_number}.png\")\n",
        "    trainer.fig.write_image(graph_path)\n",
        "    print(f\"Graph saved to {graph_path}\")\n",
        "\n",
        "def save_checkpoint(trial_number, trainer):\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_trial_{trial_number}.pt\")\n",
        "    torch.save({\n",
        "        'online_model_state_dict': trainer.online_model.state_dict(),\n",
        "        'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
        "        'scheduler_state_dict': trainer.scheduler.state_dict(),\n",
        "        'best_spearman': trainer.best_spearman,\n",
        "        'best_pearson': trainer.best_pearson,\n",
        "        'epochs': trainer.epochs,\n",
        "        'iterations': trainer.iterations,\n",
        "    }, checkpoint_path)\n",
        "\n",
        "    save_plot(trial_number, trainer)\n",
        "\n",
        "    return checkpoint_path\n",
        "\n",
        "def load_checkpoint(trainer, checkpoint_path):\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        trainer.online_model.load_state_dict(checkpoint['online_model_state_dict'])\n",
        "        trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        trainer.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        trainer.best_spearman = checkpoint.get('best_spearman', -float(\"inf\"))\n",
        "        trainer.best_pearson = checkpoint.get('best_pearson', -float(\"inf\"))\n",
        "        trainer.epochs = checkpoint.get('epochs', [])\n",
        "        trainer.iterations = checkpoint.get('iterations', [])\n",
        "        print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
        "\n",
        "def objective(trial):\n",
        "    config = {\n",
        "        \"model_name\": \"bert-base-uncased\",\n",
        "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128]),\n",
        "        \"projection_depth\": trial.suggest_int(\"projection_depth\", 2, 3),\n",
        "        \"projection_size\": trial.suggest_categorical(\"projection_size\", [2048, 4096, 6144, 8192]),\n",
        "        \"epochs\": 1,  # fixed at 1 for faster evaluation\n",
        "        \"warmup_proportion\": 0.0,\n",
        "        \"max_seq_length\": 75,  # or trial.suggest_categorical(\"max_seq_length\", [32, 75]),\n",
        "        \"aug_p\": trial.suggest_uniform(\"aug_p\", 0.1, 0.4),\n",
        "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-4),\n",
        "        \"model_save_path\": os.path.join(CHECKPOINT_DIR, f\"train_stsb_bt-distilbert-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_trial{trial.number}\"),\n",
        "        \"num_workers\": 10,\n",
        "        \"weight_decay\": trial.suggest_uniform(\"weight_decay\", 1e-4, 0.2),\n",
        "        \"lambda_bt\": trial.suggest_uniform(\"lambda_bt\", 0.001, 0.2),\n",
        "        \"lambda_mixup\": trial.suggest_uniform(\"lambda_mixup\", 0.1, 1.5),\n",
        "        \"use_amp\": True,\n",
        "        \"patience\": 1000\n",
        "    }\n",
        "\n",
        "    trainer = BarlowTwinsNCSE(config)\n",
        "    # Resume from checkpoint if available\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_trial_{trial.number}.pt\")\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        load_checkpoint(trainer, checkpoint_path)\n",
        "\n",
        "    try:\n",
        "        trainer.fit(trial=trial)\n",
        "    except KeyboardInterrupt:\n",
        "        save_checkpoint(trial.number, trainer)\n",
        "        raise optuna.TrialPruned(\"Trial interrupted and checkpoint saved.\")\n",
        "\n",
        "    # Evaluate after training\n",
        "    val_metrics = trainer._evaluate_without_heads()\n",
        "    current_score = val_metrics.get(\"spearman_cosine\", -float(\"inf\"))\n",
        "\n",
        "    global BEST_SCORE\n",
        "    # Save the checkpoint only if this trial outperforms previous trials.\n",
        "    if current_score > BEST_SCORE:\n",
        "        BEST_SCORE = current_score\n",
        "        print(f\"Trial {trial.number} achieved a new best score: {current_score:.4f}. Saving checkpoint.\")\n",
        "        save_checkpoint(trial.number, trainer)\n",
        "    else:\n",
        "        print(f\"Trial {trial.number} did not improve the best score ({BEST_SCORE:.4f}). Checkpoint not saved.\")\n",
        "\n",
        "    trainer.cleanup()\n",
        "    return current_score\n",
        "\n",
        "# Create or resume an Optuna study persisted on Google Drive\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=\"llm_finetuning_study\",\n",
        "    storage=f\"sqlite:///{STUDY_DB_PATH}\",\n",
        "    load_if_exists=True,\n",
        "    pruner=optuna.pruners.HyperbandPruner(),\n",
        ")\n",
        "\n",
        "'''\n",
        "study.enqueue_trial(\n",
        "    {\n",
        "        \"batch_size\": 128,\n",
        "        \"projection_depth\": 2,\n",
        "        \"projection_size\": 8192,\n",
        "        \"aug_p\": 0.1205,\n",
        "        \"learning_rate\": 2.919e-5,\n",
        "        \"weight_decay\": 0.1337,\n",
        "        \"lambda_bt\": 0.1128,\n",
        "        \"lambda_mixup\": 1.094,\n",
        "    }\n",
        ")\n",
        "\n",
        "study.enqueue_trial(\n",
        "    {\n",
        "        \"batch_size\": 128,\n",
        "        \"projection_depth\": 3,\n",
        "        \"projection_size\": 8192,\n",
        "        \"aug_p\": 0.1323,\n",
        "        \"learning_rate\": 3.064e-5,\n",
        "        \"weight_decay\": 0.08471,\n",
        "        \"lambda_bt\": 0.1575,\n",
        "        \"lambda_mixup\": 1.289,\n",
        "    }\n",
        ")\n",
        "'''\n",
        "\n",
        "study.optimize(objective, n_trials=20)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "authorship_tag": "ABX9TyN+BuzQ5j8Xqpl8k77jBEoD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}